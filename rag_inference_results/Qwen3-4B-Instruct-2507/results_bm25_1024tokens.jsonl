{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 927, "num_retrieved": 2, "inference_time": 0.5875944346189499, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since Unix epoch\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if tz is None:\n        return Time.from_clock_time(Clock().local_time(), UnixEpoch)\n    else:\n        # Convert nanoseconds to a datetime with timezone, then extract time\n        dt = DateTime.utc_now()\n        dt = dt.replace(tzinfo=timezone.utc).astimezone(tz)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The time in nanoseconds since the Unix epoch\n    :param tz: Optional timezone information\n    :return: Time\n    \"\"\"\n    if tz is None:\n        return Time.from_clock_time(Clock().local_time(), UnixEpoch)\n    else:\n        # Convert nanoseconds to a datetime with timezone, then extract time\n        dt = DateTime.utc_now()\n        dt = dt.replace(tzinfo=timezone.utc).astimezone(tz)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since Unix epoch\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if tz is None:\n        return Time.from_clock_time(Clock().local_time(), UnixEpoch)\n    else:\n        # Convert nanoseconds to a datetime with timezone, then extract time\n        dt = DateTime.utc_now()\n        dt = dt.replace(tzinfo=timezone.utc).astimezone(tz)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since Unix epoch\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if tz is None:\n        return Time.from_clock_time(Clock().local_time(), UnixEpoch)\n    else:\n        # Convert nanoseconds to a datetime with timezone, then extract time\n        dt = DateTime.utc_now()\n        dt = dt.replace(tzinfo=timezone.utc).astimezone(tz)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since Unix epoch\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if tz is None:\n        return Time.from_clock_time(Clock().local_time(), UnixEpoch)\n    else:\n        dt = DateTime.utc_now().replace(tzinfo=timezone.utc).astimezone(tz)\n        return Time.from_timetz(dt.timetz())"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 966, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 966, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 966, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (Point, CartesianPoint, WGS84Point)):\n        # Assuming the dehydrate_point function is part of the spatial module\n        # and handles the conversion to a serialized format\n        return _hydration.dehydrate_point(value)\n    else:\n        raise TypeError(\"Expected a Point, CartesianPoint, or WGS84Point instance\")", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (Point, CartesianPoint, WGS84Point)):\n        # Assuming the dehydrate logic is based on the point type\n        # This is a placeholder implementation based on context\n        # Actual implementation would depend on the internal dehydrate_point function\n        return {\"type\": \"point\", \"coordinates\": value.coordinates}\n    return None", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (Point, CartesianPoint, WGS84Point)):\n        # Convert point to a format suitable for dehydrating (e.g., tuple or dict)\n        # This is a simplified representation; actual implementation would depend on the driver's encoding\n        return {\n            \"type\": _point_type,\n            \"coordinates\": (value.x, value.y)\n        }\n    else:\n        raise TypeError(f\"Expected a Point, CartesianPoint, or WGS84Point, got {type(value)}\")", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (Point, CartesianPoint, WGS84Point)):\n        # Assuming the dehydrate_point function is part of the spatial module\n        # and handles the conversion to a serialized format\n        return _hydration.dehydrate_point(value)\n    else:\n        raise TypeError(f\"Expected a Point, CartesianPoint, or WGS84Point, but got {type(value)}\")", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    return _hydration.dehydrate_point(value)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 953, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def keys(self):\n    \"\"\"Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 874, "num_retrieved": 2, "inference_time": 0.5875944346189499, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol version must be a tuple of two integers\")\n        \n        # Check if the protocol version is supported\n        # Based on the context, we have AsyncBolt4x2 for version (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    \n    # If no protocol version is provided, return all supported versions\n    return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol version must be a tuple of two integers\")\n        \n        # Assuming only AsyncBolt4x2 is defined in context and supports (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol version must be a tuple of two integers\")\n        \n        # Assuming only AsyncBolt4x2 is defined in context and supports (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol_version must be a tuple of two integers\")\n        \n        # Assuming only AsyncBolt4x2 is defined in the context and supports (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol_version must be a tuple of two integers\")\n        \n        # Assuming only AsyncBolt4x2 is defined in the context and supports (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 891, "num_retrieved": 1, "inference_time": 0.5875944346189499, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # Handle exceptions if needed, but typically transaction will be rolled back\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # If an exception occurs, rollback the transaction\n                tx.rollback()\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # Rollback transaction on exception\n                tx.rollback()\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # Re-raise the exception after transaction is committed or rolled back\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.session.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # If an exception occurs, rollback the transaction\n                tx.rollback()\n                raise e\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 934, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 923, "num_retrieved": 2, "inference_time": 0.5875944346189499, "generate_results": ["def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 954, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return {key: value for key, value in self.items()}\n    \n    result = {}\n    for key in keys:\n        try:\n            if isinstance(key, int):\n                result[key] = self[key]\n            else:\n                result[key] = self[key]\n        except IndexError:\n            raise IndexError(f\"Index out of bounds: {key}\")\n    \n    # Add missing keys with None values\n    for key in self.keys():\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return {key: value for key, value in self.items()}\n    \n    result = {}\n    for key in keys:\n        try:\n            if isinstance(key, int):\n                result[key] = self[key]\n            else:\n                result[key] = self[key]\n        except IndexError:\n            raise IndexError(f\"Index out of bounds: {key}\")\n    \n    # Add missing keys with None values\n    for key in self.keys():\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return {k: v for k, v in self.items()}\n    \n    result = {}\n    for key in keys:\n        try:\n            if isinstance(key, int):\n                result[key] = self[key]\n            else:\n                result[key] = self[key]\n        except IndexError:\n            raise IndexError(f\"Index out of bounds: {key}\")\n    \n    # Add missing keys with None values\n    for key in self.keys():\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return {k: v for k, v in self.items()}\n    \n    result = {}\n    for key in keys:\n        try:\n            if isinstance(key, int):\n                result[key] = self[key]\n            else:\n                result[key] = self[key]\n        except IndexError:\n            raise IndexError(f\"Index out of bounds: {key}\")\n    \n    # Add missing keys with None values\n    for key in self.keys():\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return {k: v for k, v in self.items()}\n    \n    result = {}\n    for key in keys:\n        try:\n            if isinstance(key, int):\n                result[key] = self[key]\n            else:\n                result[key] = self[key]\n        except IndexError:\n            raise IndexError(f\"Index out of bounds: {key}\")\n    \n    # Add missing keys with None values\n    for key in self.keys():\n        if key not in result:\n            result[key] = None\n    \n    return result"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 949, "num_retrieved": 3, "inference_time": 0.5875944346189499, "generate_results": ["def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return int.from_bytes(self[-2:], byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return int.from_bytes(self[-2:], byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return int.from_bytes(self[-2:], byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return int.from_bytes(self[-2:], byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return int.from_bytes(self[-2:], byteorder='big')"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.5875944346189499, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending to output queue\n    # This is a placeholder - actual implementation would depend on the internal\n    # message queue and protocol handling\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending to output queue\n    # This is a placeholder - actual implementation would depend on the internal\n    # messaging protocol and queue management of the driver\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Append a DISCARD message to the output queue with the specified parameters\n    # The actual implementation would depend on the internal message queue or protocol\n    # This is a placeholder for the actual message construction logic\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Append a DISCARD message to the output queue with the given parameters\n    # The actual implementation would depend on the internal message queue logic\n    # of the Neo4j driver, but based on the pattern of pull and similar methods,\n    # this would involve constructing a DISCARD message with the provided parameters\n    # and adding it to the output queue.\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending logic\n    # This is a placeholder - actual implementation would depend on the internal\n    # message queue or protocol handling\n    pass"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 890, "num_retrieved": 1, "inference_time": 0.5875944346189499, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        'type': 'BEGIN',\n        'mode': mode,\n        'bookmarks': bookmarks,\n        'metadata': metadata,\n        'timeout': timeout,\n        'db': db,\n        'imp_user': imp_user,\n        'dehydration_hooks': dehydration_hooks,\n        'hydration_hooks': hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._output_queue.append(message)\n    \n    # Return a Response object with the provided handlers\n    return self._create_response(handlers=handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        'type': 'BEGIN',\n        'mode': mode,\n        'bookmarks': bookmarks,\n        'metadata': metadata,\n        'timeout': timeout,\n        'db': db,\n        'imp_user': imp_user,\n        'dehydration_hooks': dehydration_hooks,\n        'hydration_hooks': hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._output_queue.append(message)\n    \n    # Return a Response object for the transaction\n    return self._create_response(handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        \"type\": \"BEGIN\",\n        \"mode\": mode,\n        \"bookmarks\": bookmarks,\n        \"metadata\": metadata,\n        \"timeout\": timeout,\n        \"db\": db,\n        \"imp_user\": imp_user,\n        \"dehydration_hooks\": dehydration_hooks,\n        \"hydration_hooks\": hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._queue.append(message)\n    \n    # Return a Response object for the operation\n    return self._response(message, handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        \"type\": \"BEGIN\",\n        \"mode\": mode,\n        \"bookmarks\": bookmarks,\n        \"metadata\": metadata,\n        \"timeout\": timeout,\n        \"db\": db,\n        \"imp_user\": imp_user,\n        \"dehydration_hooks\": dehydration_hooks,\n        \"hydration_hooks\": hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._output_queue.append(message)\n    \n    # Return a Response object for the operation\n    return self._create_response(handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    # This is a placeholder - actual implementation would involve\n    # sending a BEGIN message via the underlying transport layer\n    # and returning a Response object with appropriate handlers\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks,\n                              metadata=metadata, timeout=timeout,\n                              db=db, imp_user=imp_user,\n                              dehydration_hooks=dehydration_hooks,\n                              hydration_hooks=hydration_hooks, **handlers)"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 980, "num_retrieved": 4, "inference_time": 0.5875944346189499, "generate_results": ["def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else -0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    if fractional_part < 0.5:\n        return integer_part\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    else:  # fractional_part == 0.5\n        return integer_part if integer_part % 2 == 0 else integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else -0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    if fractional_part < 0.5:\n        return integer_part\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    else:  # fractional_part == 0.5\n        return integer_part if integer_part % 2 == 0 else integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    if fractional_part < 0.5:\n        return integer_part\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    else:  # fractional_part == 0.5\n        return integer_part if integer_part % 2 == 0 else integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 932, "num_retrieved": 2, "inference_time": 0.5875944346189499, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base class for the point\n    class Point:\n        def __init__(self, *coordinates):\n            self.coordinates = coordinates\n            self.srid = None\n        \n        def __repr__(self):\n            return f\"Point({self.coordinates}, srid={self.srid})\"\n    \n    # Create a new class that inherits from Point\n    class_name = name.capitalize()\n    point_class = type(class_name, (Point,), {})\n    \n    # Set up SRID mapping for the point class\n    srid_table = {}\n    for srid, dim in srid_map.items():\n        if dim == 2:\n            srid_table[srid] = (point_class, 2)\n        elif dim == 3:\n            srid_table[srid] = (point_class, 3)\n    \n    # Add the srid_table to the point class as an attribute\n    point_class.srid_table = srid_table\n    \n    # Add fields to the point class if provided\n    if fields:\n        for field_name, field_value in fields.items():\n            setattr(point_class, field_name, field_value)\n    \n    return point_class", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(type):\n        def __new__(cls, name, bases, attrs):\n            # Create the new class\n            new_class = super().__new__(cls, name, bases, attrs)\n            \n            # Add a hydrate method if needed\n            if 'hydrate' not in attrs:\n                def hydrate(srid, *coordinates):\n                    try:\n                        point_class, dim = srid_map[srid]\n                    except KeyError:\n                        point = Point(coordinates)\n                        point.srid = srid\n                        return point\n                    else:\n                        if len(coordinates) != dim:\n                            raise ValueError(\"SRID %d requires %d coordinates (%d provided)\" % (srid, dim, len(coordinates)))\n                        return point_class(coordinates)\n                new_class.hydrate = hydrate\n            \n            return new_class\n    \n    # Define the base class attributes\n    attrs = {\n        '__module__': __name__,\n        '__doc__': f\"Point subclass for {name}\",\n        'srid': None,\n        'coordinates': None,\n    }\n    \n    # Create the new class using the metaclass\n    return PointSubclass(name, (Point,), attrs)", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base Point class\n    class Point:\n        def __init__(self, coordinates):\n            self.coordinates = coordinates\n            self.srid = None\n\n        def __repr__(self):\n            return f\"Point({self.coordinates}, srid={self.srid})\"\n\n    # Create a new class that inherits from Point\n    class_name = f\"{name}Point\"\n    point_class = type(class_name, (Point,), {})\n\n    # Set up SRID mapping\n    srid_table = {}\n    for srid, dim in fields.items():\n        if srid in srid_map:\n            srid_table[srid] = (point_class, dim)\n        else:\n            srid_table[srid] = (Point, dim)\n\n    # Add SRID-specific hydration logic\n    def hydrate_point(srid, *coordinates):\n        try:\n            point_class, dim = srid_table[srid]\n        except KeyError:\n            point = Point(coordinates)\n            point.srid = srid\n            return point\n        else:\n            if len(coordinates) != dim:\n                raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n            return point_class(coordinates)\n\n    # Attach hydration function to the class\n    point_class.hydrate_point = hydrate_point\n\n    return point_class", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base class for the point\n    class PointSubclass:\n        def __init__(self, *coordinates):\n            self.coordinates = coordinates\n            self.srid = None\n        \n        def __repr__(self):\n            return f\"{name}({self.coordinates}, srid={self.srid})\"\n    \n    # Add attributes to the class based on fields and srid_map\n    # For each SRID in srid_map, associate it with a specific dimension and point class\n    srid_table = {}\n    for srid, dim in srid_map.items():\n        srid_table[srid] = (PointSubclass, dim)\n    \n    # Update the class to include the srid_table for hydration\n    PointSubclass.srid_table = srid_table\n    \n    # Return the dynamically created class\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base class for the point\n    class Point:\n        def __init__(self, coordinates):\n            self.coordinates = coordinates\n            self.srid = None\n        \n        def __repr__(self):\n            return f\"Point({self.coordinates}, srid={self.srid})\"\n    \n    # Create a new class that inherits from Point\n    class_name = f\"{name}Point\"\n    \n    # Define the class attributes\n    class_dict = {\n        \"__module__\": __name__,\n        \"__doc__\": f\"Point subclass for {name}\",\n        \"__init__\": Point.__init__,\n        \"__repr__\": Point.__repr__,\n    }\n    \n    # Add SRID mapping if provided\n    if srid_map:\n        class_dict[\"srid_map\"] = srid_map\n    \n    # Add field mapping for the point\n    if fields:\n        class_dict[\"fields\"] = fields\n    \n    # Create the new class\n    point_class = type(class_name, (Point,), class_dict)\n    \n    # Register the class in the srid_table if needed\n    if srid_map:\n        for srid, dim in srid_map.items():\n            if dim == len(fields):\n                point_class.srid = srid\n                point_class.dim = dim\n    \n    return point_class"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.5875944346189499, "generate_results": ["def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.8216110318899155, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that need to be set inline\n    r_options = \"\"\"\n    # Set R options that cannot be configured via environment variables\n    options(stringsAsFactors = FALSE)\n    options(scipen = 999)\n    options(warn = 1)\n    \"\"\"\n    return code + r_options", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via env variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = \"\"\"\n    # Set common R options that can only be configured at runtime\n    options(stringsAsFactors = FALSE)\n    options(warn = 1)\n    options(scipen = 999)\n    options(digits = 7)\n    \"\"\"\n    return code + \"\\n\" + r_options", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via env variables\n    # Example: setting R options like 'options(stringsAsFactors = FALSE)'\n    # This is a placeholder implementation - actual R options would depend on specific needs\n    r_options = \"options(stringsAsFactors = FALSE, warn = 1)\\n\"\n    return r_options + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Example: setting R options like 'options(stringsAsFactors = FALSE)'\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = \"options(stringsAsFactors = FALSE, warn = 1)\\n\"\n    return r_options + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via env variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = \"\"\"\n    # Set common R options that cannot be configured via environment variables\n    options(stringsAsFactors = FALSE)\n    options(warn = 1)\n    options(show.signif.stars = FALSE)\n    options(digits = 6)\n    \"\"\"\n    return code + \"\\n\" + r_options"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 971, "num_retrieved": 4, "inference_time": 0.8216110318899155, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If color is requested, enable pty if supported\n    if color:\n        # Use a pty if available on the platform\n        import pty\n        # In practice, this would involve setting up a pseudo-terminal\n        # For now, just pass through the color flag to the underlying command\n        pass\n    \n    # Execute the command with the given arguments and jobs\n    # This is a simplified implementation - actual xargs behavior would involve\n    # spawning processes and managing input/output\n    import subprocess\n    import os\n    \n    # Build the command with the arguments\n    process = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        bufsize=0,\n    )\n    \n    # Feed each argument to the command\n    for arg in varargs:\n        process.stdin.write(arg + '\\n')\n    \n    # Close stdin to signal end of input\n    process.stdin.close()\n    \n    # Wait for completion and get results\n    stdout, stderr = process.communicate()\n    \n    # Return exit code and output\n    return process.returncode, stdout.encode() if stdout else b''", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Create the command to run with xargs\n    # This is a simplified implementation - in practice, xargs would spawn\n    # multiple processes to run the command on each file argument\n    import subprocess\n    import sys\n    \n    # Determine if we should use a pty for color support\n    if color and sys.platform in ('linux', 'darwin'):\n        # Use a pty to support color output\n        pass  # pty handling is abstracted away in the actual implementation\n    \n    # Build the command with the provided arguments\n    # The actual xargs behavior would involve spawning processes\n    # with the given command and file arguments\n    \n    # For now, we'll just run the command on each file argument\n    # in a way that respects the target concurrency\n    \n    # Since we don't have the full implementation of xargs, \n    # we'll return a placeholder result\n    try:\n        # Use subprocess to run the command with the file arguments\n        # This is a simplified version that runs the command once per file\n        # In a real implementation, this would be parallelized\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) if varargs else '',\n            text=True,\n            capture_output=True,\n            encoding='utf-8',\n            shell=False,\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    except Exception as e:\n        return 1, str(e).encode('utf-8')", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If color is True, we may need to create a pty for color support\n    if color and sys.platform in ('linux', 'darwin'):\n        # Use a pty for color support on supported platforms\n        import pty\n        master, slave = pty.openpty()\n        try:\n            # Set up the command to run with the slave terminal\n            cmd_with_pty = cmd + ['--pty']\n            # Execute the command with the slave fd\n            process = subprocess.Popen(\n                cmd_with_pty,\n                stdin=subprocess.PIPE,\n                stdout=slave,\n                stderr=slave,\n                text=True,\n                **kwargs\n            )\n            # Close the slave and return the result\n            os.close(slave)\n            stdout, stderr = process.communicate()\n            return process.returncode, (stdout + stderr).encode('utf-8')\n        except Exception as e:\n            # If pty setup fails, fall back to normal execution\n            pass\n    \n    # Fall back to normal execution without pty\n    try:\n        # Use subprocess to run the command with the provided arguments\n        process = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            **kwargs\n        )\n        \n        # Feed the arguments to the command\n        input_data = '\\n'.join(varargs) + '\\n'\n        stdout, stderr = process.communicate(input=input_data)\n        \n        # Return the exit code and combined output\n        return process.returncode, (stdout + stderr).encode('utf-8')\n    except Exception as e:\n        # Handle any exceptions that occur during execution\n        return 1, str(e).encode('utf-8')", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine if we should use a pty based on color setting and platform\n    if color and sys.platform != 'win32':\n        # Use pty for color support on supported platforms\n        import pty\n        # This is a simplified implementation - actual pty handling would depend on context\n        pass\n    \n    # Execute the command with the given arguments and concurrency\n    # This is a simplified version - actual implementation would use subprocess\n    import subprocess\n    import os\n    \n    # Prepare the command and arguments\n    process = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        **kwargs\n    )\n    \n    # Feed each argument to the command\n    for arg in varargs:\n        process.stdin.write(arg + '\\n')\n    process.stdin.flush()\n    \n    # Wait for completion and get results\n    stdout, stderr = process.communicate()\n    \n    # Return exit code and output\n    return process.returncode, stdout.encode() if stdout else b''", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Create the command to run with xargs\n    # This is a simplified implementation - in practice, this would spawn\n    # multiple processes using xargs with the specified concurrency\n    import subprocess\n    import sys\n    \n    # Determine if we should use a pty for color support\n    if color and sys.platform in ('linux', 'darwin'):\n        # Use a pty to support color output\n        pass  # pty handling is abstracted away in the actual implementation\n    \n    # Build the command to pass to xargs\n    # In a real implementation, this would use subprocess.Popen with xargs\n    # For now, we return a placeholder result\n    try:\n        # Run the command with the provided arguments\n        # This is a simplified version - actual implementation would use xargs\n        # to distribute the varargs across multiple processes\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) if varargs else '',\n            text=True,\n            capture_output=True,\n            encoding='utf-8',\n            timeout=300,\n        )\n        return result.returncode, result.stdout.encode('utf-8') if result.stdout else b''\n    except subprocess.TimeoutExpired:\n        return 1, b\"Command timed out\\n\"\n    except Exception as e:\n        return 1, f\"Error: {str(e)}\\n\".encode('utf-8')"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1005, "num_retrieved": 5, "inference_time": 0.8216110318899155, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 974, "num_retrieved": 4, "inference_time": 0.8216110318899155, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 911, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly formatted for Windows execution\n        # Also, deep-path shebangs need to be handled correctly\n        \n        # If the command starts with a shebang, we need to normalize it\n        # by replacing the shebang with the actual executable path\n        # This is a simplified implementation based on common patterns\n        \n        # Check if the first element of the command starts with '#!'\n        if cmd and cmd[0].startswith('#!'):\n            # Replace the shebang with the actual executable path\n            # This is a placeholder - actual implementation would depend\n            # on the specific shebang parsing logic\n            # For now, we just return the original command\n            pass\n        \n        # For Windows, we need to ensure the command is properly formatted\n        # to avoid issues with shebang parsing\n        # This includes handling deep-path shebangs\n        \n        # Normalize the command by ensuring it's in a format that Windows can handle\n        # This is a simplified version - actual implementation would require\n        # more detailed shebang parsing and path resolution\n        \n        # Return the original command as is for now\n        return cmd\n    \n    # On non-Windows platforms, return the command as is\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly formatted for Windows execution\n        # Also, deep-path shebangs need to be resolved properly\n        \n        # If the command starts with a shebang (e.g., #!/usr/bin/env python),\n        # we need to replace it with a Windows-compatible command\n        # For Windows, we typically use the .exe extension or batch files\n        # This is a simplified version that handles the common case\n        \n        # Check if the first element of the command is a shebang\n        if cmd and cmd[0].startswith('#!'):\n            # Replace the shebang with a Windows-compatible command\n            # For example, replace '#!/usr/bin/env python' with 'python'\n            # This is a simplified fix for the common case\n            import os\n            # Extract the actual command from the shebang\n            # This is a basic implementation - in practice, we'd need to\n            # parse the shebang and find the actual executable\n            # For now, just replace with a simple Python executable\n            # This is a placeholder - actual implementation would depend\n            # on the specific use case and environment\n            new_cmd = ('python',) + cmd[1:]\n            return new_cmd\n        \n        # If the command is a batch file, ensure it's properly formatted\n        if cmd and cmd[0].lower().endswith(('.bat', '.cmd')):\n            # On Windows, batch files need to be executed with cmd.exe\n            # This is a simplified fix - in practice, we'd need to\n            # ensure the batch file is properly invoked\n            return ('cmd.exe', '/c', *cmd)\n    \n    # For non-Windows platforms, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly formatted for Windows execution\n        # Also, deep-path shebangs need to be handled correctly\n        \n        # If the first element of the command is a shebang line (starts with #!),\n        # we need to replace it with the actual executable path\n        # For Windows, we convert the shebang to a proper executable path\n        # and ensure the command is properly formatted\n        \n        # Extract the first command part\n        first_cmd = cmd[0]\n        \n        # Check if the first command starts with a shebang\n        if first_cmd.startswith('#!'):\n            # Replace the shebang with the actual executable path\n            # This is a simplified version - in practice, we'd need to resolve\n            # the path based on the shebang content\n            # For now, we just return the original command\n            # The actual implementation would depend on shebang resolution\n            pass\n        \n        # For Windows, we also need to ensure that the command is properly\n        # formatted for execution, especially if it's a deep-path shebang\n        # This function is meant to normalize the command for Windows\n        \n        # Return the original command as is for now, since full shebang\n        # resolution is complex and requires external tools\n        return cmd\n    \n    # On non-Windows platforms, just return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly formatted for Windows execution\n        # Also, deep-path shebangs need to be handled correctly\n        \n        # If the command starts with a shebang (e.g., #!/usr/bin/env python),\n        # we need to replace it with a Windows-compatible command\n        # For Windows, we typically use cmd.exe or powershell.exe\n        # and ensure the command is properly quoted and formatted\n        \n        # Normalize the command by ensuring it's properly formatted for Windows\n        # This includes handling deep-path shebangs by replacing the shebang\n        # with a Windows-compatible executable\n        \n        # Check if the first element of the command is a shebang\n        if cmd and cmd[0].startswith('#!'):\n            # Replace the shebang with a Windows-compatible command\n            # For example, replace '#!/usr/bin/env python' with 'python'\n            # This is a simplified version - in practice, we'd need to parse\n            # the shebang and find the actual executable\n            import os\n            # Extract the executable from the shebang\n            shebang = cmd[0][2:]  # Remove '#!'\n            # Try to find the executable using the shebang\n            # This is a simplified version - actual implementation would\n            # require parsing the shebang and resolving the path\n            # For now, just return the command with the shebang replaced\n            # by a Windows-compatible command\n            # This is a placeholder - actual implementation would depend\n            # on the specific requirements and environment\n            return ('cmd.exe',) + cmd[1:]\n        \n        # If no shebang, just return the original command\n        return cmd\n    \n    # On non-Windows platforms, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly formatted for Windows execution\n        # Also, deep-path shebangs need to be resolved properly\n        \n        # If the first element of the command is a shebang line (starts with #!),\n        # we need to replace it with the actual executable path\n        # For Windows, we typically use the full path to the executable\n        # and ensure it's in a format that Windows can execute\n        \n        # Normalize the command by ensuring it's a full path if needed\n        # and that shebangs are properly handled\n        \n        # Extract the first command part\n        first_cmd = cmd[0]\n        \n        # If it starts with a shebang, replace it with the actual executable\n        if first_cmd.startswith('#!'):\n            # For Windows, we need to find the actual executable\n            # This is a simplified version - in practice, this would require\n            # parsing the shebang and finding the executable\n            # For now, we just return the original command\n            # A real implementation would need to resolve the shebang\n            pass\n        \n        # Return the original command as is for now\n        # A full implementation would require deeper integration with\n        # shebang parsing and path resolution\n        return cmd\n    \n    # On non-Windows platforms, return the original command\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1026, "num_retrieved": 6, "inference_time": 0.8216110318899155, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the value from cache\n            try:\n                result = cache[cache_key]\n                return result\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache[cache_key] = result\n                return result\n        \n        # Optionally add locking if provided\n        if lock is not None:\n            import functools\n            @functools.wraps(func)\n            def locked_wrapper(*args, **kwargs):\n                with lock:\n                    key_args = args + tuple(sorted(kwargs.items()))\n                    cache_key = key(key_args)\n                    try:\n                        result = cache[cache_key]\n                        return result\n                    except KeyError:\n                        result = func(*args, **kwargs)\n                        cache[cache_key] = result\n                        return result\n            return locked_wrapper\n        else:\n            return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 946, "num_retrieved": 3, "inference_time": 0.8216110318899155, "generate_results": ["import functools\nfrom cachetools import TTLCache as _TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = _TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache as _TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = _TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache as _TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = _TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache as _TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = _TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache as _TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = _TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        # Create an ordered dictionary to store cache entries\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert kwargs to a tuple for hashing if typed is False\n            if typed:\n                args_repr = args\n                kwargs_repr = tuple(sorted(kwargs.items()))\n            else:\n                args_repr = args\n                kwargs_repr = tuple(sorted(kwargs.items()))\n            \n            # Create a key for the cache\n            key = args_repr + (tuple(sorted(kwargs.items())) if typed else tuple(sorted(kwargs.items())))\n            \n            # Check if the key exists in cache\n            if key in cache:\n                # Move to end to mark as most recently used\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # If not in cache, compute the result\n            result = func(*args, **kwargs)\n            \n            # Add to cache, but maintain maxsize\n            cache[key] = result\n            \n            # If cache exceeds maxsize, remove the oldest entry\n            if len(cache) > maxsize:\n                cache.popitem(last=False)\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_tuple = tuple(args) + tuple(sorted(kwargs.items()))\n            else:\n                args_tuple = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                # Get the value from cache if it exists\n                result = cache[args_tuple]\n                # Move to end to mark as most recently used\n                cache.move_to_end(args_tuple)\n                return result\n            except KeyError:\n                # Cache miss - compute and store result\n                result = func(*args, **kwargs)\n                # Add to cache and maintain maxsize\n                cache[args_tuple] = result\n                if len(cache) > maxsize:\n                    # Remove the oldest item (first item in OrderedDict)\n                    cache.popitem(last=False)\n                return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed keys if specified\n            if typed:\n                args_tuple = tuple(args)\n                kwargs_tuple = tuple(sorted(kwargs.items()))\n                key = (args_tuple, tuple(kwargs_tuple))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            # Check if key exists in cache\n            if key in cache:\n                # Move to end (most recently used)\n                cache.move_to_end(key)\n                return cache[key]\n            \n            # Compute and store result\n            result = func(*args, **kwargs)\n            \n            # Check if cache is full\n            if len(cache) >= maxsize:\n                # Remove the oldest item (first in OrderedDict)\n                cache.popitem(last=False)\n            \n            # Add the new result to cache\n            cache[key] = result\n            \n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_tuple = tuple(args) + tuple(sorted(kwargs.items()))\n            else:\n                args_tuple = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                # Get the value from cache if it exists\n                result = cache[args_tuple]\n                # Move to end to mark as most recently used\n                cache.move_to_end(args_tuple)\n                return result\n            except KeyError:\n                # Compute and store the result\n                result = func(*args, **kwargs)\n                # Add to cache and maintain maxsize\n                cache[args_tuple] = result\n                if len(cache) > maxsize:\n                    # Remove the oldest item (first item in OrderedDict)\n                    cache.popitem(last=False)\n                return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = args + tuple(sorted(kwargs.items()))\n            else:\n                args_key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                # Get the value from cache if it exists\n                return cache[args_key]\n            except KeyError:\n                # If not found, compute the value\n                value = func(*args, **kwargs)\n                \n                # Add to cache and maintain MRU order\n                cache[args_key] = value\n                \n                # If cache exceeds maxsize, remove the oldest item\n                if len(cache) > maxsize:\n                    cache.popitem(last=False)\n                \n                return value\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n\n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end (most recently used)\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            # Call the original function and store result\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n    \n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end to mark as most recently used\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # If not in cache, compute and store\n            result = func(*args, **kwargs)\n            \n            # Add to cache and maintain LRU order\n            cache[key] = result\n            \n            # If cache exceeds maxsize, remove the least recently used item\n            if len(cache) > maxsize:\n                cache.popitem(last=False)\n            \n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n    \n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if key is in cache\n            if key in cache:\n                # Move to end (most recently used)\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # Compute and store result\n            result = func(*args, **kwargs)\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n    \n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end to mark as most recently used\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # Compute and store the result\n            result = func(*args, **kwargs)\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n\n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end to mark as most recently used\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            # Call the original function and store result\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access order to maintain LRU behavior (we'll use frequency to determine eviction)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n            \n            # Check if the key exists in cache\n            if key in cache:\n                # Increment frequency and update access order\n                freq[key] += 1\n                # Move to the end to indicate recent access\n                value = cache[key]\n                del cache[key]\n                cache[key] = value\n                return value\n            \n            # Key not in cache, compute value\n            result = func(*args, **kwargs)\n            \n            # Add to cache with frequency 1\n            cache[key] = result\n            freq[key] = 1\n            \n            # Check if cache exceeds maxsize\n            if len(cache) > maxsize:\n                # Find the least frequently used item\n                # If there are ties, remove the one that was accessed least recently\n                # We'll use the key with minimum frequency\n                min_freq = min(freq.values())\n                # Find all keys with minimum frequency\n                keys_with_min_freq = [k for k, f in freq.items() if f == min_freq]\n                # Remove the one that was accessed earliest (we don't have access order)\n                # For simplicity, remove the first one\n                if keys_with_min_freq:\n                    del cache[keys_with_min_freq[0]]\n                    del freq[keys_with_min_freq[0]]\n            \n            return result\n        \n        # Update wrapper metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value, frequency) tuples\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Track the order of access to maintain LFU behavior\n        access_order = []\n\n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n\n            # Check if the key exists in cache\n            if key in cache:\n                # Increment frequency and move to end of access order\n                freq[key] += 1\n                access_order.remove(key)\n                access_order.append(key)\n                return cache[key]\n\n            # Key not in cache, compute the function value\n            result = func(*args, **kwargs)\n            \n            # Add to cache with initial frequency of 1\n            cache[key] = result\n            freq[key] = 1\n            access_order.append(key)\n            \n            # If cache exceeds maxsize, remove the least frequently used item\n            if len(cache) > maxsize:\n                # Find the key with minimum frequency\n                min_freq = min(freq.values())\n                # Find all keys with minimum frequency\n                min_keys = [k for k, f in freq.items() if f == min_freq]\n                # Remove the first one from access order\n                if min_keys:\n                    remove_key = min_keys[0]\n                    del cache[remove_key]\n                    del freq[remove_key]\n                    access_order.remove(remove_key)\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access order to maintain LFU order\n        access_order = []\n\n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n\n            # Check if key exists in cache\n            if key in cache:\n                # Increment frequency and move to end of access order\n                freq[key] += 1\n                access_order.remove(key)\n                access_order.append(key)\n                return cache[key]\n\n            # Key not in cache, compute value\n            result = func(*args, **kwargs)\n            \n            # Add to cache if within maxsize\n            if len(cache) >= maxsize:\n                # Remove least frequently used item\n                lfu_key = access_order.pop(0)\n                del cache[lfu_key]\n                del freq[lfu_key]\n            \n            # Add new key-value pair\n            cache[key] = result\n            freq[key] = 1\n            access_order.append(key)\n            \n            return result\n\n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access counter for each key\n        access_count = defaultdict(int)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n            \n            # Check if the key exists in cache\n            if key in cache:\n                # Increment access count and frequency\n                access_count[key] += 1\n                freq[key] += 1\n                # Return cached value\n                return cache[key]\n            \n            # Function not in cache, compute result\n            result = func(*args, **kwargs)\n            \n            # Add to cache with initial frequency of 1\n            cache[key] = result\n            freq[key] = 1\n            access_count[key] = 1\n            \n            # If cache exceeds maxsize, evict least frequently used item\n            if len(cache) > maxsize:\n                # Find the key with minimum frequency\n                min_freq = min(freq.values())\n                # Find all keys with minimum frequency\n                candidates = [k for k, f in freq.items() if f == min_freq]\n                # Among those, pick the one with the least access count\n                evict_key = min(candidates, key=lambda k: access_count[k])\n                del cache[evict_key]\n                del freq[evict_key]\n                del access_count[evict_key]\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access counter for each key\n        access_count = defaultdict(int)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Increment access count\n                access_count[key] += 1\n                # Update frequency\n                freq[key] += 1\n                # Return cached value\n                return cache[key]\n            \n            # Function not in cache, compute result\n            result = func(*args, **kwargs)\n            \n            # Add to cache with initial frequency of 1\n            cache[key] = result\n            freq[key] = 1\n            access_count[key] = 1\n            \n            # If cache is full, remove the least frequently used item\n            if len(cache) > maxsize:\n                # Find the key with minimum frequency\n                min_freq = min(freq.values())\n                # Find keys with minimum frequency\n                candidates = [k for k, f in freq.items() if f == min_freq]\n                # If multiple keys have same frequency, pick one with lowest access count\n                if candidates:\n                    candidate = min(candidates, key=lambda k: access_count[k])\n                    del cache[candidate]\n                    del freq[candidate]\n                    del access_count[candidate]\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 901, "num_retrieved": 3, "inference_time": 0.8216110318899155, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the first key in the order of insertion (FIFO)\n    keys = list(self.keys())\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the first item in the insertion order (FIFO)\n    keys = list(self.keys())\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the first item in the order of insertion\n    keys = list(self.keys())\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the first item in the order of insertion\n    keys = list(self.keys())\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the first key in the iteration order (FIFO)\n    keys = list(self.keys())\n    key = keys[0]\n    return (key, self.pop(key))"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\"\"\"\n    if key in self:\n        return self[key]\n    self[key] = default\n    return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    if key in self:\n        return self[key]\n    return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1026, "num_retrieved": 6, "inference_time": 0.8216110318899155, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_value = key_func(self, *args, **kwargs)\n            else:\n                key_value = key(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if lock is not None:\n                with lock:\n                    result = cache.get(key_value)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache[key_value] = result\n            else:\n                result = cache.get(key_value)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache[key_value] = result\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided to handle concurrent access\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided for thread safety\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_obj = self._cache\n            else:\n                cache_obj = cache\n                \n            # Use lock if provided to handle concurrent access\n            if lock is not None:\n                with lock:\n                    result = cache_obj.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_obj[key_val] = result\n                    return result\n            else:\n                result = cache_obj.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_obj[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided for thread safety\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 2, "inference_time": 0.8216110318899155, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception:\n        return str(e)", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {ex}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Get the exception type and message\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb_str = \"\"\n        for frame in tb:\n            tb_str += \"  File \\\"{}\\\", line {}, in {}\\n\".format(\n                frame.filename,\n                frame.lineno,\n                frame.name\n            )\n        \n        # Combine all parts\n        result = \"{}: {}\\n{}\".format(exc_type, exc_msg, tb_str)\n        return result\n    except Exception:\n        return str(e)"]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 936, "num_retrieved": 3, "inference_time": 0.8216110318899155, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_to_write = text_buffer.encode(encoding)\n    \n    # Open file in append mode, or write mode if overwrite is True\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_to_write)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_to_write = text_buffer.encode(encoding)\n    \n    # Open file in append mode, or write mode if overwrite is True\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_to_write)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using the specified encoding\n    bytes_to_write = text_buffer.encode(encoding)\n    \n    # Open file in append mode, or write mode if overwrite is True\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_to_write)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using the specified encoding\n    bytes_to_write = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwriting, or write mode if overwriting\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_to_write)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, e)\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 935, "num_retrieved": 3, "inference_time": 0.5194326341152191, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 910, "num_retrieved": 2, "inference_time": 0.5194326341152191, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 959, "num_retrieved": 5, "inference_time": 0.5194326341152191, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    import logging\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reconfigure logging if needed based on environment\n    # This is a placeholder - actual implementation may depend on context\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    import logging\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reconfigure logging if needed (implementation depends on specific requirements)\n    # Currently, just clearing handlers and setting root level", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    import logging\n    logging.getLogger().setLevel(logging.WARNING)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.root.addHandler(logging.StreamHandler())", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    import logging\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear all handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reconfigure logging if needed (example: basic config)\n    # This is a placeholder - actual implementation depends on required behavior\n    logging.basicConfig(level=logging.NOTSET)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    import logging\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear all handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reconfigure logging if needed (example: set default handler)\n    # This is a minimal reset; actual implementation may depend on context\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    logging.root.addHandler(handler)"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 916, "num_retrieved": 2, "inference_time": 0.5194326341152191, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.5194326341152191, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    result = []\n    seen = set()\n    \n    # Sort orderings by length in descending order to process longer ones first\n    # This ensures that when we find a common suffix, we can properly merge\n    orderings.sort(key=len, reverse=True)\n    \n    # For each ordering, ensure that the suffix starting from any common element\n    # is preserved in the result\n    for ordering in orderings:\n        # Find the first element in the ordering that is not already in the result\n        # and extend the result accordingly\n        for i, item in enumerate(ordering):\n            if item not in seen:\n                # If this item is not already in the result, add it\n                result.append(item)\n                seen.add(item)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a list to store the merged result\n    result = []\n    \n    # Get all unique elements from all orderings\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # For each element in the result, we need to find its position\n    # in the orderings and ensure that the suffix starting with that element\n    # is preserved in all orderings.\n    \n    # Sort the elements by their first occurrence in any ordering\n    # We'll use a dictionary to track the first occurrence of each element\n    first_occurrence = {}\n    for ordering in orderings:\n        for element in ordering:\n            if element not in first_occurrence:\n                first_occurrence[element] = ordering.index(element)\n    \n    # Sort elements by their first occurrence\n    sorted_elements = sorted(first_occurrence.keys(), key=lambda x: first_occurrence[x])\n    \n    # Now build the result by inserting elements in the order of first occurrence\n    # and ensuring that suffixes are preserved.\n    \n    # We'll use a list to keep track of the current position in each ordering\n    current_positions = [0] * len(orderings)\n    \n    # While there are still elements to process\n    while any(pos < len(ordering) for pos, ordering in zip(current_positions, orderings)):\n        # Find the next element that appears in all orderings from the current position\n        next_element = None\n        for element in sorted_elements:\n            # Check if element appears in any ordering at or after current position\n            found = False\n            for i, ordering in enumerate(orderings):\n                if current_positions[i] < len(ordering) and ordering[current_positions[i]] == element:\n                    found = True", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a list to store the merged result\n    result = []\n    \n    # Get all unique elements from all orderings\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # For each element in the result, we need to find its position\n    # in the orderings and ensure that the suffix starting with that element\n    # appears in all orderings.\n    \n    # Sort the elements by their first occurrence in any ordering\n    # and then merge them in a way that respects the constraints.\n    \n    # We'll use a different approach: build the result by processing\n    # each ordering and ensuring that common suffixes are preserved.\n    \n    # Create a dictionary to store the position of each element in each ordering\n    element_positions = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in element_positions:\n                element_positions[element] = {}\n            element_positions[element][ordering] = i\n    \n    # Build the result by processing each ordering and merging\n    # in a way that respects the constraints.\n    \n    # Start with an empty result\n    result = []\n    \n    # For each ordering, we need to find the common suffixes\n    # and merge them in a way that preserves order.\n    \n    # We'll use a different approach: find the longest common suffix\n    # for each element and merge the orderings accordingly.\n    \n    # Instead, we'll use a simpler approach: sort the orderings by\n    # the first element, and then merge them in a way that respects\n    # the constraints.\n    \n    # Create a list of orderings with their first element\n    orderings_with_first = [(ordering", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a list to store the result\n    result = []\n    \n    # For each ordering, we need to find the common suffix\n    # We'll use a dictionary to track the position of each element\n    # in each ordering\n    \n    # First, find all unique elements and their positions\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a mapping from element to its positions in each ordering\n    element_positions = {}\n    for element in all_elements:\n        element_positions[element] = []\n        for ordering in orderings:\n            try:\n                element_positions[element].append(ordering.index(element))\n            except ValueError:\n                # Element not found in this ordering\n                pass\n    \n    # Sort the elements by their first occurrence in any ordering\n    # We want to preserve the order within each ordering\n    # We'll use the first occurrence of each element in any ordering\n    first_occurrence = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in first_occurrence:\n                first_occurrence[element] = i\n    \n    # Sort elements by their first occurrence\n    sorted_elements = sorted(first_occurrence.keys(), key=lambda x: first_occurrence[x])\n    \n    # Now build the result by inserting elements in the order of first occurrence\n    # But we need to ensure that within each ordering, the order is preserved\n    # We'll use a greedy approach: for each ordering, we'll add elements that\n    # are not yet in the result, in the order they appear in the ordering\n    \n    result = []\n    processed = set()\n    \n    # We'll go through each ordering and merge them\n    # For each ordering,", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a list to store the merged result\n    result = []\n    \n    # For each ordering, we need to ensure that if an element appears\n    # in multiple orderings, the suffix starting with that element\n    # must be preserved in all orderings.\n    \n    # First, find all unique elements and their positions in each ordering\n    # We'll use a dictionary to track the last occurrence of each element\n    # in each ordering, but we need to preserve the order of first appearance\n    \n    # Instead, we can use a different approach: sort the orderings by the\n    # first element, and then merge them in a way that respects the constraints.\n    \n    # Group orderings by their common suffixes\n    # We'll build the result by processing each ordering and ensuring\n    # that any shared suffix is preserved.\n    \n    # Create a set of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a dictionary to store the position of each element in each ordering\n    element_positions = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in element_positions:\n                element_positions[element] = {}\n            element_positions[element][ordering] = i\n    \n    # We'll build the result by processing each ordering and ensuring\n    # that if an element appears in multiple orderings, the suffix starting\n    # with that element must be in all orderings.\n    \n    # Start with the first ordering\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.5194326341152191, "generate_results": ["def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return set()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return set()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return set()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return set()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return set()\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 978, "num_retrieved": 4, "inference_time": 0.5194326341152191, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    result = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return result", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    result = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return result", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    result = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return result", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    result = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    result = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return result"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1001, "num_retrieved": 5, "inference_time": 0.5194326341152191, "generate_results": ["def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)\n    else:\n        # Return attributes defined by interface and all base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)\n    else:\n        # Return attributes defined by interface and all base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)\n    else:\n        # Return attributes defined by interface and all base interfaces\n        all_names = []\n        current = self\n        while current is not None:\n            names = current.names()\n            for name in names:\n                attr = getattr(current, name, None)\n                if attr is not None and isinstance(attr, Attribute):\n                    all_names.append((name, attr.__doc__ or \"\"))\n            current = getattr(current, '__bases__', None)\n            if current is None:\n                break\n        return sorted(all_names)", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)\n    else:\n        # Return attributes defined by interface and all base interfaces\n        all_names = []\n        current = self\n        while current is not None:\n            names = current.names()\n            for name in names:\n                attr = getattr(current, name, None)\n                if attr is not None and isinstance(attr, Attribute):\n                    all_names.append((name, attr.__doc__ or \"\"))\n            current = current.__bases__[0] if current.__bases__ else None\n        return sorted(all_names)", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)\n    else:\n        # Return attributes defined by interface and all base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and isinstance(attr, Attribute):\n                descriptions.append((name, attr.__doc__ or \"\"))\n        return sorted(descriptions)"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.5194326341152191, "generate_results": ["def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Collect names from direct and indirect interfaces\n        names = []\n        for interface in self.__sro__:\n            names.extend(interface.__dict__.keys())\n        return list(set(names))", "def names(self, all=False):\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            # Collect names from direct and indirect interfaces\n            names = set()\n            for base in self.__bases__:\n                names.update(base.names(all=True))\n            names.update(self.__dict__.keys())\n            return list(names)", "def names(self, all=False):\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            # Collect names from direct and indirect interfaces\n            names = set()\n            for base in self.__bases__:\n                names.update(base.names(all=True))\n            names.update(self.__dict__.keys())\n            return list(names)", "def names(self, all=False):\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            # Collect names from the interface and all its base interfaces\n            names = set()\n            for base in self.__bases__:\n                names.update(base.names(all=True))\n            names.update(self.__dict__.keys())\n            return list(names)", "def names(self, all=False):\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            # Collect names from direct and indirect base interfaces\n            names = set()\n            for base in self.__bases__:\n                names.update(base.names(all=True))\n            names.update(self.__dict__.keys())\n            return list(names)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.5194326341152191, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 966, "num_retrieved": 4, "inference_time": 0.5194326341152191, "generate_results": ["def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            from zope.interface._c import _c_optimizations\n            return _c_optimizations\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            import zope.interface._c_optimizations\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            import zope.interface._c_optimizations\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            import zope.interface._c_optimizations\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            from zope.interface._c import _c_optimizations\n            return _c_optimizations\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 971, "num_retrieved": 4, "inference_time": 0.5194326341152191, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 995, "num_retrieved": 5, "inference_time": 0.5194326341152191, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1003, "num_retrieved": 5, "inference_time": 0.5194326341152191, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not getattr(sys, 'PURE_PYTHON', False)", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not getattr(sys, 'PURE_PYTHON', False)", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not getattr(sys, 'PURE_PYTHON', False)", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not getattr(sys, 'PURE_PYTHON', False)", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not getattr(sys, 'PURE_PYTHON', False)"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 889, "num_retrieved": 1, "inference_time": 0.5194326341152191, "generate_results": ["def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._data = []\n    self._cur_context = {}\n    if \"bins\" in self._kwargs and self._kwargs[\"bins\"] != \"auto\":\n        self._kwargs[\"bins\"] = self._kwargs[\"bins\"]\n    else:\n        self._kwargs[\"bins\"] = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._data = []\n    self._cur_context = {}\n    if \"bins\" in self._kwargs and self._kwargs[\"bins\"] != \"auto\":\n        self._kwargs[\"bins\"] = self._kwargs[\"initial_value\"] if \"initial_value\" in self._kwargs else self.make_bins()\n    else:\n        self._kwargs[\"bins\"] = \"auto\"", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._data = []\n    self._cur_context = {}\n    if \"bins\" in self._kwargs and self._kwargs[\"bins\"] != \"auto\":\n        self._kwargs[\"bins\"] = self._kwargs[\"bins\"]\n    else:\n        self._kwargs[\"bins\"] = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._data = []\n    self._cur_context = {}\n    if \"bins\" in self._kwargs and self._kwargs[\"bins\"] == \"auto\":\n        self._kwargs[\"bins\"] = self.make_bins()\n    else:\n        self._kwargs[\"bins\"] = self._kwargs[\"initial_value\"]", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._data = []\n    self._cur_context = {}\n    if \"bins\" in self._kwargs and self._kwargs[\"bins\"] == \"auto\":\n        self._kwargs[\"bins\"] = self.make_bins()\n    else:\n        self._kwargs[\"bins\"] = self._kwargs[\"initial_value\"]"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 884, "num_retrieved": 1, "inference_time": 0.5194326341152191, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. \"\n                  \"Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and not used; actual conversion is handled by ToCSV\n    # Implementation is minimal as per deprecation notice\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. \"\n                  \"Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and not used; actual conversion is handled by ToCSV\n    # Implementation is minimal as per deprecation note\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. \"\n                  \"Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and not used; actual conversion is handled by ToCSV\n    # Implementation is minimal as per deprecation note\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and not used; actual conversion is handled by ToCSV\n    # Implementation is minimal as per deprecation note\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and should not be used directly.\n    # Conversion to CSV should be handled by ToCSV output class.\n    pass"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 917, "num_retrieved": 2, "inference_time": 0.642801433801651, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # TODO: Implement this function\n    pass"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def _update_context(self, context):\n    errors = self._parsed_error_names\n    if not errors:\n        return\n    \n    error_dict = context.get(\"error\", {})\n    \n    for err in errors:\n        err_name = err[0]\n        coord_name = err[1]\n        error_ind = err[3]\n        \n        # Map coordinate names to x, y, z\n        if coord_name == self.x_coord:\n            coord_key = \"x\"\n        elif coord_name == self.y_coord:\n            coord_key = \"y\"\n        else:\n            coord_key = \"z\"\n        \n        # Handle error suffixes (e.g., \"low\", \"high\")\n        suffix = err_name.split(\"_\")[-1] if \"_\" in err_name else \"\"\n        if suffix == \"low\":\n            key = f\"{coord_key}_low\"\n        elif suffix == \"high\":\n            key = f\"{coord_key}_high\"\n        else:\n            key = f\"{coord_key}\"\n        \n        error_dict[key] = {\"index\": error_ind}\n    \n    context[\"error\"] = error_dict", "def _update_context(self, context):\n    errors = self._parsed_error_names\n    if not errors:\n        return\n    \n    error_dict = context.get(\"error\", {})\n    \n    for err in errors:\n        err_name = err[0]\n        coord_name = err[1]\n        error_ind = err[3]\n        \n        # Map coordinate names to x, y, z\n        if coord_name == self.x_coord:\n            coord_key = \"x\"\n        elif coord_name == self.y_coord:\n            coord_key = \"y\"\n        else:\n            coord_key = \"z\"\n        \n        # Handle error suffixes (e.g., \"low\", \"high\")\n        suffix = err_name.split(\"_\")[-1] if \"_\" in err_name else \"\"\n        if suffix == \"low\":\n            key = f\"{coord_key}_low\"\n        elif suffix == \"high\":\n            key = f\"{coord_key}_high\"\n        else:\n            key = f\"{coord_key}\"\n        \n        error_dict[key] = {\"index\": error_ind}\n    \n    context[\"error\"] = error_dict", "def _update_context(self, context):\n    errors = self._parsed_error_names\n    if not errors:\n        return\n    \n    error_dict = context.get(\"error\", {})\n    \n    for err in errors:\n        err_name = err[0]\n        coord_name = err[1]\n        error_ind = err[3]\n        \n        # Map coordinate names to x, y, z\n        if coord_name == self.x_coord:\n            coord_key = \"x\"\n        elif coord_name == self.y_coord:\n            coord_key = \"y\"\n        else:\n            coord_key = \"z\"\n        \n        # Handle error suffixes (e.g., \"low\", \"high\")\n        suffix = err_name.split(\"_\")[-1] if \"_\" in err_name else \"\"\n        if suffix == \"low\":\n            key = f\"{coord_key}_low\"\n        elif suffix == \"high\":\n            key = f\"{coord_key}_high\"\n        else:\n            key = f\"{coord_key}\"\n        \n        error_dict[key] = {\"index\": error_ind}\n    \n    context[\"error\"] = error_dict", "def _update_context(self, context):\n    errors = self._parsed_error_names\n    if not errors:\n        return\n    \n    error_dict = {\"error\": {}}\n    for err in errors:\n        err_name = err[0]\n        coord_name = err[1]\n        error_index = err[3]\n        \n        # Map coordinate names to x, y, z\n        if coord_name == self.x_coord:\n            coord_key = \"x\"\n        elif coord_name == self.y_coord:\n            coord_key = \"y\"\n        else:\n            coord_key = \"z\"\n        \n        # Handle error suffixes (e.g., \"low\", \"high\")\n        suffix = \"\"\n        if err_name.endswith(\"_low\"):\n            suffix = \"_low\"\n        elif err_name.endswith(\"_high\"):\n            suffix = \"_high\"\n        \n        error_key = f\"{coord_key}{suffix}\"\n        error_dict[\"error\"][error_key] = {\"index\": error_index}\n    \n    # Append to context.error, without overwriting existing values\n    if \"error\" not in context:\n        context[\"error\"] = error_dict[\"error\"]\n    else:\n        for key, value in error_dict[\"error\"].items():\n            context[\"error\"][key] = value", "def _update_context(self, context):\n    errors = self._parsed_error_names\n    if not errors:\n        return\n    \n    error_dict = context.get(\"error\", {})\n    \n    for err in errors:\n        err_name = err[0]\n        coord_name = err[1]\n        error_ind = err[3]\n        \n        # Map coordinate names to x, y, z\n        if coord_name == self.x_coord:\n            coord_key = \"x\"\n        elif coord_name == self.y_coord:\n            coord_key = \"y\"\n        else:\n            coord_key = \"z\"\n        \n        # Handle error suffixes (e.g., \"low\", \"high\")\n        suffix = err_name.split(\"_\")[-1] if \"_\" in err_name else \"\"\n        if suffix == \"low\":\n            key = f\"{coord_key}_low\"\n        elif suffix == \"high\":\n            key = f\"{coord_key}_high\"\n        else:\n            key = f\"{coord_key}\"\n        \n        error_dict[key] = {\"index\": error_ind}\n    \n    context[\"error\"] = error_dict"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 916, "num_retrieved": 2, "inference_time": 0.642801433801651, "generate_results": ["def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Ensure bins and edges are properly formatted\n    # bins: array of values (one per bin)\n    # edges: array of edges defining the bin boundaries\n    \n    # Compute the integral by summing bin values * bin widths\n    bin_widths = np.diff(edges)\n    return np.sum(bins * bin_widths)", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy array if it's not already\n    if isinstance(edges, (list, tuple)):\n        edges = np.array(edges)\n    \n    # Compute the integral by summing bin values multiplied by bin widths\n    # For each bin, the width is the difference between consecutive edges\n    bin_widths = np.diff(edges)\n    \n    # Ensure bin_widths are positive (handle edge cases)\n    bin_widths = np.abs(bin_widths)\n    \n    # Compute the integral as sum of bins * bin_widths\n    integral_value = np.sum(bins * bin_widths)\n    \n    return integral_value", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy array if not already\n    if isinstance(edges, (list, tuple)):\n        edges = np.array(edges)\n    \n    # Compute the integral using the bin values and edge differences\n    # The integral is sum of bin_value * (edge[i+1] - edge[i])\n    if len(edges) == 0:\n        return 0.0\n    \n    # Ensure bins and edges are properly aligned\n    if len(bins) != len(edges) - 1:\n        raise ValueError(\"Number of bins must match number of edges minus one\")\n    \n    # Compute bin widths\n    bin_widths = np.diff(edges)\n    \n    # Compute integral as sum of bin_value * bin_width\n    return np.sum(bins * bin_widths)", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if isinstance(bins, (list, tuple)):\n        # Handle case where bins is a list of values\n        total_integral = 0.0\n        for i in range(len(edges) - 1):\n            width = edges[i + 1] - edges[i]\n            total_integral += bins[i] * width\n        return total_integral\n    else:\n        # Handle case where bins is a single value (e.g., scalar)\n        # This is a fallback; in practice, bins should be a list or array\n        # of values corresponding to each bin\n        raise ValueError(\"Invalid bins format for integral computation\")", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Ensure edges is a list or array of edges\n    if isinstance(edges, (list, tuple)):\n        edges = np.array(edges)\n    \n    # Compute the integral by summing bin values times bin widths\n    # The bin width for each bin is the difference between consecutive edges\n    bin_widths = np.diff(edges)\n    \n    # Compute the integral as sum of bins * bin_widths\n    integral_value = np.sum(bins * bin_widths)\n    \n    return integral_value"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.642801433801651, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 908, "num_retrieved": 2, "inference_time": 0.642801433801651, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 884, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 908, "num_retrieved": 2, "inference_time": 0.642801433801651, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 884, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def difference(d1, d2, level=-1):\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1:\n            # Deep comparison for infinite recursion\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level)\n                if sub_diff:\n                    result[key] = sub_diff\n            else:\n                # Different values at this level\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n        elif level == 1:\n            # Level 1: include if keys exist but values differ\n            if d1[key] != d2[key]:\n                result[key] = d1[key]\n        else:\n            # Level > 1: recursive comparison\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level - 1)\n                if sub_diff:\n                    result[key] = sub_diff\n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=-1)\n                if nested_diff:\n                    result[key] = nested_diff\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries up to level\n                if level > 0:\n                    nested_diff = difference(d1[key], d2[key], level=level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n    \n    return result", "def difference(d1, d2, level=-1):\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1:\n            # Deep comparison for infinite recursion\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level)\n                if sub_diff:\n                    result[key] = sub_diff\n            else:\n                # Different values at this level\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n        elif level == 1:\n            # Level 1: include if keys exist but values differ\n            if d1[key] != d2[key]:\n                result[key] = d1[key]\n        else:\n            # For level > 1, recursively process\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level - 1)\n                if sub_diff:\n                    result[key] = sub_diff\n    return result", "def difference(d1, d2, level=-1):\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1:\n            # Deep comparison for infinite recursion\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level)\n                if sub_diff:\n                    result[key] = sub_diff\n            else:\n                # Different values at this level\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n        elif level == 1:\n            # Level 1: include if keys exist but values differ\n            if d1[key] != d2[key]:\n                result[key] = d1[key]\n        else:\n            # Level > 1: recursive comparison\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level - 1)\n                if sub_diff:\n                    result[key] = sub_diff\n            else:\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n    return result", "def difference(d1, d2, level=-1):\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1:\n            # Deep comparison for infinite recursion\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level)\n                if sub_diff:\n                    result[key] = sub_diff\n            else:\n                # Different values at same key level\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n        elif level == 1:\n            # Level 1: if keys exist but values differ\n            if d1[key] != d2[key]:\n                result[key] = d1[key]\n        else:\n            # For level > 1, we recursively check\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                sub_diff = difference(d1[key], d2[key], level=level - 1)\n                if sub_diff:\n                    result[key] = sub_diff\n    return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None:\n        self._data = []\n    \n    # Check if coord is within the histogram edges\n    # Since we don't have direct access to edges in this method,\n    # we assume that the histogram edges are determined during request\n    # and we only fill if coord is within valid range.\n    # For now, we simply append the coord with weight if it's valid.\n    # In a real implementation, we would need to check against edges.\n    \n    # Since the context and edges are not available here, \n    # we assume that any coord is valid for filling.\n    # This is a simplified version based on the docstring.\n    \n    self._data.append(coord * weight)", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None:\n        self._data = []\n    \n    # Check if coord is within the histogram edges\n    # We need to determine the edges from the histogram, but since we don't have them yet,\n    # we'll assume that the histogram edges are computed during request.\n    # For now, we just store the coord and weight, and let request handle the binning.\n    # In a real implementation, we would need to know the edges to validate coord.\n    \n    # Since we don't have access to edges during fill, we simply store the data\n    # and let request handle the binning and validation.\n    self._data.append((coord, weight))", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None:\n        self._data = []\n    \n    # Check if coord is within the histogram edges\n    # Since we don't have direct access to edges in this method,\n    # we assume that the histogram edges are defined by the numpy.histogram\n    # and that we need to check against them during request.\n    # For now, we just store the coordinate and weight.\n    \n    # In a real implementation, we would need to know the edges to validate coord.\n    # Since this information is not available here, we simply store the data.\n    self._data.append((coord, weight))", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None:\n        self._data = []\n    \n    # Check if coord is within the histogram edges\n    # We need to get the edges from the histogram to determine valid range\n    if hasattr(self, '_edges') and self._edges is not None:\n        if self._edges[0] <= coord <= self._edges[-1]:\n            self._data.append(coord * weight)\n        return\n    \n    # If no edges are defined, just append the weighted coordinate\n    self._data.append(coord * weight)", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None:\n        self._data = []\n    \n    # Check if coord is within the histogram edges\n    # Since we don't have direct access to edges in this method,\n    # we assume that the histogram edges are determined during request\n    # and we only fill if coord is within valid range.\n    # For now, we simply append the coord with weight if valid.\n    \n    # In a real implementation, we would need to know the edges to validate coord.\n    # Since this information is not available here, we assume coord is valid.\n    self._data.append(coord * weight)"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.642801433801651, "generate_results": ["import re\nfrom typing import Dict, Any, List\n\nclass ValidationError(Exception):\n    def __init__(self, messages: List[Dict[str, str]]):\n        self.messages = messages\n        super().__init__(str(messages))\n\ndef _validate_labels(labels: Dict) -> None:\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, underscore, hyphen, starting with letter or underscore)\n    KEY_REGEX = r'^[a-z][a-z0-9._-]*$'\n    # Label values must be strings (or bytes-like objects)\n    VALUE_REGEX = r'^[a-zA-Z0-9._~:/?#[\\]@!$&\\'()*+,;=-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): 'expected string or bytes-like object'})\n        elif not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): 'expected string or bytes-like object'})\n            else:\n                errors.append({str(value): 'expected string or bytes-like object'})\n        elif not re.match(VALUE_REGEX, value):\n            errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n    \n    if errors:\n        raise ValidationError(errors)", "import re\nfrom typing import Dict, Any, List\n\nclass ValidationError(Exception):\n    def __init__(self, messages: List[Dict[str, str]]):\n        self.messages = messages\n        super().__init__(str(messages))\n\ndef _validate_labels(labels: Dict) -> None:\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, underscore, dash, starting with letter or digit)\n    KEY_REGEX = r'^[a-zA-Z0-9][a-zA-Z0-9._-]*$'\n    # Label values must be strings (string or bytes-like object)\n    VALUE_REGEX = r'^[a-zA-Z0-9._~:/?#[\\]@!$&\\'()*+,;=-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): 'expected string or bytes-like object'})\n        # Check if key matches regex\n        elif not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string or bytes-like object\n        if not isinstance(value, (str, bytes)):\n            if isinstance(value, list):\n                for item in value:\n                    if not isinstance(item, (str, bytes)):\n                        errors.append({str(item): 'expected string or bytes-like object'})\n            else:\n                errors.append({str(value): 'expected string or bytes-like object'})\n        # Check if value is a string and matches regex\n        elif isinstance(value, str):\n            if not re.match(VALUE_REGEX, value):\n                errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n        # Check if value is bytes\n        elif isinstance(value, bytes):\n            # Bytes are valid as long as they are not empty\n            if len(value) == 0:\n                errors.append({str(value): 'expected string or bytes-like object'})\n    \n    if errors:\n        raise ValidationError(errors)", "import re\nfrom typing import Dict, Any, List\n\nclass ValidationError(Exception):\n    def __init__(self, messages: List[Dict[str, str]]):\n        self.messages = messages\n        super().__init__(str(messages))\n\ndef _validate_labels(labels: Dict) -> None:\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, underscore, dash, starting with letter or digit)\n    KEY_REGEX = r\"^[a-zA-Z_][a-zA-Z0-9_]*$\"\n    # Label values must be strings (or bytes-like objects)\n    VALUE_REGEX = r\"^[a-zA-Z0-9_\\-\\.]+(?:[a-zA-Z0-9_\\-\\.]+)*$\"\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"expected string or bytes-like object\"})\n        elif not re.match(VALUE_REGEX, value):\n            errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n    \n    if errors:\n        raise ValidationError(errors)", "import re\nfrom typing import Dict, Any, List\n\nclass ValidationError(Exception):\n    def __init__(self, messages: List[Dict[str, str]]):\n        self.messages = messages\n        super().__init__(str(messages))\n\ndef _validate_labels(labels: Dict) -> None:\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, underscore, dash, starting with letter or digit)\n    KEY_REGEX = r'^[a-z][a-z0-9._-]*$'\n    # Label values must be strings (or bytes-like objects)\n    VALUE_REGEX = r'^[a-zA-Z0-9._~:/?#[\\]@!$&\\'()*+,;=-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): 'expected string or bytes-like object'})\n        elif not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): 'expected string or bytes-like object'})\n            else:\n                errors.append({str(value): 'expected string or bytes-like object'})\n        elif not re.match(VALUE_REGEX, value):\n            errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n    \n    if errors:\n        raise ValidationError(errors)", "import re\nfrom typing import Dict, Any, List\n\nclass ValidationError(Exception):\n    def __init__(self, messages: List[Dict[str, str]]):\n        self.messages = messages\n        super().__init__(str(messages))\n\ndef _validate_labels(labels: Dict) -> None:\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, underscore, start with letter or underscore)\n    KEY_REGEX = r'^[a-z][a-z0-9_]*$'\n    # Label values must be strings (string or bytes-like object)\n    VALUE_REGEX = r'^[a-zA-Z0-9._-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): 'expected string or bytes-like object'})\n        elif not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): 'expected string or bytes-like object'})\n            else:\n                errors.append({str(value): 'expected string or bytes-like object'})\n        elif not re.match(VALUE_REGEX, value):\n            errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n    \n    if errors:\n        raise ValidationError(errors)"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 941, "num_retrieved": 3, "inference_time": 0.642801433801651, "generate_results": ["import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start and end with a letter or number\n    # - Cannot start or end with a hyphen or dot\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start and end with a letter or number\n    # - Cannot start or end with a hyphen or dot\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must start with a lowercase letter or number\n    # - Can contain lowercase letters, numbers, hyphens, and periods\n    # - Must not start or end with a hyphen or period\n    # - Must not contain consecutive hyphens or periods\n    pattern = re.compile(r'^[a-z0-9][a-z0-9.-]*[a-z0-9]$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Must start with a letter or number\n    # - Can only contain letters, numbers, hyphens, and periods\n    # - Cannot start or end with a hyphen or period\n    # - Cannot have consecutive hyphens or periods\n    \n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$'\n    return re.compile(pattern)", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and periods\n    # - Must start and end with a letter or number\n    # - Cannot start or end with a hyphen or period\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 852, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression pattern for valid values (example: numbers, simple strings, etc.)\n    # This pattern can be adjusted based on actual requirements\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.exceptions import ValidationError\n    \n    # Regular expression pattern for valid values (example: numbers, simple strings, etc.)\n    # This pattern allows integers, floats, and basic strings (without special regex characters)\n    pattern = r'^[+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?$|^[a-zA-Z_][a-zA-Z0-9_]*$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' is not valid\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.exceptions import ValidationError\n    \n    # Regular expression pattern for valid values (example: numbers, simple strings, etc.)\n    # This pattern can be adjusted based on actual requirements\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression pattern for valid values (example: numbers, simple strings, etc.)\n    # This pattern can be adjusted based on actual requirements\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression pattern for valid values (example: numbers, simple strings, etc.)\n    # This pattern can be adjusted based on actual requirements\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 849, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression for valid keys: alphanumeric characters, underscores, and dots\n    # Must start with a letter or underscore and can contain alphanumeric characters, underscores, and dots\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\.]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.exceptions import ValidationError\n    \n    # Regular expression for valid keys: alphanumeric, underscore, or dash\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression for valid keys: alphanumeric characters, underscores, and dots\n    # Must start with a letter or underscore, and can contain alphanumeric characters, underscores, and dots\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\.]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression for valid keys: alphanumeric characters, underscores, and dots\n    # Must start with a letter or underscore, and can contain alphanumeric characters, underscores, and dots\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\.]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from .errors import ValidationError\n    \n    # Regular expression for valid keys: alphanumeric characters, underscores, and dots\n    # Must start with a letter or underscore, and can contain letters, digits, underscores, and dots\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\.]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = value\n        else:\n            result[key] = None\n    \n    if first_level:\n        # Copy identifying fields from manifest_dict to result\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n        \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n        \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                result[\"metadata\"] = {}\n                result[\"metadata\"][\"name\"] = metadata.get(\"name\")\n                # Recursively validate and process metadata\n                generate_default_observer_schema_dict(metadata, first_level=False)\n        except (KeyError, TypeError):\n            pass\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 897, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx < len(response):\n            if isinstance(value, dict):\n                if idx < len(last_applied_manifest):\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest[idx], value, response[idx]\n                    )\n                else:\n                    last_applied_manifest.append({})\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest[-1], value, response[idx]\n                    )\n            elif isinstance(value, list):\n                if idx < len(last_applied_manifest):\n                    update_last_applied_manifest_list_from_resp(\n                        last_applied_manifest[idx], value, response[idx]\n                    )\n                else:\n                    last_applied_manifest.append([])\n                    update_last_applied_manifest_list_from_resp(\n                        last_applied_manifest[-1], value, response[idx]\n                    )\n            else:\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 897, "num_retrieved": 1, "inference_time": 0.642801433801651, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for observed_field, observed_value in observer_schema.items():\n        if observed_field not in response:\n            raise KeyError(f\"Observed field '{observed_field}' not present in Kubernetes response\")\n        if observed_field not in last_applied_manifest:\n            last_applied_manifest[observed_field] = response[observed_field]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for observed_field, observed_value in observer_schema.items():\n        if observed_field not in last_applied_manifest:\n            if observed_field not in response:\n                raise KeyError(f\"Observed field '{observed_field}' not present in Kubernetes response\")\n            last_applied_manifest[observed_field] = response[observed_field]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for observed_field, observed_value in observer_schema.items():\n        if observed_field not in response:\n            raise KeyError(f\"Observed field '{observed_field}' not present in Kubernetes response\")\n        if observed_field not in last_applied_manifest:\n            last_applied_manifest[observed_field] = response[observed_field]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for observed_field, observed_value in observer_schema.items():\n        if observed_field not in last_applied_manifest:\n            if observed_field not in response:\n                raise KeyError(f\"Observed field '{observed_field}' not present in Kubernetes response\")\n            last_applied_manifest[observed_field] = response[observed_field]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for observed_field, observed_value in observer_schema.items():\n        if observed_field not in response:\n            raise KeyError(f\"Observed field '{observed_field}' not present in Kubernetes response\")\n        if observed_field not in last_applied_manifest:\n            last_applied_manifest[observed_field] = response[observed_field]"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.7125549465417862, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n\n    # Iterate over each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        # (this would typically be checked against app.spec.observer_schema or similar)\n        # For now, assume no custom schema is defined, so generate default one\n        resource_observer_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        observer_schema.append(resource_observer_schema)\n\n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        # (this would typically be checked against app.spec.observer_schema or similar)\n        # For now, assume no custom schema is defined, so generate default one\n        \n        # Default observer schema for a Kubernetes resource\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add to observer schema list\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        # (this would typically be checked against app.spec.observer_schema or similar)\n        # For now, assume no custom schema is defined, so generate default one\n        resource_observer_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        observer_schema.append(resource_observer_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n\n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        # (this would typically be checked against app.spec.observer_schema or similar)\n        # For now, assume no custom schema is defined, so generate default one\n        resource_observer_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        observer_schema.append(resource_observer_schema)\n\n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n\n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        # (this would typically be checked against app.spec.observer_schema or similar)\n        # For now, assume no custom schema is defined, so generate default one\n        resource_observer_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        observer_schema.append(resource_observer_schema)\n\n    return observer_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 889, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    # Create a converter instance with the appropriate settings\n    converter = self\n    \n    # Convert the SQL and parameters using the converter\n    formatted_sql, converted_params = converter.convert(sql, params)\n    \n    return formatted_sql, converted_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    # Create a converter instance with the appropriate settings\n    converter = self.__class__(\n        escape_char=None,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style,\n    )\n    \n    # Convert the SQL and parameters\n    formatted_sql, converted_params = converter.convert(sql, params)\n    \n    return formatted_sql, converted_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    # Create a converter instance with the appropriate settings\n    converter = self\n    \n    # Convert the SQL and parameters using the converter\n    formatted_sql, converted_params = converter.convert(sql, params)\n    \n    return formatted_sql, converted_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    # Create a converter instance with the appropriate settings\n    converter = self\n    \n    # Convert the SQL and parameters using the converter\n    formatted_sql, converted_params = converter.convert(sql, params)\n    \n    return formatted_sql, converted_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    # Create a converter instance with the appropriate settings\n    converter = self\n    \n    # Convert the SQL and parameters using the converter\n    formatted_sql, converted_params = converter.convert(sql, params)\n    \n    return formatted_sql, converted_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 889, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    formatted_sql = sql\n    converted_params = []\n    \n    for params in many_params:\n        if isinstance(params, dict):\n            converted_params.append(self.convert_params(params))\n        else:\n            converted_params.append(self.convert_params(params))\n    \n    # Apply conversion to the SQL string\n    formatted_sql = self._in_regex.sub(\n        lambda m: self._out_format.format(\n            *self.convert_params(m.group(0), params)\n        ),\n        sql\n    )\n    \n    return formatted_sql, converted_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    formatted_sql = sql\n    converted_params = []\n    \n    for params in many_params:\n        if isinstance(params, dict):\n            converted_params.append(self.convert_params(params))\n        else:\n            converted_params.append(self.convert_params(params))\n    \n    # Replace in-style parameters in SQL with out-style parameters\n    # This is a simplified version - actual implementation would use regex\n    # to find and replace in-style parameters with out-style ones\n    import re\n    in_regex = self._in_regex\n    out_format = self._out_format\n    \n    # Convert each parameter set using the converter\n    converted_params = []\n    for params in many_params:\n        converted_params.append(self.convert_params(params))\n    \n    # Format the SQL by replacing in-style parameters with out-style ones\n    # This assumes the converter has a method to convert individual parameter sets\n    formatted_sql = self._in_regex.sub(\n        lambda m: self._out_format.format(*[self.convert_param(p) for", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    formatted_sql = sql\n    converted_params = []\n    \n    for params in many_params:\n        if isinstance(params, dict):\n            converted_params.append(self.convert_params(params))\n        else:\n            converted_params.append(self.convert_params(params))\n    \n    # Apply the conversion to the SQL string\n    # This is a simplified version - actual implementation would depend on the specific converter logic\n    # For example, using regex to replace in-style parameters with out-style ones\n    import re\n    in_regex = self._in_regex\n    out_format = self._out_format\n    \n    # Replace in-style parameters with out-style parameters\n    formatted_sql = in_regex.sub(\n        lambda m: self._out_format.format(*[self.convert_param(m.group(0), param) for param in params]),\n        sql\n    )\n    \n    return formatted_sql, converted_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    formatted_sql = sql\n    converted_params = []\n    \n    for params in many_params:\n        if isinstance(params, dict):\n            converted_params.append(self.convert_params(params))\n        else:\n            converted_params.append(self.convert_params(params))\n    \n    # Replace in-style parameters in SQL with out-style parameters\n    # This is a simplified version - actual implementation would depend on the specific converter logic\n    # For now, we assume the converter handles the transformation\n    formatted_sql = self._convert_sql(sql, converted_params)\n    \n    return formatted_sql, converted_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    formatted_sql = sql\n    converted_params = []\n    \n    for params in many_params:\n        if isinstance(params, dict):\n            converted_params.append(self.convert_params(params))\n        else:\n            converted_params.append(self.convert_params(params))\n    \n    # Replace in-style parameters in SQL with out-style parameters\n    # This is a simplified version - actual implementation would use regex\n    # to find and replace in-style parameters with out-style ones\n    import re\n    in_regex = self._in_regex\n    out_format = self._out_format\n    \n    # This is a placeholder - actual implementation would process the SQL\n    # and replace in-style parameters with out-style ones based on the regex\n    # and format string\n    result_sql = formatted_sql\n    \n    # If we have a regex to match in-style parameters, we would use it\n    # to extract and replace them with out-style parameters\n    # For now, we assume the conversion is done by the convert_params method"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 926, "num_retrieved": 3, "inference_time": 0.7125549465417862, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        path = self.root_fs\n    \n    # Validate the object hierarchy\n    validator = Validator(check_digests=True, show_warnings=True)\n    try:\n        obj_fs = open_fs(path, create=False)\n        if not obj_fs.exists(path):\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n        \n        # Validate the object using the validator\n        if validator.validate(obj_fs):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed: %s\", str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        path = self.root_fs\n    \n    # Validate the object at the given path\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    # Check if there are any errors\n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        path = self.root_fs\n    \n    # Validate the object hierarchy\n    validator = Validator(check_digests=True, show_warnings=True)\n    try:\n        obj_fs = open_fs(path, create=False)\n        if not obj_fs.exists(path):\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n        \n        # Validate the object using the validator\n        if validator.validate(obj_fs):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed: %s\", str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        path = self.root_fs\n    \n    # Validate the object hierarchy\n    validator = Validator(check_digests=True, show_warnings=True)\n    try:\n        obj_fs = open_fs(path, create=False)\n        if not obj_fs.exists(path):\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n        \n        # Validate the object using the validator\n        if validator.validate(obj_fs):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed: %s\", str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        path = self.root_fs\n    \n    # Validate the object at the given path\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    # Check if there are any errors\n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 905, "num_retrieved": 2, "inference_time": 0.7125549465417862, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    if prefix:\n        return f\"{prefix} {self.status}\"\n    return self.status", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    if prefix:\n        return f\"{prefix}: {self.status}\"\n    return f\"{self.status}\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    if prefix:\n        return f\"{prefix} {self.status}\"\n    return self.status", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    if prefix:\n        return f\"{prefix} {self.status}\"\n    return self.status", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    if prefix:\n        return f\"{prefix}: {self.status}\"\n    return f\"{self.status}\""]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 885, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    result = prefix + \"\\n\"\n    if self.num_errors > 0 or self.num_warnings > 0 or self.info > 0:\n        if self.num_errors > 0:\n            result += \"Errors: {}\\n\".format(self.num_errors)\n        if self.num_warnings > 0:\n            result += \"Warnings: {}\\n\".format(self.num_warnings)\n        if self.info > 0:\n            result += \"Info: {}\\n\".format(self.info)\n        result += \"\\n\"\n        for msg in self.messages:\n            result += msg + \"\\n\"\n    else:\n        result += \"No validation issues found.\\n\"\n    return result.strip()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    result = prefix + \"\\n\"\n    if self.num_errors > 0 or self.num_warnings > 0 or self.info > 0:\n        if self.num_errors > 0:\n            result += \"Errors: {}\\n\".format(self.num_errors)\n        if self.num_warnings > 0:\n            result += \"Warnings: {}\\n\".format(self.num_warnings)\n        if self.info > 0:\n            result += \"Info: {}\\n\".format(self.info)\n        result += \"\\n\"\n        for msg in self.messages:\n            result += msg + \"\\n\"\n    else:\n        result += \"No validation issues found.\\n\"\n    return result.strip()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    result = prefix + \"\\n\"\n    if self.num_errors > 0 or self.num_warnings > 0 or self.info > 0:\n        if self.num_errors > 0:\n            result += \"Errors: {}\\n\".format(self.num_errors)\n        if self.num_warnings > 0:\n            result += \"Warnings: {}\\n\".format(self.num_warnings)\n        if self.info > 0:\n            result += \"Info: {}\\n\".format(self.info)\n        result += \"\\n\"\n        for msg in self.messages:\n            result += msg + \"\\n\"\n    else:\n        result += \"No validation issues found.\\n\"\n    return result.strip()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    result = prefix + \"\\n\"\n    if self.num_errors > 0 or self.num_warnings > 0 or self.info > 0:\n        if self.num_errors > 0:\n            result += \"Errors: {}\\n\".format(self.num_errors)\n        if self.num_warnings > 0:\n            result += \"Warnings: {}\\n\".format(self.num_warnings)\n        if self.info > 0:\n            result += \"Info: {}\\n\".format(self.info)\n        result += \"\\n\"\n        for msg in self.messages:\n            result += msg + \"\\n\"\n    else:\n        result += \"No validation issues found.\\n\"\n    return result.strip()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    result = prefix + '\\n'\n    if self.num_errors > 0 or self.num_warnings > 0 or self.info > 0:\n        if self.num_errors > 0:\n            result += 'Errors: {}\\n'.format(self.num_errors)\n        if self.num_warnings > 0:\n            result += 'Warnings: {}\\n'.format(self.num_warnings)\n        if self.info > 0:\n            result += 'Info: {}\\n'.format(self.info)\n        result += '\\n'\n        for msg in self.messages:\n            result += msg + '\\n'\n    else:\n        result += 'No validation issues found.\\n'\n    return result.strip()"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 2, "inference_time": 0.7125549465417862, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 906, "num_retrieved": 2, "inference_time": 0.7125549465417862, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        spec_version = inventory.get('type')\n        if spec_version is None or not self._is_valid_spec_version(spec_version):\n            spec_version = self.spec_version\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=spec_version)\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    return inventory, inv_validator\n\ndef _is_valid_spec_version(self, version):\n    \"\"\"Check if the given version string is valid.\"\"\"\n    # Placeholder implementation - actual validation logic depends on specification\n    valid_versions = ['1.0', '1.1', '1.2']  # Example valid versions\n    return version in valid_versions", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        spec_version = inventory.get('type')\n        if spec_version is None or not self._is_valid_spec_version(spec_version):\n            spec_version = self.spec_version\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=spec_version)\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    return inventory, inv_validator\n\ndef _is_valid_spec_version(self, version):\n    \"\"\"Check if the given version string is valid.\"\"\"\n    # Placeholder implementation - actual validation logic would depend on valid versions\n    valid_versions = ['1.0', '1.1', '1.2']\n    return version in valid_versions", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        spec_version = inventory.get('type')\n        if spec_version is None or not self._is_valid_spec_version(spec_version):\n            spec_version = self.spec_version\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=spec_version)\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    return inventory, inv_validator\n\ndef _is_valid_spec_version(self, version):\n    \"\"\"Check if the given version is a valid specification version.\"\"\"\n    # Placeholder implementation - actual validation logic would depend on valid versions\n    valid_versions = ['1.0', '1.1', '1.2']  # Example valid versions\n    return version in valid_versions", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        spec_version = inventory.get('type')\n        if spec_version is None or not self._is_valid_spec_version(spec_version):\n            spec_version = self.spec_version\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=spec_version)\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    return inventory, inv_validator\n\ndef _is_valid_spec_version(self, version):\n    \"\"\"Check if the given version string is valid.\"\"\"\n    # Placeholder implementation - actual validation logic depends on specification\n    valid_versions = ['1.0', '1.1', '1.2']  # Example valid versions\n    return version in valid_versions", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        spec_version = inventory.get('type')\n        if spec_version is None or not self._is_valid_spec_version(spec_version):\n            spec_version = self.spec_version\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=spec_version)\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    return inventory, inv_validator\n\ndef _is_valid_spec_version(self, version):\n    \"\"\"Check if the given version string is valid.\"\"\"\n    # Placeholder implementation - actual validation logic depends on specification\n    valid_versions = ['1.0', '1.1', '1.2']\n    return version in valid_versions"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 882, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        # Extract digests from the manifest file\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        \n        # Check if all digests in the manifest are present in digests_used\n        for file_path, algorithm, digest in manifest_digests:\n            if (file_path, algorithm, digest) not in digests_used:\n                self.log.warning(f'W011', manifest_file=manifest_file, file_path=file_path, algorithm=algorithm, digest=digest)\n    \n    # Verify that all digests used are accounted for in the manifest\n    for file_path, algorithm, digest in digests_used:\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        if (file_path, algorithm, digest) not in manifest_digests:\n            self.log.error(f'E065', file_path=file_path, algorithm=algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        # Extract digests from the manifest file (assuming manifest_files contain digest information)\n        # This is a placeholder - actual implementation would parse the manifest file\n        # to extract digest entries and compare against digests_used\n        pass\n    # Placeholder implementation - actual logic depends on manifest format and digest structure\n    return True", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        # Extract digests from the manifest file\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        \n        # Check if all digests in the manifest are present in digests_used\n        for file_path, algorithm, digest in manifest_digests:\n            if (file_path, algorithm, digest) not in digests_used:\n                self.log.warning(f'W011', manifest_file=manifest_file, file_path=file_path, algorithm=algorithm, digest=digest)\n    \n    # Verify that all digests used are accounted for in the manifest\n    for file_path, algorithm, digest in digests_used:\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        if (file_path, algorithm, digest) not in manifest_digests:\n            self.log.error(f'E065', file_path=file_path, algorithm=algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        # Extract digests from the manifest file\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        \n        # Check if all digests in the manifest are present in digests_used\n        for file_path, algorithm, digest in manifest_digests:\n            if (file_path, algorithm, digest) not in digests_used:\n                self.log.warning(f'W011', manifest_file=manifest_file, file_path=file_path, algorithm=algorithm, digest=digest)\n    \n    # Verify that all digests used are accounted for in the manifest\n    for file_path, algorithm, digest in digests_used:\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        if (file_path, algorithm, digest) not in manifest_digests:\n            self.log.error(f'E065', file_path=file_path, algorithm=algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        # Extract digests from the manifest file\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        \n        # Check if all digests in the manifest are present in digests_used\n        for file_path, algorithm, digest in manifest_digests:\n            if (file_path, algorithm, digest) not in digests_used:\n                self.log.warning(f'W011', manifest_file=manifest_file, file_path=file_path, algorithm=algorithm, digest=digest)\n    \n    # Verify that all digests used are accounted for in the manifest\n    for file_path, algorithm, digest in digests_used:\n        manifest_digests = self.extract_digests_from_manifest(manifest_file)\n        if (file_path, algorithm, digest) not in manifest_digests:\n            self.log.error(f'E065', file_path=file_path, algorithm=algorithm, digest=digest)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 883, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Compare the inventory files to ensure prior is a valid prior version\n    current_inv_file = 'inventory.json'\n    prior_inv_file = prior.inventory_digest_files.get('inventory.json', None)\n    \n    if not prior_inv_file:\n        self.log.error('E065', prior=prior)\n        return False\n    \n    # Validate that the inventory in the prior version is consistent with the current one\n    # by checking if the content is identical\n    if not ocfl_files_identical(self.obj_fs, prior_inv_file, current_inv_file):\n        self.log.error('E066', prior_inv_file=prior_inv_file, current_inv_file=current_inv_file)\n        return False\n    \n    # Ensure that the digest algorithms match\n    if self.digest_algorithm != prior.digest_algorithm:\n        self.log.error('E067', current_algorithm=self.digest_algorithm, prior_algorithm=prior.digest_algorithm)\n        return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Compare the inventory files to ensure prior is a valid prior version\n    current_inv_file = 'inventory.json'\n    prior_inv_file = prior.inventory_digest_files.get(prior.current_version, None)\n    \n    if not prior_inv_file:\n        self.log.error('E065', prior_version=prior.current_version)\n        return False\n    \n    # Ensure the inventory in the prior version is a subset of the current inventory\n    # This is a simplified check based on the assumption that prior versions should\n    # contain only files present in the current version or earlier\n    current_inventory = self.load_inventory(current_inv_file)\n    prior_inventory = prior.load_inventory(prior_inv_file)\n    \n    # Check that all files in prior inventory are present in current inventory\n    for file_path in prior_inventory.get('files', []):\n        if file_path not in current_inventory.get('files', []):\n            self.log.error('E066', file=file_path, prior_version=prior.current_version)\n            return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Compare the inventory files to ensure prior version is valid\n    current_inv_file = 'inventory.json'\n    prior_inv_file = prior.inventory_digest_files.get(prior.current_version, None)\n    \n    if not prior_inv_file:\n        self.log.error('E065', prior_version=prior.current_version)\n        return False\n    \n    # Validate that the inventory in the prior version is consistent with the current one\n    # by checking if the content and digests match\n    if not ocfl_files_identical(self.obj_fs, prior_inv_file, current_inv_file):\n        self.log.error('E066', prior_inv_file=prior_inv_file, current_inv_file=current_inv_file)\n        return False\n    \n    # Ensure that the digest algorithms match\n    if self.digest_algorithm != prior.digest_algorithm:\n        self.log.error('E067', current_digest=self.digest_algorithm, prior_digest=prior.digest_algorithm)\n        return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Compare the inventory files to ensure prior is a valid prior version\n    current_inv_file = 'inventory.json'\n    prior_inv_file = prior.inventory_digest_files.get(prior.current_version, None)\n    \n    if not prior_inv_file:\n        self.log.error('E065', prior_version=prior.current_version)\n        return False\n    \n    # Verify that the current inventory is consistent with the prior inventory\n    # by checking if the content and digests match\n    if not ocfl_files_identical(self.obj_fs, current_inv_file, prior_inv_file):\n        self.log.error('E066', current_inv_file=current_inv_file, prior_inv_file=prior_inv_file)\n        return False\n    \n    # Ensure that the prior version's inventory is not newer than the current one\n    if prior.spec_version > self.spec_version:\n        self.log.error('E067', prior_version=prior.current_version, current_version=self.current_version)\n        return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Compare the inventory files to ensure prior is a valid prior version\n    current_inv_file = 'inventory.json'\n    prior_inv_file = prior.inventory_digest_files.get(prior.current_version, None)\n    \n    if not prior_inv_file:\n        self.log.error('E065', prior_version=prior.current_version)\n        return False\n    \n    # Check if the inventory files are identical\n    if not ocfl_files_identical(self.obj_fs, current_inv_file, prior_inv_file):\n        self.log.error('E066', current_inv_file=current_inv_file, prior_inv_file=prior_inv_file)\n        return False\n    \n    # Validate that the digest algorithms match\n    if self.digest_algorithm != prior.digest_algorithm:\n        self.log.error('E067', current_digest=self.digest_algorithm, prior_digest=prior.digest_algorithm)\n        return False\n    \n    return True"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 882, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    manifest = inventory['manifest']\n    version_data = inventory.get('versions', {}).get(version, {})\n    \n    # Get the state for the version\n    state = version_data.get('state', {})\n    \n    # For each digest in the state, map logical paths to content files\n    for digest, file_paths in state.items():\n        # Find the logical path in the manifest for this digest\n        if digest in manifest:\n            logical_path = manifest[digest]\n            # Collect all content files associated with this digest\n            content_files = set()\n            for file_path in file_paths:\n                # If the file path is in a later version, include it\n                # (this is handled by the state tracking)\n                content_files.add(file_path)\n            logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    manifest = inventory['manifest']\n    version_data = inventory.get('versions', {}).get(version, {})\n    \n    # Get the state for the version\n    state = version_data.get('state', {})\n    \n    # Process each digest in the state\n    for digest, file_paths in state.items():\n        # Find the logical path in the manifest for this digest\n        if digest in manifest:\n            logical_path = manifest[digest]\n            # Collect all content files associated with this digest\n            content_files = set()\n            for file_path in file_paths:\n                # Add the file path to content_files\n                content_files.add(file_path)\n            # Map logical path to content files\n            logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    manifest = inventory['manifest']\n    version_data = inventory.get('versions', {}).get(version, {})\n    \n    # Get the state for the version\n    state = version_data.get('state', {})\n    \n    # For each digest in the state, map logical paths to content files\n    for digest, file_paths in state.items():\n        # Find the logical path in the manifest for this digest\n        if digest in manifest:\n            logical_path = manifest[digest]\n            # Collect all content files (including duplicates from later versions)\n            content_files = set()\n            for file_path in file_paths:\n                # Add the file path to content files\n                content_files.add(file_path)\n            logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    manifest = inventory['manifest']\n    version_data = inventory.get('versions', {}).get(version, {})\n    \n    # Get the state for the version\n    state = version_data.get('state', {})\n    \n    # For each digest in the state, map logical paths to content files\n    for digest, file_paths in state.items():\n        # Find the logical path in the manifest for this digest\n        if digest in manifest:\n            logical_path = manifest[digest]\n            # Collect all content files (source paths) for this digest\n            content_files = set()\n            for file_path in file_paths:\n                # If the file path is in a later version, include it\n                # (this is handled by the state accumulation)\n                content_files.add(file_path)\n            logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    manifest = inventory['manifest']\n    version_data = inventory.get('versions', {}).get(version, {})\n    \n    # Extract the state for the given version\n    state = version_data.get('state', {})\n    \n    # For each digest in the state, map logical paths to content files\n    for digest, file_paths in state.items():\n        # Find the logical path in the manifest for this digest\n        if digest in manifest:\n            logical_path = manifest[digest]\n            # Collect all content files (source paths) for this digest\n            content_files = set()\n            for file_path in file_paths:\n                # If the file path is already in the manifest, it's a logical path\n                # Otherwise, it's a content file\n                content_files.add(file_path)\n            logical_path_map[logical_path] = content_files\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 882, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return True\n    \n    # Ensure fixity is a dictionary\n    if not isinstance(fixity, dict):\n        self.log.error('E055', fixity=fixity)\n        return False\n    \n    # Check that fixity contains only file paths as keys\n    for file_path in fixity:\n        if not isinstance(file_path, str):\n            self.log.error('E056', file_path=file_path)\n            return False\n        \n        # Verify that the file path is in the manifest_files\n        if file_path not in manifest_files:\n            self.log.warning('W011', file_path=file_path)\n            return False\n    \n    return True", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return True\n\n    # Ensure fixity is a dictionary\n    if not isinstance(fixity, dict):\n        self.log.error('E065', fixity=fixity)\n        return False\n\n    # Check that fixity contains only file paths and digest information\n    for file_path, digest_info in fixity.items():\n        # File path should be a string\n        if not isinstance(file_path, str):\n            self.log.error('E066', file_path=file_path)\n            return False\n\n        # Digest info should be a dictionary with algorithm and digest\n        if not isinstance(digest_info, dict):\n            self.log.error('E067', file_path=file_path, digest_info=digest_info)\n            return False\n\n        algorithm = digest_info.get('algorithm')\n        digest = digest_info.get('digest')\n\n        # Algorithm and digest must be present\n        if not algorithm or not digest:\n            self.log.error('E068', file_path=file_path)\n            return False\n\n        # Validate that the file path is in the manifest_files\n        if file_path not in manifest_files:\n            self.log.warning('W011', file_path=file_path)\n            return False\n\n    return True", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return True\n\n    # Ensure fixity is a dictionary\n    if not isinstance(fixity, dict):\n        self.log.error('E065', fixity=fixity)\n        return False\n\n    # Check that fixity contains only file paths as keys\n    for file_path in fixity:\n        if not isinstance(file_path, str):\n            self.log.error('E066', file_path=file_path)\n            return False\n\n        # Verify that the file path is in the manifest_files\n        if file_path not in manifest_files:\n            self.log.warning('W011', file_path=file_path)\n            return False\n\n    return True", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return True\n\n    # Ensure fixity is a dictionary\n    if not isinstance(fixity, dict):\n        self.log.error('E065', fixity=fixity)\n        return False\n\n    # Check that fixity contains only file paths as keys\n    for file_path in fixity.keys():\n        if not isinstance(file_path, str):\n            self.log.error('E066', file_path=file_path)\n            return False\n\n        # Verify that the file path is in the manifest_files\n        if file_path not in manifest_files:\n            self.log.warning('W011', file_path=file_path)\n            return False\n\n    return True", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return True\n    \n    # Ensure fixity is a dictionary\n    if not isinstance(fixity, dict):\n        self.log.error('E055', fixity=fixity)\n        return False\n    \n    # Check that fixity block contains only files from manifest\n    for file_path, digest_info in fixity.items():\n        if not isinstance(file_path, str):\n            self.log.error('E056', file_path=file_path)\n            return False\n        \n        # Check if file path is in manifest_files\n        if file_path not in manifest_files:\n            self.log.warning('W011', file_path=file_path)\n            return False\n        \n        # Validate digest info structure\n        if not isinstance(digest_info, dict):\n            self.log.error('E057', file_path=file_path)\n            return False\n        \n        # Check that digest_info has required keys\n        required_keys = ['algorithm', 'digest']\n        for key in required_keys:\n            if key not in digest_info:\n                self.log.error('E058', file_path=file_path, key=key)\n                return False\n    \n    return True"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 920, "num_retrieved": 3, "inference_time": 0.7125549465417862, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 885, "num_retrieved": 1, "inference_time": 0.7125549465417862, "generate_results": ["def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    filename_root, __ = os.path.splitext(xml_filename)\n    grouped_files = {}\n    for file in files:\n        file_root, file_ext = os.path.splitext(file)\n        if file_root.startswith(filename_root):\n            if filename_root not in grouped_files:\n                grouped_files[filename_root] = []\n            grouped_files[filename_root].append(file)\n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    filename_root, __ = os.path.splitext(xml_filename)\n    grouped_files = {}\n    for file in files:\n        file_root, file_ext = os.path.splitext(file)\n        if file_root.startswith(filename_root):\n            if filename_root not in grouped_files:\n                grouped_files[filename_root] = []\n            grouped_files[filename_root].append(file)\n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    filename_root, _ = os.path.splitext(xml_filename)\n    grouped_files = {}\n    for file in files:\n        file_root, file_ext = os.path.splitext(file)\n        if file_root.startswith(filename_root):\n            if filename_root not in grouped_files:\n                grouped_files[filename_root] = []\n            grouped_files[filename_root].append(file)\n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    filename_root, _ = os.path.splitext(xml_filename)\n    grouped_files = {}\n    for file in files:\n        file_root, file_ext = os.path.splitext(file)\n        if file_root.startswith(filename_root):\n            if filename_root not in grouped_files:\n                grouped_files[filename_root] = []\n            grouped_files[filename_root].append(file)\n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    result = {}\n    for file in files:\n        if file.endswith(\".xml\"):\n            basename = os.path.splitext(file)[0]\n            if basename == xml_filename:\n                result[basename] = file\n    return result"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 939, "num_retrieved": 3, "inference_time": 0.7125549465417862, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.7125549465417862, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 906, "num_retrieved": 2, "inference_time": 0.8274647891521454, "generate_results": ["import os\nimport glob\nfrom pathlib import Path\nfrom typing import Dict, List\n\ndef _explore_folder(folder: str) -> Dict[str, Dict]:\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    folder_path = Path(folder)\n    if not folder_path.exists() or not folder_path.is_dir():\n        return {}\n\n    package_data = {}\n    \n    # Find XML files (assuming XML files have .xml extension)\n    xml_files = list(folder_path.glob(\"*.xml\"))\n    \n    for xml_file in xml_files:\n        xml_basename = xml_file.stem\n        package_id = xml_basename\n        \n        # Find corresponding renditions and assets\n        rendition_files = list(folder_path.glob(f\"{xml_basename}*.pdf\"))\n        asset_files = list(folder_path.glob(f\"{xml_basename}*.jpg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.jpeg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.png\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.gif\"))\n        \n        # Group files by type\n        package_data[package_id] = {\n            'xml': str(xml_file),\n            'renditions': [str(f) for f in rendition_files],\n            'assets': [str(f) for f in asset_files]\n        }\n    \n    return package_data", "import os\nimport glob\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    folder = Path(folder)\n    if not folder.exists():\n        raise FileNotFoundError(f\"Folder not found: {folder}\")\n    \n    # Dictionary to group files by XML basename\n    packages = defaultdict(dict)\n    \n    # Look for XML files (assuming XML files have .xml extension)\n    xml_files = list(folder.glob(\"*.xml\"))\n    \n    for xml_file in xml_files:\n        xml_basename = xml_file.stem\n        xml_path = str(xml_file)\n        \n        # Find corresponding renditions and assets\n        rendition_files = list(folder.glob(f\"{xml_basename}*.pdf\"))\n        asset_files = list(folder.glob(f\"{xml_basename}*.jpg\"))  # Adjust extension as needed\n        asset_files.extend(folder.glob(f\"{xml_basename}*.jpeg\"))\n        asset_files.extend(folder.glob(f\"{xml_basename}*.png\"))\n        asset_files.extend(folder.glob(f\"{xml_basename}*.gif\"))\n        \n        # Group files by type\n        packages[xml_basename]['xml'] = xml_path\n        packages[xml_basename]['renditions'] = [str(f) for f in rendition_files]\n        packages[xml_basename]['assets'] = [str(f) for f in asset_files]\n    \n    return dict(packages)", "import os\nimport glob\nfrom pathlib import Path\nfrom typing import Dict, List\n\ndef _explore_folder(folder: str) -> Dict[str, Dict]:\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    folder_path = Path(folder)\n    if not folder_path.exists():\n        raise FileNotFoundError(f\"Folder not found: {folder}\")\n\n    xml_files = list(folder_path.glob(\"*.xml\"))\n    if not xml_files:\n        return {}\n\n    package_data = {}\n\n    for xml_file in xml_files:\n        xml_basename = xml_file.stem\n        package_id = xml_basename\n        \n        # Find corresponding rendition and asset files\n        rendition_files = list(folder_path.glob(f\"{xml_basename}*.pdf\"))\n        asset_files = list(folder_path.glob(f\"{xml_basename}*.jpg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.jpeg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.png\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.gif\"))\n        \n        package_data[package_id] = {\n            'xml': str(xml_file),\n            'renditions': [str(f) for f in rendition_files],\n            'assets': [str(f) for f in asset_files]\n        }\n\n    return package_data", "import os\nimport glob\nfrom pathlib import Path\nfrom typing import Dict, List\n\ndef _explore_folder(folder: str) -> Dict[str, Dict]:\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    folder_path = Path(folder)\n    if not folder_path.exists():\n        raise FileNotFoundError(f\"Folder not found: {folder}\")\n\n    xml_files = list(folder_path.glob(\"*.xml\"))\n    if not xml_files:\n        return {}\n\n    package_data = {}\n\n    for xml_file in xml_files:\n        xml_basename = xml_file.stem\n        package_id = xml_basename\n        \n        # Find corresponding renditions and assets\n        rendition_files = list(folder_path.glob(f\"{xml_basename}*.pdf\"))\n        asset_files = list(folder_path.glob(f\"{xml_basename}*.jpg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.jpeg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.png\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.gif\"))\n        \n        # Group files by type\n        renditions = [{\"uri\": str(f), \"name\": f.name} for f in rendition_files]\n        assets = [{\"uri\": str(f), \"name\": f.name} for f in asset_files]\n        \n        package_data[package_id] = {\n            \"xml\": {\"uri\": str(xml_file), \"name\": xml_file.name},\n            \"renditions\": renditions,\n            \"assets\": assets\n        }\n    \n    return package_data", "import os\nimport glob\nfrom pathlib import Path\nfrom typing import Dict, List\n\ndef _explore_folder(folder: str) -> Dict[str, Dict]:\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    folder_path = Path(folder)\n    if not folder_path.exists():\n        raise FileNotFoundError(f\"Folder not found: {folder}\")\n\n    xml_files = list(folder_path.glob(\"*.xml\"))\n    if not xml_files:\n        return {}\n\n    package_data = {}\n\n    for xml_file in xml_files:\n        xml_basename = xml_file.stem\n        package_id = xml_basename\n        \n        # Find corresponding renditions and assets\n        rendition_files = list(folder_path.glob(f\"{xml_basename}*.pdf\"))\n        asset_files = list(folder_path.glob(f\"{xml_basename}*.jpg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.jpeg\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.png\")) + \\\n                      list(folder_path.glob(f\"{xml_basename}*.gif\"))\n        \n        package_data[package_id] = {\n            'xml': str(xml_file),\n            'renditions': [str(f) for f in rendition_files],\n            'assets': [str(f) for f in asset_files]\n        }\n\n    return package_data"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 922, "num_retrieved": 4, "inference_time": 0.8274647891521454, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    ext = os.path.splitext(file_path)[1]\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file_path == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file_path == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    ext = os.path.splitext(file_path)[1]\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file_path == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file_path == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    ext = os.path.splitext(file_path)[1]\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file_path == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file_path == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    ext = os.path.splitext(file_path)[1]\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file_path == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file_path == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    ext = os.path.splitext(file_path)[1]\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file_path == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file_path == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.8274647891521454, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base no idioma e caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma do PDF de rendio (ex: 'en', 'pt', 'es')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n    \n    Returns\n    -------\n    dict\n        Dicionrio com informaes sobre o PDF de rendio\n    \"\"\"\n    # Extrai o nome do arquivo e extenso\n    file_name, ext = os.path.splitext(file_path)\n    \n    # Verifica se  um PDF\n    if ext.lower() != '.pdf':\n        raise ValueError(\"O arquivo deve ter extenso .pdf\")\n    \n    # Define o nome do arquivo com base no idioma\n    rendition_name = f\"{file_name}-{lang}{ext}\"\n    \n    # Retorna um dicionrio com as informaes do PDF de rendio\n    return {\n        'uri': file_path,\n        'name': rendition_name\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona uma verso de rendio (rendition) ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'en', 'pt', 'es')\n    file_path : str\n        Caminho do arquivo de rendio\n    \n    Returns\n    -------\n    dict\n        Dicionrio com informaes da rendio, incluindo o nome original e o nome com idioma\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o nome da rendio com base no idioma\n    if lang == 'original':\n        rendition_name = file_name\n    else:\n        rendition_name = f\"{prefix}-{lang}.{os.path.splitext(file_name)[1]}\"\n    \n    return {\n        \"original\": prefix,\n        \"en\": rendition_name if lang == 'en' else None\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um rendition ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma do rendition (ex: 'en', 'pt', 'es')\n    file_path : str\n        Caminho do arquivo do rendition\n    \n    Returns\n    -------\n    dict\n        Dicionrio com informaes do rendition, incluindo o nome original e o nome com idioma\n    \"\"\"\n    file_name, ext = os.path.splitext(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o papel do arquivo usando a funo get_file_role\n    role = file_utils.get_file_role(file_path, prefix, [lang])\n    \n    # Se for um rendition, cria o nome com o idioma\n    if role == 'renditions':\n        rendition_name = f\"{prefix}-{lang}{ext}\"\n        return {\n            \"original\": f\"{prefix}{ext}\",\n            \"rendition\": rendition_name\n        }\n    \n    return {\"original\": file_path}", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma do PDF de rendio (ex: 'en', 'pt', 'es')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com o nome do arquivo e o caminho, no formato:\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\"\n        }\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    prefix = file.rsplit('-', 1)[0] if '-' in file else file\n    \n    if ext == '.pdf':\n        if lang == 'original':\n            return {'original': file_path}\n        else:\n            rendition_name = f\"{prefix}-{lang}{ext}\"\n            return {lang: rendition_name}\n    return {}", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base no idioma e caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma do PDF de rendio (ex: 'en', 'pt', 'es')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com informaes do PDF de rendio, incluindo nome e URI\n    \"\"\"\n    # Extrai o nome do arquivo e extenso\n    file_name, ext = os.path.splitext(file_path)\n    \n    # Define o prefixo base (sem idioma) do arquivo original\n    prefix = os.path.basename(file_path).replace(f'-{lang}', '').replace(f'{lang}', '')\n    \n    # Verifica se o arquivo  um PDF de rendio com base no idioma\n    if ext == '.pdf':\n        # Se o nome do arquivo corresponde ao prefixo + idioma,  um PDF de rendio\n        if file_name == f\"{prefix}-{lang}\":\n            return {\n                'uri': file_path,\n                'name': f\"{prefix}-{lang}{ext}\"\n            }\n    \n    # Caso contrrio, tenta identificar como rendio com base no mtodo de deteco\n    # (usando a funo get_file_role para verificar o papel do arquivo)\n    file_role = file_utils.get_file_role(file_path, prefix, [lang])\n    \n    if file_role == 'renditions':\n        return {\n            'uri': file_path,\n            'name': f\"{prefix}-{lang}{ext}\"\n        }\n    \n    return None"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 934, "num_retrieved": 3, "inference_time": 0.8274647891521454, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with the given basename and file path.\"\"\"\n    self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with the given basename and file path.\"\"\"\n    self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with the given basename and file path.\"\"\"\n    self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with the given basename and file path.\"\"\"\n    self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with the given basename and file path.\"\"\"\n    self._assets[basename] = file_path"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 986, "num_retrieved": 4, "inference_time": 0.8274647891521454, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import zipfile\n    import os\n    \n    data = {}\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file_info)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import zipfile\n    import os\n    \n    file_data = {}\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                content = zf.read(filename)\n                if basename not in file_data:\n                    file_data[basename] = []\n                file_data[basename].append(content)\n    return file_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import zipfile\n    import os\n    \n    file_data = {}\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                content = zf.read(filename)\n                if basename not in file_data:\n                    file_data[basename] = []\n                file_data[basename].append(content)\n    return file_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import zipfile\n    import os\n    \n    data = {}\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file_info)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import zipfile\n    import os\n    \n    file_data = {}\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                content = zf.read(filename)\n                if basename not in file_data:\n                    file_data[basename] = []\n                file_data[basename].append(content)\n    return file_data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 914, "num_retrieved": 2, "inference_time": 0.8274647891521454, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 995, "num_retrieved": 4, "inference_time": 0.8274647891521454, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    return content.replace('w:st=\"', 'w-st=\"')", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1010, "num_retrieved": 5, "inference_time": 0.8274647891521454, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1103, "num_retrieved": 8, "inference_time": 0.8274647891521454, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a period or space to extract number and supplement\n    # Common patterns: \"1(2)\", \"1-2\", \"1(2)\", \"1-2(3)\", etc.\n    issue_str = issue.strip()\n    \n    # Try to find the number part (digits before a parenthesis or dash)\n    import re\n    match = re.search(r'^(\\d+)(?:[.-](\\d+))?(?:\\((\\d+)\\))?', issue_str)\n    \n    if match:\n        number = int(match.group(1))\n        supplement = match.group(3) if match.group(3) else \"\"\n        return number, supplement\n    \n    # Fallback: if no clear pattern, return 0 and empty string\n    return 0, \"\"", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a dot or space, and the number is before it\n    parts = issue.strip().split('.')\n    if len(parts) == 1:\n        # If no dot, check for space or other separators\n        parts = issue.strip().split()\n    \n    number = 0\n    supplement = \"\"\n    \n    # Try to extract number (first part)\n    if len(parts) >= 1:\n        number_part = parts[0].strip()\n        if number_part.isdigit():\n            number = int(number_part)\n    \n    # Extract supplement (if exists)\n    if len(parts) > 1:\n        supplement_part = parts[1].strip()\n        if supplement_part:\n            supplement = supplement_part\n    \n    # If no number found, try to extract from the full string\n    if number == 0:\n        # Try to find the first digit sequence as number\n        match = re.search(r'^(\\d+)', issue)\n        if match:\n            number = int(match.group(1))\n    \n    # If no supplement found, try to extract from the rest\n    if not supplement:\n        # Look for any non-digit characters after the number\n        match = re.search(r'(\\d+)([^\\d]+)', issue)\n        if match:\n            supplement = match.group(2).strip()\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a period or space to extract number and supplement\n    # Common patterns: \"1(2)\", \"1-2\", \"1(2)\", \"1-2(3)\", etc.\n    issue_str = issue.strip()\n    \n    # Try to find the number part (digits before parentheses or hyphens)\n    import re\n    match = re.search(r'^(\\d+)(?:[^\\d]*(\\d+))?', issue_str)\n    \n    # If we have a match, extract number and supplement\n    if match:\n        number = int(match.group(1))\n        supplement = match.group(2) if match.group(2) else \"\"\n        return number, supplement\n    \n    # Fallback: if no clear pattern, return 0 and empty string\n    return 0, \"\"", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # Common formats: \"1(2)\", \"1-2\", \"1(2)\", \"1-2(3)\", etc.\n    issue_str = issue.strip()\n    \n    # Try to find the main number (before any parentheses or hyphens)\n    number_match = re.search(r'^(\\d+)(?:[^\\d]*(?:\\((\\d+)\\)|\\-(\\d+))?)$', issue_str)\n    \n    if number_match:\n        number = int(number_match.group(1))\n        supplement = \"\"\n        if number_match.group(2):\n            supplement = number_match.group(2)\n        elif number_match.group(3):\n            supplement = number_match.group(3)\n        return number, supplement\n    \n    # Fallback: if no clear pattern, return 0 and empty string\n    return 0, \"\"", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a dot or in a separate part\n    parts = issue.strip().split('.')\n    if len(parts) == 1:\n        # If no dot, assume number is the whole string and supplement is empty\n        return int(parts[0]), \"\"\n    else:\n        # Extract number (first part) and supplement (remaining parts)\n        number_part = parts[0]\n        supplement_part = '.'.join(parts[1:]) if len(parts) > 1 else \"\"\n        \n        # Convert number to integer, default to 0 if empty or invalid\n        try:\n            number = int(number_part)\n        except (ValueError, TypeError):\n            number = 0\n            \n        return number, supplement_part"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 0.8274647891521454, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 0.8274647891521454, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it matches absorption or negative absorption patterns\n        # This is a simplified implementation assuming expressions are in a form that can be evaluated\n        \n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using Boolean algebra rules\n        # We'll assume the expressions are already in canonical form\n        # and apply the rules directly\n        \n        # For now, return the original expressions as a placeholder\n        # A full implementation would require parsing and evaluating sub-expressions\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation - actual logic would depend on expression structure\n        # In practice, this would require parsing the expression tree\n        \n        # For now, we return the original expression as a placeholder\n        # A full implementation would need to traverse the expression tree\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        # Absorption: A & (A | B) = A, A | (A & B) = A\n        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(expr, Expression):\n            # Check for absorption cases\n            # Case 1: A & (A | B) = A\n            # Case 2: A | (A & B) = A\n            # Case 3: A & (~A | B) = A & B\n            # Case 4: A | (~A & B) = A | B\n            \n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect the patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, we'll just return the expression as is\n            # A full implementation would require recursive traversal\n            # and pattern matching on the expression structure\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation - actual logic would depend on expression structure\n            # For now, we return the expression as is since full parsing and simplification is complex\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 926, "num_retrieved": 2, "inference_time": 0.8274647891521454, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.registry.register(hook, handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.registry.register(hook, handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.registry.register(hook, handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.registry.register(hook, handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.registry.register(hook, handler)\n        return handler\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 939, "num_retrieved": 3, "inference_time": 0.8274647891521454, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd_host\": etcd_host,\n        \"etcd_port\": etcd_port\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.8274647891521454, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Validate input\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError('dt must be a datetime.datetime object')\n    \n    # If the datetime already has this tzinfo, we can use it directly\n    if dt.tzinfo is self:\n        # We know this is unambiguous, so we just convert\n        return self._localize(dt)\n    \n    # Otherwise, we need to convert from UTC to local time\n    # First, determine if the datetime is ambiguous\n    is_ambiguous = False\n    fold = 0\n    \n    # Check if the timezone has an is_ambiguous method\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            is_ambiguous = is_ambiguous_fn(dt)\n            if is_ambiguous:\n                # If ambiguous, we need to determine fold state\n                # We'll use the fold attribute to determine which occurrence\n                # to use (first or second)\n                fold = 0  # Default to first occurrence\n                # In practice, we'd need to check fold attribute\n                # But since we're in fromutc, we assume fold=0 is correct\n        except Exception:\n            pass\n    \n    # Create a local datetime with fold=0 by default\n    local_dt = self._localize(dt.replace(tzinfo=None))\n    \n    # If ambiguous, we need to check fold\n    if is_ambiguous:\n        # Try fold=0 and fold=1 to see which one matches\n        # In practice, we'd use the fold attribute to determine\n        # which occurrence to use\n        # For now, we'll just return fold=0\n        fold = 0\n    \n    # Return the localized datetime\n    return local_dt.replace(tzinfo=self, fold=fold)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Validate input\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError('dt must be a datetime.datetime object')\n    \n    # If the datetime already has this tzinfo, we can use it directly\n    if dt.tzinfo is self:\n        # We know this is unambiguous, so we just convert\n        return self._localize(dt)\n    \n    # Otherwise, we need to convert from UTC to local time\n    # First, determine if the datetime is ambiguous\n    is_ambiguous = False\n    fold = 0\n    \n    # Check if the timezone has an is_ambiguous method\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            is_ambiguous = is_ambiguous_fn(dt)\n            if is_ambiguous:\n                # If ambiguous, we need to determine which fold to use\n                # We'll use fold=0 by default, but if the datetime is ambiguous\n                # and we're in a fold state, we need to handle it properly\n                fold = 0\n        except Exception:\n            pass\n    \n    # If no is_ambiguous method, check fold attribute\n    if not is_ambiguous:\n        # Create two possible times with fold=0 and fold=1\n        dt_fold_0 = dt.replace(tzinfo=self, fold=0)\n        dt_fold_1 = dt.replace(tzinfo=self, fold=1)\n        \n        # Check if they have different offsets or DST\n        if dt_fold_0.utcoffset() != dt_fold_1.utcoffset() or dt_fold_0.dst() != dt_fold_1.dst():\n            is_ambiguous = True\n            # Use fold=0 as the default (first occurrence)\n            fold = 0\n    \n    # Convert from UTC to local time with the appropriate fold\n    # Since we're in", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Validate input\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError('dt must be a datetime.datetime object')\n    \n    # If the datetime already has this tzinfo, we can use it directly\n    if dt.tzinfo is self:\n        # We know this is unambiguous, so we just convert\n        return dt.astimezone(self)\n    \n    # Create a timezone-aware datetime with our tzinfo\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    is_ambiguous = False\n    try:\n        # Check if the timezone has an is_ambiguous method\n        is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n        if is_ambiguous_fn is not None:\n            is_ambiguous = self.is_ambiguous(dt_with_tz)\n    except Exception:\n        pass\n    \n    # If not ambiguous, return the standard conversion\n    if not is_ambiguous:\n        return dt_with_tz.astimezone(self)\n    \n    # If ambiguous, determine which fold to use (fold=0 is the first occurrence)\n    # We use fold=0 by default since we know it's unambiguous in this context\n    # and we want to return the first occurrence chronologically\n    wall_0 = enfold(dt_with_tz, fold=0)\n    wall_1 = enfold(dt_with_tz, fold=1)\n    \n    # Choose the fold that matches the original dt's time\n    # Since we know it's unambiguous, we pick fold=0 by default\n    # (the first occurrence)\n    return wall_0.astimezone(self)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Validate input\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError('dt must be a datetime.datetime object')\n    \n    # If the datetime already has this tzinfo, we can use it directly\n    if dt.tzinfo is self:\n        # We know this is unambiguous, so we just convert\n        return self._localize(dt)\n    \n    # Create a timezone-aware datetime with our tzinfo\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    is_ambiguous = False\n    try:\n        # Check if the timezone has an is_ambiguous method\n        is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n        if is_ambiguous_fn is not None:\n            is_ambiguous = is_ambiguous_fn(dt_with_tz)\n    except Exception:\n        pass\n    \n    # If not ambiguous, return the localized datetime\n    if not is_ambiguous:\n        return self._localize(dt_with_tz)\n    \n    # If ambiguous, determine fold state\n    # We need to check if fold=0 or fold=1 gives the same offset\n    wall_0 = enfold(dt_with_tz, fold=0)\n    wall_1 = enfold(dt_with_tz, fold=1)\n    \n    # If both have the same offset and DST, then it's not ambiguous\n    if wall_0.utcoffset() == wall_1.utcoffset() and wall_0.dst() == wall_1.dst():\n        # This should not happen if is_ambiguous is True\n        return self._localize(dt_with_tz)\n    \n    # Otherwise, we need to determine which fold to use\n    # Since we're converting from UTC, we use fold=0 by default\n    # (the first occurrence chronologically)\n    return self._localize(wall_0)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Validate input\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError('dt must be a datetime.datetime object')\n    \n    # If the datetime already has this tzinfo, we can use it directly\n    if dt.tzinfo is self:\n        # We know this is unambiguous, so we just convert\n        return self._localize(dt)\n    \n    # Create a timezone-aware datetime with our tzinfo\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    is_ambiguous = False\n    try:\n        # Check if the timezone has an is_ambiguous method\n        is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n        if is_ambiguous_fn is not None:\n            is_ambiguous = is_ambiguous_fn(dt_with_tz)\n    except Exception:\n        pass\n    \n    # If not ambiguous, return the localized datetime\n    if not is_ambiguous:\n        return self._localize(dt_with_tz)\n    \n    # If ambiguous, determine which fold to use (fold=0 is the first occurrence)\n    # We use fold=0 by default since we know it's unambiguous in this context\n    # and we want to return the first occurrence chronologically\n    wall_0 = enfold(dt_with_tz, fold=0)\n    wall_1 = enfold(dt_with_tz, fold=1)\n    \n    # Choose the one with the smaller offset (first occurrence)\n    if wall_0.utcoffset() < wall_1.utcoffset():\n        return self._localize(wall_0)\n    else:\n        return self._localize(wall_1)"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 2, "inference_time": 0.8274647891521454, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr, 0)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer part, and handle fractional part\n                int_part = int(value)\n                frac_part = value - int_part\n                if frac_part > 0:\n                    # Add the fractional part as additional units\n                    if attr == 'days':\n                        result.days = int_part + 1\n                        result.hours += int(frac_part * 24)\n                    elif attr == 'hours':\n                        result.hours = int_part + int(frac_part * 60)\n                    elif attr == 'minutes':\n                        result.minutes = int_part + int(frac_part * 60)\n                    elif attr == 'seconds':\n                        result.seconds = int_part + int(frac_part * 60)\n            else:\n                setattr(result, attr, int(value))\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr, 0)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer part and adjust remaining fractional part\n                int_part = int(value)\n                frac_part = value - int_part\n                if frac_part > 0:\n                    # Add the fractional part as additional units\n                    if attr == 'days':\n                        result.days = int_part + 1\n                        result.hours += int(frac_part * 24)\n                    elif attr == 'hours':\n                        result.hours = int_part + int(frac_part * 60)\n                    elif attr == 'minutes':\n                        result.minutes = int_part + int(frac_part * 60)\n                    elif attr == 'seconds':\n                        result.seconds = int_part + int(frac_part * 60)\n            else:\n                setattr(result, attr, int(value))\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr, 0)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer part, and handle fractional part\n                int_part = int(value)\n                frac_part = value - int_part\n                if frac_part > 0:\n                    # Add the fractional part as additional units\n                    if attr == 'days':\n                        result.days = int_part + 1\n                        result.hours += int(frac_part * 24)\n                    elif attr == 'hours':\n                        result.hours = int_part + int(frac_part * 60)\n                    elif attr == 'minutes':\n                        result.minutes = int_part + int(frac_part * 60)\n                    elif attr == 'seconds':\n                        result.seconds = int_part + int(frac_part * 60)\n                else:\n                    result.__dict__[attr] = int_part\n            else:\n                result.__dict__[attr] = int(value)\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr, 0)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer part, and handle fractional part\n                int_part = int(value)\n                frac_part = value - int_part\n                if frac_part > 0:\n                    # Add the fractional part as additional units\n                    if attr == 'days':\n                        result.days = int_part + 1\n                        result.hours += int(frac_part * 24)\n                    elif attr == 'hours':\n                        result.hours = int_part + int(frac_part * 60)\n                    elif attr == 'minutes':\n                        result.minutes = int_part + int(frac_part * 60)\n                    elif attr == 'seconds':\n                        result.seconds = int_part + int(frac_part * 60)\n            else:\n                setattr(result, attr, int(value))\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr, 0)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer part, and handle fractional part\n                int_part = int(value)\n                frac_part = value - int_part\n                if frac_part > 0:\n                    # Add the fractional part as additional units\n                    if attr == 'days':\n                        result.days = int_part + 1\n                        result.hours += int(frac_part * 24)\n                    elif attr == 'hours':\n                        result.hours = int_part + int(frac_part * 60)\n                    elif attr == 'minutes':\n                        result.minutes = int_part + int(frac_part * 60)\n                    elif attr == 'seconds':\n                        result.seconds = int_part + int(frac_part * 60)\n                    elif attr == 'weeks':\n                        result.weeks = int_part + int(frac_part * 7)\n                    elif attr == 'months':\n                        result.months = int_part + int(frac_part * 12)\n                    elif attr == 'years':\n                        result.years = int_part + int(frac_part * 12)\n                else:\n                    result.__dict__[attr] = int_part\n            else:\n                result.__dict__[attr] = int(value)\n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.8274647891521454, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if isinstance(namefunc, str):\n        return namefunc\n    else:\n        return namefunc.encode('ascii')", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return as is (already bytes or unicode)\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if hasattr(namefunc, '__name__'):\n        # This is a function that might be used for tzname\n        # In Python 2, we need to ensure unicode output is converted to bytestrings\n        @wraps(namefunc)\n        def wrapper(*args, **kwargs):\n            result = namefunc(*args, **kwargs)\n            if isinstance(result, six.text_type):\n                # Convert unicode to bytes in Python 2\n                return result.encode('ascii')\n            return result\n        return wrapper\n    return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 973, "num_retrieved": 5, "inference_time": 0.9034321010112762, "generate_results": ["def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Return default version information if unable to read _version.py\n        return {\"version\": \"unknown\"}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Return default version information if unable to read _version.py\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Return default version information if unable to read _version.py\n        return {\"version\": \"unknown\"}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        return {\"version\": \"unknown\"}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Return default version information if unable to read _version.py\n        return {\"version\": \"unknown\"}"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 950, "num_retrieved": 4, "inference_time": 0.9034321010112762, "generate_results": ["def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if \"+\" in pieces[\"version\"]:\n        return \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if '+' in pieces.get('version', ''):\n        return '.'\n    return '+'", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if \"+\" in pieces[\"version\"]:\n        return \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if \"+\" in pieces[\"version\"]:\n        return \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if \"post\" in pieces[\"closest-tag\"]:\n        return \".\"\n    else:\n        return \"+\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command list\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    if args:\n        cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            env=env,\n            capture_output=True if not hide_stderr else False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            print(\"Command output:\", result.stdout)\n            \n        return result.returncode, result.stdout if not hide_stderr else \"\"\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(\"Command failed with return code:\", e.returncode)\n            print(\"Error output:\", e.stdout)\n        return e.returncode, e.stdout\n    except Exception as e:\n        if verbose:\n            print(\"Command raised an exception:\", str(e))\n        return -1, str(e)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command list\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    if args:\n        cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            env=env,\n            capture_output=True if not hide_stderr else False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            print(\"Command output:\", result.stdout)\n            \n        return result.returncode, result.stdout if not hide_stderr else \"\"\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(\"Command failed with return code:\", e.returncode)\n            print(\"Error output:\", e.stdout)\n        return e.returncode, e.stdout\n    except Exception as e:\n        if verbose:\n            print(\"Command raised an exception:\", str(e))\n        return -1, str(e)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command list\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    if args:\n        cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            env=env,\n            capture_output=True if not hide_stderr else False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            print(\"Command output:\", result.stdout)\n            \n        return result.returncode, result.stdout if not hide_stderr else \"\"\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(\"Command failed with return code:\", e.returncode)\n            print(\"Error output:\", e.stdout)\n        return e.returncode, e.stdout\n    except Exception as e:\n        if verbose:\n            print(\"Command raised an exception:\", str(e))\n        return -1, str(e)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add args to the command if provided\n    if args:\n        cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            env=env,\n            capture_output=True if not hide_stderr else False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            print(\"Command output:\", result.stdout)\n            \n        return result.returncode, result.stdout if not hide_stderr else \"\"\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(\"Command failed with return code:\", e.returncode)\n            print(\"Error output:\", e.stdout)\n        return e.returncode, e.stdout\n    except Exception as e:\n        if verbose:\n            print(\"Command raised an exception:\", str(e))\n        return -1, str(e)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command list\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    if args:\n        cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr)\n        \n        return result\n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(f\"Command failed with return code {e.returncode}\")\n            if e.stdout:\n                print(e.stdout)\n            if e.stderr:\n                print(e.stderr)\n        raise"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 927, "num_retrieved": 3, "inference_time": 0.9034321010112762, "generate_results": ["def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = os.path.dirname(os.path.abspath(__file__))\n    return get_config_from_root(root)", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = os.path.dirname(os.path.abspath(__file__))\n    return get_config_from_root(root)", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = os.path.dirname(os.path.abspath(__file__))\n    return get_config_from_root(root)", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = os.path.dirname(os.path.abspath(__file__))\n    return get_config_from_root(root)", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = os.path.dirname(os.path.abspath(__file__))\n    return get_config_from_root(root)"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 930, "num_retrieved": 3, "inference_time": 0.9034321010112762, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    root_dir = '/'\n    \n    # Walk through the filesystem starting from root\n    try:\n        for dirpath, dirs, files in ocfl_walk(self, dir=root_dir):\n            # Process files in current directory\n            for filename in files:\n                file_path = fs.path.join(dirpath, filename)\n                num_objects += 1\n                \n                # Validate object if requested\n                if validate_objects:\n                    try:\n                        info = self.getinfo(file_path)\n                        if info.is_dir:\n                            # Skip directories as they are not objects\n                            continue\n                        # Validate object content or metadata based on requirements\n                        # This is a simplified validation - actual validation would depend on OCFL spec\n                        good_objects += 1\n                    except fs.errors.ResourceNotFound:\n                        if show_warnings:\n                            print(f\"Warning: File not found at {file_path}\")\n                        continue\n                else:\n                    # If not validating objects, just count them\n                    good_objects += 1\n                    \n            # Prune directories to avoid descending into unwanted paths\n            # This is handled by ocfl_walk's pruning logic\n            \n    except Exception as e:\n        if show_warnings:\n            print(f\"Error during hierarchy validation: {e}\")\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    root_dir = '/'\n    \n    # Walk through the filesystem starting from root\n    try:\n        for dirpath, dirs, files in ocfl_walk(self, dir=root_dir):\n            # Process files in current directory\n            for filename in files:\n                file_path = fs.path.join(dirpath, filename)\n                num_objects += 1\n                \n                # Validate object if requested\n                if validate_objects:\n                    try:\n                        info = self.getinfo(file_path)\n                        if info.is_dir:\n                            # Skip directories as they are not objects\n                            continue\n                        # Validate object content or metadata as needed\n                        good_objects += 1\n                    except fs.errors.ResourceNotFound:\n                        if show_warnings:\n                            print(f\"Warning: File not found at {file_path}\")\n                        continue\n                else:\n                    # If not validating objects, just count\n                    good_objects += 1\n                    \n            # Prune directories if needed (based on OCFL spec)\n            # Only descend into directories that are not excluded\n            dirs_to_descend = []\n            for d in dirs:\n                dir_path = fs.path.join(dirpath, d)\n                try:\n                    info = self.getinfo(dir_path)\n                    if info.is_dir:\n                        dirs_to_descend.append(d)\n                except fs.errors.ResourceNotFound:\n                    continue\n            # Only descend into valid directories\n            if dirs_to_descend:\n                stack.append(fs.path.join(dirpath, dirs_to_descend[0]))\n                \n    except Exception as e:\n        if show_warnings:\n            print(f\"Warning: Error during hierarchy validation: {e}\")\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    root_dir = '/'\n    \n    # Walk through the filesystem starting from root\n    try:\n        for dirpath, dirs, files in ocfl_walk(self, dir=root_dir):\n            # Process files in current directory\n            for filename in files:\n                file_path = fs.path.join(dirpath, filename)\n                num_objects += 1\n                \n                # Validate object if requested\n                if validate_objects:\n                    try:\n                        info = self.getinfo(file_path)\n                        if info.is_dir:\n                            # Skip directories as they are not objects\n                            continue\n                        # Validate object content or metadata as needed\n                        # Placeholder for actual validation logic\n                        good_objects += 1\n                    except fs.errors.ResourceNotFound:\n                        if show_warnings:\n                            print(f\"Warning: File not found at {file_path}\")\n                        continue\n                else:\n                    # If not validating objects, just count\n                    good_objects += 1\n                    \n            # Prune directories as per OCFL specification\n            # Only descend into directories that are not excluded\n            dirs_to_keep = []\n            for d in dirs:\n                dir_path = fs.path.join(dirpath, d)\n                try:\n                    info = self.getinfo(dir_path)\n                    if info.is_dir:\n                        dirs_to_keep.append(d)\n                except fs.errors.ResourceNotFound:\n                    continue\n            dirs = dirs_to_keep\n            \n    except Exception as e:\n        if show_warnings:\n            print(f\"Error during hierarchy validation: {e}\")\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    root_dir = '/'\n    \n    # Walk through the filesystem starting from root\n    try:\n        for dirpath, dirs, files in ocfl_walk(self, dir=root_dir):\n            # Process files in current directory\n            for filename in files:\n                file_path = fs.path.join(dirpath, filename)\n                num_objects += 1\n                \n                # Validate object if requested\n                if validate_objects:\n                    try:\n                        info = self.getinfo(file_path)\n                        if info.is_dir:\n                            # Skip directories as they are not objects\n                            continue\n                        # Validate object content or metadata as needed\n                        # Placeholder for actual validation logic\n                        good_objects += 1\n                    except fs.errors.ResourceNotFound:\n                        if show_warnings:\n                            print(f\"Warning: File not found at {file_path}\")\n                        continue\n                else:\n                    # If not validating objects, just count\n                    good_objects += 1\n                    \n            # Prune directories if needed (based on OCFL spec)\n            # Only descend into directories that are not excluded\n            dirs_to_descend = []\n            for d in dirs:\n                dir_path = fs.path.join(dirpath, d)\n                try:\n                    info = self.getinfo(dir_path)\n                    if info.is_dir:\n                        dirs_to_descend.append(d)\n                except fs.errors.ResourceNotFound:\n                    continue\n            # Only descend into directories that are valid\n            if not dirs_to_descend:\n                continue\n                \n            # Add directories to stack for further traversal\n            for d in dirs_to_descend:\n                stack.append(fs.path.join(dirpath, d))\n                \n    except Exception as e:\n        if show_warnings:\n            print(f\"Warning: Error during hierarchy validation: {e}\")\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    root_dir = '/'\n    \n    # Walk through the filesystem starting from root\n    try:\n        for dirpath, dirs, files in ocfl_walk(self, dir=root_dir):\n            # Process files in current directory\n            for filename in files:\n                file_path = fs.path.join(dirpath, filename)\n                num_objects += 1\n                \n                # Validate object if requested\n                if validate_objects:\n                    try:\n                        info = self.getinfo(file_path)\n                        if info.is_dir:\n                            # Skip directories as they are not objects\n                            continue\n                        # Validate object content or metadata as needed\n                        # Placeholder for actual validation logic\n                        good_objects += 1\n                    except fs.errors.ResourceNotFound:\n                        if show_warnings:\n                            print(f\"Warning: File not found at {file_path}\")\n                        continue\n                else:\n                    # If not validating objects, just count\n                    good_objects += 1\n                    \n            # Prune directories as per OCFL specification\n            # Only descend into directories that are not excluded\n            dirs_to_descend = []\n            for d in dirs:\n                dir_path = fs.path.join(dirpath, d)\n                try:\n                    info = self.getinfo(dir_path)\n                    if info.is_dir:\n                        dirs_to_descend.append(d)\n                except fs.errors.ResourceNotFound:\n                    continue\n            # Only descend into directories that are valid\n            dirs = dirs_to_descend\n            \n    except Exception as e:\n        if show_warnings:\n            print(f\"Error during hierarchy validation: {e}\")\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 905, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = \"1.0\"\n    self.digest_algorithm = \"sha256\"\n    self.content_directory = \"content\"\n    self.inventory_digest_files = \"inventory.md\"\n    self.root_inv_validator = None\n    self.obj_fs = None", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = \"1.0\"\n    self.digest_algorithm = \"sha256\"\n    self.content_directory = \"content\"\n    self.inventory_digest_files = \"inventory.json\"\n    self.root_inv_validator = None\n    self.obj_fs = None", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = \"1.0\"\n    self.digest_algorithm = \"sha256\"\n    self.content_directory = \"content\"\n    self.inventory_digest_files = \"inventory.md\"\n    self.root_inv_validator = None\n    self.obj_fs = None", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = \"1.0\"\n    self.digest_algorithm = \"sha256\"\n    self.content_directory = \"content\"\n    self.inventory_digest_files = \"inventory.md\"\n    self.root_inv_validator = None\n    self.obj_fs = None", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = \"1.0\"\n    self.digest_algorithm = \"sha256\"\n    self.content_directory = \"content\"\n    self.inventory_digest_files = \"inventory.digest\"\n    self.root_inv_validator = None\n    self.obj_fs = None"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 908, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        \n        # Check if the version has zero padding\n        # Determine padding size by checking if the version starts with zeros\n        # For example, v002 has padding of 3, v003 has padding of 3, etc.\n        padding_size = len(match.group(1))\n        \n        # If the version number is 1, it's v1, already handled\n        if version_num == 1:\n            return 'v2'\n        \n        # Try to find the pattern by checking if there's a zero-prefixed version\n        # with the same padding size\n        # For example, if we have v002, v003, then next should be v004\n        # If we have v2, v3, then next should be v4\n        \n        # Check if the version is zero-prefixed\n        if version_num < 10:\n            # If version is 2-9, it's not zero-prefixed\n            # So we just increment the number\n            next_num = version_num + 1\n            return f'v{next_num}'\n        \n        # If version is >= 10, check if it's zero-prefixed\n        # For zero-prefixed versions, we need to determine the padding\n        # by checking if there's a version with the same padding size\n        # For example, if we have v002, v003, v004, then next is v005\n        # If we have v02, v03, v04, then next is v05\n        \n        # Check if the version has zero padding\n        # If the version is v002, v003, etc., then padding is 3\n        # If the version is v02, v03, etc., then padding is 2\n        \n        # Determine padding size from the version\n        padding_size =", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if not match:\n        return None\n    \n    version_num = int(match.group(1))\n    \n    # Determine if the version uses zero-padding\n    # Check if the version number has leading zeros\n    # We need to find the padding size by checking if there's a pattern\n    # For example, if we have v1, v002, v003, then padding is 2\n    # If we have v1, v2, v3, then no zero-padding\n    \n    # Check if there's a zero-prefixed version with the same number of digits\n    # We'll assume the padding size is the maximum number of digits in any version\n    # that starts with 'v' and has a number\n    \n    # First, check if we have a zero-prefixed version with the same number of digits\n    # as the current version\n    padding_size = 0\n    for v in version.split('v'):\n        if v.isdigit():\n            padding_size = max(padding_size, len(v))\n    \n    # If no padding found, assume no zero-padding\n    if padding_size == 0:\n        return f'v{version_num + 1}'\n    \n    # If we have padding, create the next version with the same padding\n    next_num = version_num + 1\n    return f'v{\"0\" * (padding_size - len(str(next_num)))}{next_num}'", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        \n        # Determine if zero-prefixed by checking if the version has leading zeros\n        # We need to find the pattern length to determine if it's zero-prefixed\n        # For example, v002 has 3 digits, v003 has 3 digits, etc.\n        \n        # Extract the number of digits in the version number\n        num_digits = len(match.group(1))\n        \n        # Try to find the pattern by checking if there's a version with a different number of digits\n        # If we have v002, v003, etc., then the pattern is zero-prefixed with num_digits\n        # If we have v2, v3, etc., then it's non-zero-prefixed\n        \n        # Check if there's a version with the same number of digits but higher\n        # This will help us determine if we're in a zero-prefixed sequence\n        \n        # We'll assume that if we have a version with the same number of digits, \n        # then we're in a zero-prefixed sequence\n        \n        # If we have v2, v3, v4, etc., then the next version is v{num_digits+1}\n        # If we have v002, v003, etc., then the next version is v{num_digits+1} with leading zeros\n        \n        # Check if there's a version with the same number of digits and higher\n        # For example, if we have v002, v003, then next is v004\n        \n        # We'll check if there's a version with the same number of digits and higher\n        # by looking at the version number\n        \n        # If we have v2, v3, v4, then next is v5\n        # If we have v002, v003, then next is v004\n        \n        # So we just increment the number and format", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if not match:\n        return None\n    \n    version_num = int(match.group(1))\n    \n    # Determine if the version uses zero-padding\n    # Check if the version number has leading zeros\n    # We need to find the padding size by checking if there's a pattern\n    # For example, if we have v1, v002, v003, then padding is 2\n    # If we have v1, v2, v3, then no padding\n    \n    # Check if there's a zero-prefixed version with the same length\n    # We'll assume the padding size is the maximum number of digits in any version\n    # that starts with v and has a zero-prefixed format\n    \n    # First, check if we have any zero-prefixed versions\n    # We'll try to find the maximum padding size by checking versions\n    # For example, if we have v1, v002, v003, then padding is 2\n    \n    # Since we don't have access to a full set of versions, we can only infer\n    # from the current version\n    \n    # If the version is not zero-prefixed, assume no padding\n    if version_num < 10:\n        return 'v2'\n    \n    # If version is zero-prefixed, we need to determine the padding size\n    # We'll assume the padding size is the number of digits in the version number\n    # For example, v002 has 3 digits, so padding is 3\n    padding_size = len(str(version_num))\n    \n    # Create the next version with the same padding\n    next_num = version_num + 1\n    next_version_str = f'v{next_num:0{padding_size}d}'\n    \n    return next_version_str", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        \n        # Check if the version has zero padding\n        # If it's v2, v3, etc., it's not zero-prefixed\n        # If it's v002, v003, etc., it's zero-prefixed\n        \n        # Determine padding size by checking if the version has leading zeros\n        # We need to find the pattern of zero padding\n        # For example, v002 means padding of 3 digits\n        \n        # Extract the padding size from the version\n        padding_size = len(version) - 2  # subtract 'v' and the number\n        \n        # If the version is zero-prefixed, the next version should maintain the same padding\n        # For example, v002 -> v003, v003 -> v004, etc.\n        next_num = version_num + 1\n        next_version_str = f'v{next_num:0{padding_size}d}'\n        \n        return next_version_str\n    \n    # If version is not in the expected format, assume it's non-zero prefixed\n    # and just increment the number\n    next_num = int(version[1:]) + 1\n    return f'v{next_num}'"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.9034321010112762, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Validate that each version has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory name is valid (should be v1, v2, etc.)\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Validate sequence - should be in order 1, 2, 3...\n        if version_num != len([v for v in version_dirs if v.startswith('v') and int(v[1:]) < version_num]):\n            self.error(\"E010\")\n            continue\n            \n        # Check if inventory exists for this version\n        # This is a placeholder - actual inventory check would depend on the inventory structure\n        # For now, we assume the presence of inventory is validated elsewhere\n        # We'll record any content digest differences from root inventory\n        # This would typically be done by comparing content digests in the version directory\n        # with those in the root inventory\n        \n        # Placeholder for digest comparison logic\n        # In a real implementation, we would parse the inventory files and compare digests\n        # For now, we just record that we're tracking differences\n        content_digest_diffs[version_dir] = {}\n    \n    # If any version is missing an inventory, report error\n    # This would be more specific in a full implementation\n    if not content_digest_diffs:\n        self.warning(\"W002\")  # No content digest differences found\n    \n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Check that each version has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Version directories should be in sequence, so we expect v1, v2, v3, etc.\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Validate that each version has an inventory\n        # This is a placeholder - actual inventory check would depend on the inventory structure\n        # For now, we just record any differences in content digests\n        # In a real implementation, we would parse the inventory and compare digests\n        \n        # Example: if we had access to the inventory data, we would compare digests\n        # For now, we just record that we're checking for differences\n        content_digest_diffs[version_dir] = {}\n    \n    # If any version is missing an inventory, report error\n    # This would be implemented with actual inventory access\n    # For now, we just return the recorded differences\n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Validate that each version directory has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory has an inventory\n        # This is a simplified check; actual implementation would depend on\n        # how inventory is accessed and validated\n        if not self.has_inventory(version_dir):\n            missing_inventories.append(version_dir)\n            self.error(f\"E012: Missing inventory for version {version_dir}\")\n    \n    # If any inventories are missing, return early\n    if missing_inventories:\n        return\n    \n    # Collect content digest differences from each version compared to root\n    root_inventory = self.get_root_inventory()\n    if not root_inventory:\n        self.error(\"E013: Root inventory not found\")\n        return\n    \n    for version_dir in version_dirs:\n        version_inventory = self.get_inventory(version_dir)\n        if not version_inventory:\n            continue\n            \n        # Compare content digests between root and current version\n        for content_path, digest in version_inventory.get('content_digests', {}).items():\n            root_digest = root_inventory.get('content_digests', {}).get(content_path)\n            if root_digest and root_digest != digest:\n                content_digest_diffs[content_path] = {\n                    'root': root_digest,\n                    'version': digest,\n                    'version_dir': version_dir\n                }\n    \n    # Return the record of content digest differences\n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Validate that each version directory has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory name is valid (should be v1, v2, etc.)\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Validate that each version has an inventory\n        # This is a placeholder - actual inventory check would depend on the inventory structure\n        # For now, we just record that we're checking for inventories\n        if not self.has_inventory(version_dir):\n            missing_inventories.append(version_dir)\n            self.error(f\"E012: Missing inventory for version {version_dir}\")\n    \n    # If any inventories are missing, return early\n    if missing_inventories:\n        return\n    \n    # Collect content digests that differ from root inventory\n    # This would typically involve comparing the content digests in each version\n    # with those in the root inventory\n    root_digests = self.get_root_content_digests()\n    for version_dir in version_dirs:\n        version_digests = self.get_version_content_digests(version_dir)\n        for content_path, digest in version_digests.items():\n            if content_path in root_digests and root_digests[content_path] != digest:\n                content_digest_diffs[content_path] = {\n                    'root': root_digests[content_path],\n                    'version': digest,\n                    'version_dir': version_dir\n                }\n    \n    # Return the record of content digest differences\n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Check that each version directory has an inventory\n    missing_inventories = []\n    for i, version_dir in enumerate(version_dirs):\n        # Version directories should be in sequence: v1, v2, v3, ...\n        expected_version = i + 1\n        version_name = f\"v{expected_version}\"\n        \n        if version_dir != version_name:\n            self.error(f\"E012: Version directory {version_dir} does not match expected sequence {version_name}\")\n            missing_inventories.append(version_dir)\n            continue\n            \n        # Validate that the version directory has an inventory\n        # This is a placeholder - actual inventory validation would depend on the inventory structure\n        # For now, we just ensure the version directory name is valid\n        if not version_dir.startswith('v'):\n            self.error(\"E013: Version directory name must start with 'v'\")\n            missing_inventories.append(version_dir)\n            continue\n    \n    # If any version directories are missing inventories, report error\n    if missing_inventories:\n        self.error(f\"E014: Missing inventories for versions: {', '.join(missing_inventories)}\")\n    \n    # Keep a record of content digests that differ from root inventory\n    # This would typically be implemented by comparing content digests\n    # between root and each version directory\n    # For now, we just return an empty record as no actual digest comparison is done\n    content_digest_diffs = {}\n    \n    return content_digest_diffs"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def find_path_type(path):\n    import os\n    import re\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path) or not os.path.isdir(path):\n        return \"E001c\"\n    \n    # Look for files starting with \"0=\" in the directory\n    files = [f for f in os.listdir(path) if f.startswith('0=')]\n    \n    if not files:\n        return \"root\"  # No 0= files, likely a root or object directory without inventory\n    \n    # Check if any file matches the expected object pattern\n    for file in files:\n        if file.startswith('0=ocfl_object_'):\n            return 'object'\n    \n    # If we have a file that looks like an inventory (inventory.json or inventory.json.<digest>)\n    for file in files:\n        if file == 'inventory.json' or file.startswith('inventory.json.'):\n            return 'file'\n    \n    # If we have a file that doesn't match known patterns\n    return \"E001a\"", "def find_path_type(path):\n    import os\n    import re\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path) or not os.path.isdir(path):\n        return \"E001c\"\n    \n    # Look for files that start with \"0=\" (Namaste files)\n    namaste_files = []\n    for entry in os.scandir(path):\n        if entry.is_file() and entry.name.startswith('0='):\n            namaste_files.append(entry.name)\n    \n    # If no Namaste files found, it's not a valid OCFL root or object\n    if not namaste_files:\n        return \"E001a\"\n    \n    # Check if any file is exactly \"0=ocfl_object_<version>\"\n    ocfl_object_file = None\n    for file in namaste_files:\n        if file == f'0=ocfl_object_{path.split(\"/\")[-1]}':\n            ocfl_object_file = file\n            break\n    \n    # If we found the ocfl_object file, it's an object directory\n    if ocfl_object_file:\n        return 'object'\n    \n    # If we have a file that starts with 0= and is not the ocfl_object file,\n    # it might be an inventory file\n    for file in namaste_files:\n        if file.startswith('0=') and file != ocfl_object_file:\n            return 'file'\n    \n    # If we have a root directory with the ocfl_object file, it's a root\n    if '0=ocfl_object_' in path:\n        return 'root'\n    \n    # Otherwise, return an error description\n    return \"E001b\"", "def find_path_type(path):\n    import os\n    import re\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path) or not os.path.isdir(path):\n        return 'error: path does not exist or is not a directory'\n    \n    # Look for files that start with \"0=\" (Namaste files)\n    namaste_files = []\n    for entry in os.listdir(path):\n        if entry.startswith('0='):\n            namaste_files.append(entry)\n    \n    # If no \"0=\" files found, it's likely a root or object directory\n    if not namaste_files:\n        # Check if it contains an inventory.json file\n        inventory_file = 'inventory.json'\n        if inventory_file in namaste_files:\n            return 'file'\n        \n        # Check if it's a root directory (should have specific expected files)\n        # Look for the version-specific object file\n        version_file_pattern = re.compile(r'0=ocfl_object_\\d+')\n        version_match = version_file_pattern.search(namaste_files[0] if namaste_files else '')\n        \n        if version_match:\n            return 'object'\n        \n        # If no clear indication, assume root\n        return 'root'\n    \n    # If we have \"0=\" files, determine type\n    # Check if any file is inventory.json\n    if 'inventory.json' in namaste_files:\n        return 'file'\n    \n    # Check if any file matches the pattern 0=ocfl_object_<version>\n    version_file_pattern = re.compile(r'0=ocfl_object_\\d+')\n    version_files = [f for f in namaste_files if version_file_pattern.search(f)]\n    \n    if version_files:\n        return 'object'\n    \n    # If none of the above, return error\n    return 'error: unknown path type'", "def find_path_type(path):\n    import os\n    import re\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path) or not os.path.isdir(path):\n        return 'error: path does not exist or is not a directory'\n    \n    # Look for files that start with \"0=\" (Namaste files)\n    namaste_files = []\n    for entry in os.scandir(path):\n        if entry.is_file() and entry.name.startswith('0='):\n            namaste_files.append(entry.name)\n    \n    # If no \"0=\" files found, it's likely a root or object directory\n    if not namaste_files:\n        # Check if the directory contains an inventory.json file\n        inventory_file = None\n        for entry in os.scandir(path):\n            if entry.is_file() and entry.name == 'inventory.json':\n                inventory_file = entry.name\n                break\n        \n        if inventory_file:\n            return 'file'  # Contains inventory.json, likely a file\n        \n        # If no inventory.json and no 0=* files, check if it's a root\n        # OCFL storage root should have a 0=ocfl_object_* file\n        # If it has no such file, it's not a valid root\n        return 'error: missing required 0=ocfl_object_* file'\n    \n    # If we have \"0=*\" files, determine type\n    # OCFL storage root should have a 0=ocfl_object_* file\n    # OCFL object directory should have a 0=ocfl_object_* file\n    # If it has a 0=ocfl_object_* file, it's either root or object\n    # Check if it has a version directory (v1, v2, etc.)\n    version_dirs = []\n    for entry in os.scandir(path):\n        if entry.is_dir() and re.match(r'v\\d+$', entry.name):\n            version_dirs.append(entry.name)\n    \n    # If it has version directories, it's an object\n    if version_dirs:\n        return 'object'\n    \n    # If it has a 0=ocfl_object_* file but no version directories, it's a root\n    # OCFL storage root should have a 0=ocfl_object_* file\n    # But it should not have version directories\n    return 'root'", "def find_path_type(path):\n    import os\n    import re\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path) or not os.path.isdir(path):\n        return \"E001c\"\n    \n    # Look for files that start with \"0=\" (Namaste files)\n    namaste_files = []\n    for entry in os.listdir(path):\n        if entry.startswith('0='):\n            namaste_files.append(entry)\n    \n    # If no Namaste files found, it's not a valid OCFL root or object\n    if not namaste_files:\n        return \"E001a\"\n    \n    # Check if any of the Namaste files match the expected object root pattern\n    # Expected pattern: 0=ocfl_object_<version>\n    pattern = re.compile(r'^0=ocfl_object_[a-zA-Z0-9._-]+$')\n    object_root_match = any(pattern.match(file) for file in namaste_files)\n    \n    # If we have a valid object root file, it's an object directory\n    if object_root_match:\n        return 'object'\n    \n    # If we have inventory.json or inventory.json.<digest> files, it's a file\n    inventory_pattern = re.compile(r'^inventory\\.json$')\n    inventory_digest_pattern = re.compile(r'^inventory\\.json\\.[a-zA-Z0-9.-]+$')\n    \n    inventory_found = any(inventory_pattern.match(file) for file in namaste_files)\n    inventory_digest_found = any(inventory_digest_pattern.match(file) for file in namaste_files)\n    \n    if inventory_found or inventory_digest_found:\n        return 'file'\n    \n    # If we have a Namaste file that doesn't match known patterns, it's an error\n    # Check if it's a version directory (v\\d+)\n    version_pattern = re.compile(r'^v\\d+$')\n    version_dirs = [d for d in os.listdir(path) if version_pattern.match(d)]\n    \n    if version_dirs:\n        return \"E046b\"\n    \n    # Otherwise, it's a root directory\n    return 'root'"]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 924, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    \n    # Ensure the params are properly formatted for Bugzilla queries\n    # This is a placeholder implementation - actual logic may depend on specific requirements\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    \n    # Ensure the params are properly formatted for Bugzilla queries\n    # This is a placeholder implementation - actual logic may depend on specific requirements\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    # Additional logic can be added here based on specific requirements", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    \n    # Ensure the params are properly formatted for Bugzilla queries\n    # This is a placeholder implementation - actual logic may depend on specific requirements\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    \n    # Ensure the params are properly formatted for Bugzilla queries\n    # This is a placeholder implementation - actual logic may depend on specific requirements\n    pass"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.9034321010112762, "generate_results": ["def deep_merge_nodes(nodes):\n    from collections import defaultdict\n    from ruamel.yaml.nodes import ScalarNode, MappingNode\n\n    # Group nodes by their key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n\n    merged = []\n    for key, node_list in key_to_nodes.items():\n        # Get the first node's key (should be the same for all)\n        key_node = node_list[0][0]\n        \n        # If there's only one node, just add it\n        if len(node_list) == 1:\n            merged.append((key_node, node_list[0][1]))\n            continue\n\n        # Otherwise, merge the values\n        value_nodes = [node[1] for node in node_list]\n        \n        # Start with the first value node\n        merged_value = value_nodes[0]\n        \n        # If the first value is a MappingNode, merge the mappings\n        if isinstance(merged_value, MappingNode):\n            merged_mapping = MappingNode()\n            merged_mapping.value = []\n            \n            # Collect all key-value pairs from all value nodes\n            all_pairs = []\n            for value_node in value_nodes:\n                if isinstance(value_node, MappingNode):\n                    for pair in value_node.value:\n                        all_pairs.append(pair)\n                else:\n                    # If it's not a mapping, just use the last value\n                    # But we already processed all nodes, so we need to handle this\n                    pass\n            \n            # Merge the mapping by combining all pairs\n            # Use a dictionary to track values, with last value winning\n            value_dict = {}\n            for pair in all_pairs:\n                key_node_pair = pair[0]\n                value_node_pair = pair[1]\n                value_dict[key_node_pair.value] = value_node_pair\n            \n            # Reconstruct the mapping\n            for key_val, value_val in value_dict.items():\n                merged_mapping.value.append(\n                    (ScalarNode(tag='tag:yaml.org,2002:str', value=key_val), value_val)\n                )\n            \n            merged.append((key_node, merged_mapping))\n        else:\n            # If the value is not a mapping, just take the last one\n            merged.append((key_node, value_nodes[-1]))\n    \n    return merged", "def deep_merge_nodes(nodes):\n    from collections import defaultdict\n    from ruamel.yaml.nodes import ScalarNode, MappingNode\n\n    # Group nodes by their key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n\n    merged = []\n    for key, node_list in key_to_nodes.items():\n        # If there's only one node, just add it\n        if len(node_list) == 1:\n            merged.append((node_list[0][0], node_list[0][1]))\n            continue\n\n        # Otherwise, merge the values\n        # Start with the first value node\n        merged_value = node_list[0][1]\n        \n        # For each subsequent node, merge its value into the current merged_value\n        for _, value_node in node_list[1:]:\n            # If the value node is a MappingNode, merge it recursively\n            if isinstance(value_node, MappingNode):\n                # If merged_value is not a MappingNode, convert it to one\n                if not isinstance(merged_value, MappingNode):\n                    merged_value = MappingNode()\n                \n                # Deep merge the two mapping nodes\n                for key_node, val_node in value_node.value:\n                    # If the key already exists in merged_value, we need to merge the values\n                    # Otherwise, just add the new key-value pair\n                    found = False\n                    for existing_key_node, existing_val_node in merged_value.value:\n                        if existing_key_node.value == key_node.value:\n                            # If the existing value is a MappingNode, merge recursively\n                            if isinstance(existing_val_node, MappingNode):\n                                # Merge the two mapping nodes\n                                merged_value.value = merge_mapping_nodes(merged_value.value, [(key_node, val_node)])\n                            else:\n                                # Otherwise, just replace with the new value\n                                existing_val_node.value = val_node.value\n                            found = True\n                            break\n                    if not found:\n                        # Add the new key-value pair\n                        merged_value.value.append((key_node, val_node))\n            else:\n                # If the value node is not a MappingNode, just take the last value\n                # (since non-MappingNode values are overwritten by the last one)\n                # We don't need to do anything special here because we're just replacing\n                # the value in the merged node\n                pass", "def deep_merge_nodes(nodes):\n    from collections import defaultdict\n    from ruamel.yaml.nodes import ScalarNode, MappingNode\n\n    # Group nodes by their key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n\n    merged = []\n    for key, node_list in key_to_nodes.items():\n        # If there's only one node, just add it\n        if len(node_list) == 1:\n            merged.append((node_list[0][0], node_list[0][1]))\n            continue\n\n        # Otherwise, merge the values\n        # Start with the first value node\n        merged_value = node_list[0][1]\n        \n        # For each subsequent node, merge its value into the current merged_value\n        for _, value_node in node_list[1:]:\n            # If the value node is a MappingNode, merge it recursively\n            if isinstance(value_node, MappingNode):\n                # If merged_value is not a MappingNode, convert it to one\n                if not isinstance(merged_value, MappingNode):\n                    merged_value = MappingNode()\n                \n                # Deep merge the two mapping nodes\n                merged_value.value.extend(value_node.value)\n            else:\n                # If the value node is not a MappingNode, just take the last value\n                # (since non-MappingNode values overwrite previous ones)\n                pass\n        \n        # Add the merged key-value pair\n        merged.append((node_list[0][0], merged_value))\n\n    return merged", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support, for"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 953, "num_retrieved": 3, "inference_time": 0.9034321010112762, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.9034321010112762, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(prog)s 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different commands (example: backup, prune, etc.)\n    # This is a simplified version - actual implementation would include all subcommands\n    \n    # Example subcommand: backup\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--exclude', nargs='+', help='Files or patterns to exclude')\n    backup_parser.add_argument('--include', nargs='+', help='Files or patterns to include')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example subcommand: prune\n    prune_parser = subparsers.add_parser('prune', help='Prune command')\n    prune_parser.add_argument('--keep', type=int, default=1,", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(prog)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different commands (example: backup, prune, etc.)\n    # This is a simplified version - actual implementation would include all subcommands\n    \n    # Example subcommand: backup\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--exclude', nargs='*', help='Files or patterns to exclude')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example subcommand: prune\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    prune_parser.add_argument('--keep', type=int, default=1, help='Number of backups to keep')\n    prune_parser.add_argument('--json', action='store_true',", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='*', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', default=[], help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Log file path')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different commands (example: backup, prune, etc.)\n    # This is a simplified version - actual implementation would include all subcommands\n    \n    # Example subcommand: backup\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', type=str, required=True, help='Repository path')\n    backup_parser.add_argument('--exclude', nargs='*', help='Files or patterns to exclude')\n    backup_parser.add_argument('--include', nargs='*', help='Files or patterns to include')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example subcommand: prune\n    prune_parser = subparsers.add_parser('prune', help='Prune command')\n    prune_parser.add_argument('--repo',", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(prog)s 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Log file path')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different commands (example: backup, prune, etc.)\n    # This is a simplified version - actual implementation would include all subcommands\n    \n    # Example subcommand: backup\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('--repo', type=str, required=True, help='Repository path')\n    backup_parser.add_argument('--exclude', nargs='+', help='Files or patterns to exclude')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example subcommand: prune\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    prune_parser.add_argument('--keep', type=int, default=1, help='Number of backups to keep')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(prog)s 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Example subparser (add more as needed)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict with subparser name as key\n    result = {'global': args}\n    \n    # Add subparser arguments if they exist\n    if hasattr(args, 'subparser_name') and args.subparser_name:\n        subparser_args = args.__dict__.copy()\n        subparser_args.pop('subparser_name', None)\n        result[args.subparser_name] = argparse.Namespace"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Create a namespace to hold the parsed arguments\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Process each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Create an argument parser instance for this subparser\n            # We use the same parser instance to parse the arguments\n            args = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = args\n            # Remove the parsed arguments from the remaining list\n            remaining_args = []\n            # Break out of the loop since we've processed all arguments\n            break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # If no subparser was able to parse the arguments, return empty dict and original args\n    if not parsed_args:\n        parsed_args = {}\n    \n    return (parsed_args, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Create a namespace to hold the parsed arguments\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Process each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Create an argument parser instance for this subparser\n            # We use the same parser instance to parse the arguments\n            args = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = args\n            # Remove the parsed arguments from the remaining list\n            remaining_args = []\n            # Break out of the loop since we've processed all arguments\n            break\n        except SystemExit:\n            # If the parser raises SystemExit, it means the arguments are not valid\n            # for this subparser, so we continue to the next one\n            pass\n    \n    # If no subparser was able to parse the arguments, return an empty dict\n    # and the original list of arguments\n    if not parsed_args:\n        return {}, unparsed_arguments\n    \n    # Return the parsed arguments and the remaining arguments\n    return parsed_args, remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Create a namespace to hold the parsed arguments\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Process each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Create an argument parser instance for this subparser\n            # We use the same parser instance to parse arguments\n            args = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = args\n            # Remove the parsed arguments from the remaining list\n            remaining_args = []\n            # We break here because we only want to parse once per subparser\n            break\n        except SystemExit:\n            # If the parser raises SystemExit, it means the arguments are not valid\n            # We still want to continue with other subparsers\n            pass\n    \n    # If no subparser was able to parse the arguments, return empty dict and original args\n    if not parsed_args:\n        return {}, unparsed_arguments\n    \n    # Return the parsed arguments and remaining arguments\n    return parsed_args, remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Create a namespace to hold the parsed arguments\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Process each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Create an argument parser instance for this subparser\n            # We use the same parser instance to parse arguments\n            args = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = args\n            # Remove the parsed arguments from remaining_args\n            remaining_args = remaining_args[args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit the program\n            # Just skip this subparser and continue\n            pass\n    \n    # If no subparser was able to parse arguments, return empty dict and original args\n    if not parsed_args:\n        parsed_args = {}\n    \n    return (parsed_args, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Create a namespace to hold the parsed arguments\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Process each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Create an argument parser instance for this subparser\n            # We use the same parser instance to parse the arguments\n            args = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = args\n            # Remove the parsed arguments from the remaining list\n            remaining_args = []\n            # Break out of the loop since we've processed all arguments\n            break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # If no subparser was able to parse the arguments, return empty dict and original args\n    if not parsed_args:\n        parsed_args = {}\n    \n    return (parsed_args, remaining_args)"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 942, "num_retrieved": 3, "inference_time": 0.9476075917482376, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool for Borg')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparser for backup\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--config', help='Configuration file')\n    \n    # Add subparser for restore\n    restore_parser = subparsers.add_parser('restore', help='Restore command')\n    restore_parser.add_argument('--repo', help='Repository path')\n    restore_parser.add_argument('--target', help='Target path')\n    \n    # Add subparser for list\n    list_parser = subparsers.add_parser('list', help='List command')\n    list_parser.add_argument('--repo', help='Repository path')\n    \n    # Add global flags\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add common flags to top-level parser\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    # Add subparsers for different actions\n    # Example subparsers (these would be defined based on actual commands)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore from a backup')\n    \n    # Add common arguments to subparsers\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('path', nargs='?', help='Path to backup')\n    \n    restore_parser.add_argument('archive', help='Archive to restore')\n    restore_parser.add_argument('path', nargs='?', help='Path to restore to')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    # Add common global flags\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run')\n    \n    # Create subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparser for backup\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--target', help='Target path')\n    \n    # Add subparser for restore\n    restore_parser = subparsers.add_parser('restore', help='Restore from a backup')\n    restore_parser.add_argument('--repo', help='Repository path')\n    restore_parser.add_argument('--target', help='Target path')\n    \n    # Add subparser for list\n    list_parser = subparsers.add_parser('list', help='List archives or paths')\n    list_parser.add_argument('--repo', help='Repository path')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add common flags to top-level parser\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run')\n    \n    # Add subparsers for actions\n    # Example: backup, restore, prune, etc.\n    backup_parser = subparsers.add_parser('backup', help='Backup archives')\n    restore_parser = subparsers.add_parser('restore', help='Restore archives')\n    prune_parser = subparsers.add_parser('prune', help='Prune old archives')\n    \n    # Add common arguments to subparsers\n    backup_parser.add_argument('repository', help='Repository path')\n    restore_parser.add_argument('archive', help='Archive to restore')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparsers for different actions\n    # Example subparsers (these would be defined based on actual commands)\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore data from a backup')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    \n    # Add common arguments to top-level parser\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    return parser, subparsers"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.9476075917482376, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.9476075917482376, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    bootstrap_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('--config', help='Configuration file path')\n    generate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('--config', help='Configuration file path')\n    validate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Other actions can be added here as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    bootstrap_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('--config', help='Configuration file path')\n    generate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('--config', help='Configuration file path')\n    validate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Other actions can be added here as needed\n    \n    # Add common arguments\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    bootstrap_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('--config', help='Configuration file path')\n    generate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('--config', help='Configuration file path')\n    validate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Other actions can be added here as needed\n    \n    # Add common arguments\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    bootstrap_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('--config', help='Configuration file path')\n    generate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('--config', help='Configuration file path')\n    validate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Other actions can be added here as needed\n    \n    # Add common arguments\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore tool')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    bootstrap_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration files')\n    generate_parser.add_argument('--config', help='Configuration file path')\n    generate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--config', help='Configuration file path')\n    validate_parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Other actions can be added here as needed\n    \n    # Add common arguments\n    parser.add_argument('--global', action='store_true', help='Use global configuration')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 889, "num_retrieved": 1, "inference_time": 0.9476075917482376, "generate_results": ["def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic() {{\n                local cur prev words cword\n                _init_completion || return\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword={COMP_CWORD}\n\n                # Check if the current word is a command or an option\n                if [[ \"${{words[cword]}}\" == \"--\"* ]]; then\n                    # Handle options\n                    case \"${{cur}}\" in\n                        {all_action_parsers})\n                            COMPREPLY=($(compgen -W \"${{all_action_parsers}}\" -- \"${{cur}}\"))\n                            ;;\n                        *)\n                            # Fallback for unknown options\n                            COMPREPLY=()\n                            ;;\n                    esac\n                elif [[ \"${{words[cword-1]}}\" == \"borgmatic\" ]]; then\n                    # Handle subcommands\n                    COMPREPLY=($(compgen -W \"{all_action_parsers}\" -- \"${{cur}}\"))\n                else\n                    # Handle other cases\n                    COMPREPLY=()\n                fi\n            }}\n\n            complete -F _borgmatic borgmatic\n            '''\n        )\n    )", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic() {{\n                local cur prev words cword\n                _init_completion || return\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword={COMP_CWORD}\n\n                # Check if the current word is a command or an option\n                if [[ \"${{words[cword]}}\" == \"--\"* ]]; then\n                    # Handle options\n                    local options=\"{all_action_parsers}\"\n                    COMPREPLY=($(compgen -W \"${options}\" -- \"${cur}\"))\n                elif [[ \"${{words[cword]}}\" == \"borgmatic\" ]]; then\n                    # Handle subcommands\n                    COMPREPLY=($(compgen -W \"{all_action_parsers}\" -- \"${cur}\"))\n                fi\n            }}\n\n            complete -F _borgmatic borgmatic\n            '''\n        )\n    )", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic() {{\n                local cur prev words cword\n                _init_completion || return\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword={COMP_CWORD}\n\n                # Check if the current word is a command or an option\n                if [[ \"${{cur}}\" == -* ]]; then\n                    # Handle options\n                    case \"${{cur}}\" in\n                        {-*}\n                            # Match exact options\n                            for option in {exact_option_args}; do\n                                if [[ \"${{option}}\" == \"${{cur}}\" || \"${{option}}\" == \"${{cur#-}}\" ]]; then\n                                    COMPREPLY+=(\"${{option}}\")\n                                fi\n                            done\n                            ;;\n                    esac\n                else\n                    # Handle commands\n                    COMPREPLY=($(compgen -W \"{all_action_parsers}\" -- \"${{cur}}\"))\n                fi\n            }}\n\n            complete -F _borgmatic borgmatic\n            '''\n        )\n    )", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic() {{\n                local cur prev words cword\n                _init_completion || return\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword={COMP_CWORD}\n\n                # Check if the current word is a command or an option\n                if [[ \"${{words[cword]}}\" == \"--\"* ]]; then\n                    # Handle options\n                    local options\n                    options=$(printf '%s\\n' {exact_option_args} | grep -E \"^[--].*$\" | tr '\\n' ' ')\n                    COMPREPLY=($(compgen -W \"${options}\" -- \"${cur}\"))\n                elif [[ \"${{words[cword-1]}}\" == \"borgmatic\" ]]; then\n                    # Handle commands\n                    COMPREPLY=($(compgen -W \"{all_action_parsers}\" -- \"${cur}\"))\n                fi\n            }}\n\n            complete -F _borgmatic borgmatic\n            '''\n        )\n    )", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic() {{\n                local cur prev words cword\n                _init_completion || return\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword={COMP_CWORD}\n\n                # Check if the current word is a command\n                if [[ \"${{words[cword]}}\" == \"borgmatic\" ]]; then\n                    # Complete actions\n                    COMPREPLY=( {all_action_parsers} )\n                elif [[ \"${{words[cword]}}\" =~ ^- ]]; then\n                    # Complete options\n                    COMPREPLY=( {exact_option_args} )\n                fi\n            }}\n\n            complete -F _borgmatic borgmatic\n            '''\n        )\n    )"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 914, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--config', help='Configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    \n    # Add subparser arguments (example)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if present\n    if hasattr(args, 'subparser_name') and args.subparser_name:\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--config', help='Configuration file')\n    global_parser.add_argument('--log-level', help='Log level', default='info')\n    \n    # Add subparser arguments (example)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Handle global arguments\n    result['global'] = args\n    \n    # Handle subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--config', help='Configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    \n    # Add subparser arguments (example)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    \n    # Add more subparsers as needed\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Handle global arguments\n    if hasattr(args, 'config'):\n        result['global'] = args\n    \n    # Handle subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--config', help='Configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    \n    # Add subparser arguments (example)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--exclude', help='Exclude pattern')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Handle global arguments\n    if hasattr(args, 'config'):\n        result['global'] = args\n    \n    # Handle subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--config', help='Configuration file')\n    global_parser.add_argument('--log-level', help='Log level', default='info')\n    \n    # Add subparser arguments (example)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--exclude', help='Exclude pattern', action='append')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Handle global arguments\n    result['global'] = args\n    \n    # Handle subparser arguments\n    if hasattr(args, 'subparser_name') and args.subparser_name:\n        result[args.subparser_name] = args\n    \n    return result"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.9476075917482376, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return (arguments, remaining_arguments)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 920, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n\n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO', help='Log level')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    global_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done without performing the action')\n    \n    # Create subparsers for actions\n    action_parsers = {}\n    action_subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example actions (these would be defined in the actual implementation)\n    actions = [\n        ('backup', 'Backup a repository'),\n        ('restore', 'Restore data from a repository'),\n        ('prune', 'Prune old backups'),\n        ('check', 'Check repository integrity'),\n        ('list', 'List repositories'),\n    ]\n    \n    for action_name, help_text in actions:\n        action_parser = action_subparsers.add_parser(action_name, help=help_text)\n        action_parser.add_argument('--repo', help='Repository path')\n        action_parser.add_argument('--target', help='Target path')\n        action_parser.add_argument('--exclude', action='append', help='Files or directories to exclude')\n        action_parsers[action_name] = action_parser\n    \n    return (parser, action_parsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_parsers as _make_parsers\n\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific structure of borgmatic's argument parsing\n    # logic which is not fully provided in the context.\n    # The function is expected to return a tuple of (global_parser, action_parsers, global_plus_action_parser)\n    \n    # Simulating the structure based on the context\n    global_parser = argparse.ArgumentParser(description=\"Borgmatic command-line tool\")\n    action_parsers = {}\n    global_plus_action_parser = global_parser\n    \n    # Example subparsers (these would be populated with actual action parsers)\n    subparsers = global_parser.add_subparsers(dest=\"action\", help=\"Available actions\")\n    \n    # Example actions (these would be actual actions defined in the code)\n    actions = [\"backup\", \"prune\", \"check\", \"list\", \"version\"]\n    for action in actions:\n        action_parser = subparsers.add_parser(action, help=f\"Perform {action} operation\")\n        action_parsers[action] = action_parser\n    \n    # Global plus action parser combines global arguments with action-specific ones\n    global_plus_action_parser = global_parser\n    \n    return (global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_parsers as _make_parsers\n\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific structure of borgmatic's argument parsing\n    # logic which is not fully provided in the context.\n    # The function should return a tuple of (global_parser, action_parsers, global_plus_action_parser)\n    \n    # Since the actual implementation details are not provided in the context,\n    # we return a dummy implementation that matches the expected tuple structure.\n    # In a real implementation, this would parse command-line arguments and\n    # build the appropriate parser hierarchy.\n    \n    global_parser = argparse.ArgumentParser(description=\"Borgmatic command-line tool\")\n    action_parsers = {}\n    global_plus_action_parser = global_parser\n    \n    # Example subparsers (these would be populated based on actual actions)\n    subparsers = global_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example actions (these would be actual actions defined in the code)\n    actions = ['backup', 'prune', 'check', 'list']\n    for action in actions:\n        action_parser = subparsers.add_parser(action, help=f'Perform {action} operation')\n        action_parsers[action] = action_parser\n    \n    return (global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_parsers as _make_parsers\n\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific structure of borgmatic's argument parsing\n    # logic which is not fully provided in the context.\n    # The function should return a tuple of (global_parser, action_parsers, global_plus_action_parser)\n    \n    # Simulate the structure based on the context\n    global_parser = argparse.ArgumentParser(description=\"Borgmatic command-line tool\")\n    action_parsers = {}\n    global_plus_action_parser = global_parser\n    \n    # Example actions and their parsers (these would be populated based on actual command definitions)\n    actions = ['backup', 'prune', 'check', 'list', 'version']\n    for action in actions:\n        action_parser = argparse.ArgumentParser()\n        action_parser.add_argument('--config', help='Path to configuration file')\n        action_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n        action_parsers[action] = action_parser\n    \n    # Combine global and action parsers\n    global_plus_action_parser = global_parser\n    \n    return (global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_parsers as _make_parsers\n\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific structure of borgmatic's command-line arguments.\n    # The function is expected to return a tuple of (global_parser, action_parsers, global_plus_action_parser)\n    \n    # Simulate the actual parser construction based on the context\n    parser = argparse.ArgumentParser(description='Borgmatic command-line tool')\n    \n    # Create subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example actions (these would be populated based on actual command definitions)\n    actions = {\n        'backup': argparse.ArgumentParser(description='Perform a backup'),\n        'restore': argparse.ArgumentParser(description='Restore a backup'),\n        'prune': argparse.ArgumentParser(description='Prune old backups'),\n    }\n    \n    for action_name, action_parser in actions.items():\n        action_parser.add_argument('--config', help='Configuration file')\n        action_parser.add_argument('--repo', help='Repository path')\n    \n    # Global parser with common flags\n    global_parser = parser\n    \n    # Global plus action parser (combines global flags with action-specific ones)\n    global_plus_action_parser = argparse.ArgumentParser()\n    global_plus_action_parser.add_argument('--config', help='Configuration file')\n    global_plus_action_parser.add_argument('--repo', help='Repository path')\n    global_plus_action_parser.add_argument('--verbose', action='store_true', help='Verbose output')\n    \n    # Return the tuple as expected\n    return (global_parser, actions, global_plus_action_parser)"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 923, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n\n    total = len(response)\n    page_size = max_results if max_results > 0 else 1000\n    \n    for i in range(0, total, page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n\n    total_items = len(response)\n    page_size = max_results if max_results > 0 else 1000\n    \n    for i in range(0, total_items, page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    total_objects = len(response)\n    page_size = max_results if max_results > 0 else 1000\n    \n    for i in range(0, total_objects, page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n\n    total_items = len(response)\n    page_size = max_results if max_results > 0 else 1000\n    \n    for i in range(0, total_items, page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n\n    total = len(response)\n    page_size = max_results if max_results > 0 else 1000\n    \n    for i in range(0, total, page_size):\n        yield response[i:i + page_size]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 952, "num_retrieved": 3, "inference_time": 0.9476075917482376, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = ''\n    num_part = size\n    \n    # Extract the unit if present\n    for char in size:\n        if char.upper() in ['K', 'M', 'G', 'T', 'P', 'E']:\n            unit = char.upper()\n            num_part = size[:size.index(char.upper())]\n            break\n    \n    # If no unit, assume it's in bytes\n    if not unit:\n        try:\n            return int(num_part)\n        except ValueError:\n            return 0\n    \n    # Convert the number part to integer\n    try:\n        num = int(num_part)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return num * 1000\n    elif unit == 'M':\n        return num * 1000000\n    elif unit == 'G':\n        return num * 1000000000\n    elif unit == 'T':\n        return num * 1000000000000\n    elif unit == 'P':\n        return num * 1000000000000000\n    elif unit == 'E':\n        return num * 1000000000000000000\n    \n    return num", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = ''\n    value_part = size\n    \n    # Extract the unit if present\n    if 'K' in size:\n        unit = 'K'\n        value_part = size.replace('K', '').strip()\n    elif 'M' in size:\n        unit = 'M'\n        value_part = size.replace('M', '').strip()\n    elif 'G' in size:\n        unit = 'G'\n        value_part = size.replace('G', '').strip()\n    elif 'T' in size:\n        unit = 'T'\n        value_part = size.replace('T', '').strip()\n    else:\n        # No unit, assume it's in bytes\n        value_part = size\n    \n    # Convert value part to integer\n    try:\n        value = int(value_part)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1000\n    elif unit == 'M':\n        return value * 1000000\n    elif unit == 'G':\n        return value * 1000000000\n    elif unit == 'T':\n        return value * 1000000000000\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = ''\n    num_part = size\n    \n    # Extract the number part and unit\n    for i, char in enumerate(size):\n        if char.isdigit() or char == '.':\n            continue\n        if char.upper() in ['K', 'M', 'G', 'T', 'P', 'E']:\n            unit = char.upper()\n            num_part = size[:i]\n            break\n    \n    # If no unit found, assume it's in bytes\n    if not unit:\n        try:\n            return int(num_part)\n        except ValueError:\n            return 0\n    \n    # Convert the number part to float\n    try:\n        num = float(num_part)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return int(num * 1000)\n    elif unit == 'M':\n        return int(num * 1000000)\n    elif unit == 'G':\n        return int(num * 1000000000)\n    elif unit == 'T':\n        return int(num * 1000000000000)\n    elif unit == 'P':\n        return int(num * 1000000000000000)\n    elif unit == 'E':\n        return int(num * 1000000000000000000)\n    \n    return int(num)", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = ''\n    value_part = size\n    \n    # Extract the unit if present\n    if size.endswith('K'):\n        value_part = size[:-1]\n        unit = 'K'\n    elif size.endswith('M'):\n        value_part = size[:-1]\n        unit = 'M'\n    elif size.endswith('G'):\n        value_part = size[:-1]\n        unit = 'G'\n    elif size.endswith('T'):\n        value_part = size[:-1]\n        unit = 'T'\n    else:\n        # No unit, assume it's in bytes\n        value_part = size\n    \n    # Convert value part to integer\n    try:\n        value = int(value_part)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1000\n    elif unit == 'M':\n        return value * 1000000\n    elif unit == 'G':\n        return value * 1000000000\n    elif unit == 'T':\n        return value * 1000000000000\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = ''\n    num_part = ''\n    \n    for char in size:\n        if char.isdigit() or char == '.':\n            num_part += char\n        else:\n            unit = char.upper()\n    \n    # If no unit is found, assume it's in bytes\n    if not unit:\n        return int(num_part)\n    \n    # Convert to bytes based on unit\n    unit_value = {\n        'K': 1000,\n        'M': 1000000,\n        'G': 1000000000\n    }\n    \n    try:\n        num = int(num_part)\n        return num * unit_value.get(unit, 1)\n    except (ValueError, TypeError):\n        return 0"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 885, "num_retrieved": 1, "inference_time": 0.9476075917482376, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            if key in result:\n                result[key] += value\n            else:\n                result[key] = value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            if key in result:\n                result[key] += value\n            else:\n                result[key] = value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            if key in result:\n                result[key] += value\n            else:\n                result[key] = value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            if key in result:\n                result[key] += value\n            else:\n                result[key] = value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            if key in result:\n                result[key] += value\n            else:\n                result[key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 917, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    for key, value in url_args.items():\n        url = url.replace(f\"{{{{{{{key}}}}}}}\", str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    for key, value in url_args.items():\n        url = url.replace(f\"{{{{{{{key}}}}}}}\", str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    for key, value in url_args.items():\n        url = url.replace(f'{{{{{{{key}}}}}}}', str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    for key, value in url_args.items():\n        url = url.replace(f'{{{{{{{key}}}}}}}', str(value))\n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 889, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', 'n/a', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', 'n/a', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', 'n/a', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', 'n/a', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', 'n/a', ''])"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.9476075917482376, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.9476075917482376, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Warning: {abs_path} has already been processed.\")\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: {absolute_path} has already been processed.\")\n        return True\n    processed.add(absolute_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Warning: {abs_path} has already been processed.\")\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: {absolute_path} has already been processed.\")\n        return True\n    processed.add(absolute_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Warning: {abs_path} has already been processed.\")\n        return True\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 997, "num_retrieved": 5, "inference_time": 0.7312778830528259, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and the two adjacent vertices (cyclically)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and the two adjacent vertices (cyclically)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and the two adjacent vertices (cyclically)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and two neighbors (with wraparound)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and the two adjacent vertices (cyclically)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 893, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1038, "num_retrieved": 6, "inference_time": 0.7312778830528259, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if isinstance(flow_params, dict):\n        for key, value in flow_params.items():\n            if isinstance(value, dict) and register_value in value:\n                flow_params[key][register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if isinstance(flow_params, dict):\n        for key, value in flow_params.items():\n            if isinstance(value, dict) and register_value in value:\n                flow_params[key][register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if isinstance(flow_params, dict):\n        for key, value in flow_params.items():\n            if isinstance(value, dict) and register_value in value:\n                flow_params[key][register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if isinstance(flow_params, dict):\n        for key, value in flow_params.items():\n            if isinstance(value, dict) and register_value in value:\n                flow_params[key][register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if isinstance(flow_params, dict):\n        for key, value in flow_params.items():\n            if isinstance(value, dict) and register_value in value:\n                flow_params[key][register_value] = register_number\n    return flow_params"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 932, "num_retrieved": 3, "inference_time": 0.7312778830528259, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 981, "num_retrieved": 5, "inference_time": 0.7312778830528259, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet.index(char)\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet.index(char)\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet.index(char)\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet.index(char)\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet.index(char)\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 952, "num_retrieved": 3, "inference_time": 0.7312778830528259, "generate_results": ["import requests\nimport os\nimport tarfile\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the .tar.gz archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create the destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive to the destination path\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport os\nfrom pathlib import Path\nimport subprocess\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the .tar.gz archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / \"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    # Use tar to extract the .tar.gz file\n    subprocess.check_output([\"tar\", \"xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    # Return the destination path where the archive was extracted\n    return destination_path", "import requests\nimport os\nfrom pathlib import Path\nimport subprocess\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the .tar.gz archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / \"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    # Use tar to extract the .tar.gz file\n    subprocess.check_output([\"tar\", \"xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    # Return the destination path where the archive was extracted\n    return destination_path", "import requests\nimport os\nfrom pathlib import Path\nimport subprocess\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the .tar.gz archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / \"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    # Use tar to extract the .tar.gz file\n    subprocess.check_output([\"tar\", \"xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    # Return the destination path where the archive was extracted\n    return destination_path", "import requests\nimport os\nimport tarfile\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create the destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1011, "num_retrieved": 5, "inference_time": 0.7312778830528259, "generate_results": ["import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.path.exists('/System/Library/CoreServices/SystemVersion.plist')", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.path.basename(os.uname().sysname) == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.path.basename(os.uname().sysname) == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.path.basename(os.uname().sysname) == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.path.exists('/usr/bin/xcrun')"]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at the start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at the start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1109, "num_retrieved": 10, "inference_time": 0.7312778830528259, "generate_results": ["def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return unquote_url(name)", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return unquote_url(name)", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return unquote_url(name)", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return unquote_url(name)", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return unquote_url(name)"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if os.name == 'nt':\n            platform = 0\n        else:\n            platform = 1\n\n    # Regex pattern to match quoted strings and unquoted tokens\n    # Handles spaces, quotes, and escaped characters\n    if platform == 1:  # POSIX (Linux, macOS, etc.)\n        # POSIX shell splitting: split on whitespace, but preserve quoted strings\n        pattern = r'([^\"\\'\\s]|\"[^\"]*\"[^\"]*|\\'[^\\']*\\')'\n        tokens = re.findall(pattern, s)\n        # Remove empty tokens and strip quotes\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return [t.strip() for t in result if t.strip()]\n    else:  # Windows (CMD)\n        # Windows CMD splitting: split on spaces, but preserve quoted strings\n        # Use a more robust pattern for Windows\n        pattern = r'([^\" ]+|\"[^\"]*\")'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return [t.strip() for t in result if t.strip()]", "import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if os.name == 'nt':\n            platform = 0\n        else:\n            platform = 1\n\n    if platform == 0:  # Windows/CMD\n        # Windows uses spaces and quotes, with quotes being escaped\n        # Use regex to split on spaces, but preserve quoted strings\n        pattern = r'(?:\"([^\"]*)\"|\\'([^\\']*)\\'|\\S+)'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            if token[0] is not None:\n                result.append(token[0])\n            elif token[1] is not None:\n                result.append(token[1])\n            else:\n                result.append(token[2] if token[2] else '')\n        return result\n\n    else:  # POSIX (Linux, macOS, etc.)\n        # POSIX uses shell-like splitting with quotes\n        pattern = r'([^\"\\s]+|\"[^\"]*\")'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return result", "import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if os.name == 'nt':\n            platform = 0\n        else:\n            platform = 1\n\n    if platform == 0:  # Windows/CMD\n        # Windows uses quotes and spaces to separate arguments\n        # Split on spaces but preserve quoted strings\n        pattern = r'(?:\"([^\"]*)\"|\\'([^\\']*)\\'|\\S+)'\n        matches = re.findall(pattern, s)\n        result = []\n        for match in matches:\n            if match[0]:  # quoted with \"\n                result.append(match[0])\n            elif match[1]:  # quoted with '\n                result.append(match[1])\n            else:  # unquoted\n                result.append(match[0])\n        return result\n\n    else:  # POSIX (Linux, macOS, etc.)\n        # POSIX uses shell-like splitting with quoted strings\n        pattern = r'(?:\"([^\"]*)\"|\\'([^\\']*)\\'|\\S+)'\n        matches = re.findall(pattern, s)\n        result = []\n        for match in matches:\n            if match[0]:  # quoted with \"\n                result.append(match[0])\n            elif match[1]:  # quoted with '\n                result.append(match[1])\n            else:  # unquoted\n                result.append(match[0])\n        return result", "import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if os.name == 'nt':\n            platform = 0\n        else:\n            platform = 1\n\n    if platform == 0:  # Windows/CMD\n        # In Windows, split on spaces, but preserve quoted strings\n        # Use regex to match quoted strings and unquoted tokens\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                tokens.append(match.group(1))\n            else:  # unquoted token\n                tokens.append(match.group(2))\n        return tokens\n\n    else:  # POSIX (Linux, macOS, etc.)\n        # POSIX split using shell-like rules\n        # Split on whitespace, but preserve quoted strings\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                tokens.append(match.group(1))\n            else:  # unquoted token\n                tokens.append(match.group(2))\n        return tokens", "import re\nimport shlex\nimport sys\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if sys.platform.startswith('win'):\n            platform = 0\n        else:\n            platform = 1\n    \n    if platform == 0:  # Windows/CMD\n        # Windows uses quotes and spaces, so we use a simple regex to split on spaces\n        # but preserve quoted strings\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        result = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                result.append(match.group(1))\n            else:  # unquoted token\n                result.append(match.group(2))\n        return result\n    \n    else:  # POSIX (Linux, macOS, etc.)\n        # POSIX uses shell-like splitting with quoted strings\n        # Use shlex.split as a fallback for robustness\n        return shlex.split(s)"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 950, "num_retrieved": 3, "inference_time": 0.7312778830528259, "generate_results": ["import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Ensure tmp_path is a Path object\n    tmp_path = Path(tmp_path)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / \"extracted_repo\"\n    extract_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the base name for the extracted files\n    if filename is None:\n        filename = os.path.basename(archive_path)\n    \n    # Extract the archive using tar\n    try:\n        # Use tar to extract the archive\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Construct the repo URL based on the extracted directory structure\n    # Assuming the extracted directory contains a single directory with the package name\n    # which will be used as the origin URL\n    extracted_dir = extract_dir\n    if not extracted_dir.exists():\n        raise RuntimeError(f\"Extracted directory not found: {extract_dir}\")\n    \n    # Get the first directory name in the extracted path\n    repo_name = next(iter(extracted_dir.iterdir()), None)\n    if repo_name is None:\n        raise RuntimeError(\"No repository directory found in extracted archive\")\n    \n    # Return the repo URL as a string (e.g., file://path/to/repo)\n    repo_url = f\"file://{str(repo_name)}\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Ensure tmp_path is a Path object\n    tmp_path = Path(tmp_path)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / \"extracted_repo\"\n    extract_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the base name for the extracted repository\n    if filename is None:\n        filename = os.path.basename(archive_path)\n    \n    # Extract the archive using tar\n    try:\n        # Use tar to extract the archive\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Construct the repo URL based on the extracted directory structure\n    # Assuming the extracted content contains a directory with the package name\n    # which will be used as the origin URL\n    extracted_dirs = list(extract_dir.iterdir())\n    if not extracted_dirs:\n        raise RuntimeError(\"No directories found in extracted archive\")\n    \n    # Use the first directory as the repo name (or use filename if provided)\n    repo_name = filename or extracted_dirs[0].name\n    \n    # Return the repo URL (e.g., file://path/to/repo)\n    return f\"file://{str(extract_dir / repo_name)}\"", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / archive_path.stem\n    extract_dir.mkdir(exist_ok=True)\n    \n    # Use tar to extract the archive\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz', '.tar.gz']:\n        cmd = ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.xz', '.txz']:\n        cmd = ['tar', '-xJf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.zip']:\n        cmd = ['unzip', '-q', str(archive_path), '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # Assuming the repo URL is derived from the filename or directory structure\n    if filename:\n        repo_url = f\"file://{extract_dir}/{filename}\"\n    else:\n        # Default to the directory name\n        repo_url = f\"file://{extract_dir}\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Ensure tmp_path is a Path object\n    tmp_path = Path(tmp_path)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / \"extracted_repo\"\n    extract_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Use tar to extract the archive\n    if archive_path.endswith(\".tar.gz\") or archive_path.endswith(\".tgz\"):\n        cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)]\n    elif archive_path.endswith(\".tar\"):\n        cmd = [\"tar\", \"-xvf\", archive_path, \"-C\", str(extract_dir)]\n    elif archive_path.endswith(\".zip\"):\n        cmd = [\"unzip\", \"-q\", archive_path, \"-d\", str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted directory structure\n    # If filename is provided, use it as the base name for the repo URL\n    if filename:\n        repo_url = f\"file://{str(extract_dir)}/{filename}\"\n    else:\n        # Use the first directory in the extracted content as the repo URL\n        # This is a fallback; in practice, the structure should be more predictable\n        repo_url = f\"file://{str(extract_dir)}/\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Ensure tmp_path is a Path object\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True, parents=True)\n    \n    # Determine the output directory name\n    if filename:\n        output_dir = tmp_dir / filename\n    else:\n        output_dir = tmp_dir / os.path.splitext(os.path.basename(archive_path))[0]\n    \n    # Extract the archive using tar or unzip based on extension\n    archive_ext = os.path.splitext(archive_path)[1].lower()\n    \n    if archive_ext in ['.tar.gz', '.tgz', '.tar.gz']:\n        cmd = ['tar', '-xzf', archive_path, '-C', str(output_dir)]\n    elif archive_ext in ['.tar.xz', '.tar.xz']:\n        cmd = ['tar', '-xJf', archive_path, '-C', str(output_dir)]\n    elif archive_ext in ['.zip']:\n        cmd = ['unzip', '-q', archive_path, '-d', str(output_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_ext}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the file repo url (path to the extracted directory)\n    return str(output_dir)"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files that are in the ignored list\n        filtered_files = []\n        for file in ignored_files:\n            if file in ignored:\n                filtered_files.append(file)\n        \n        # Sort the filtered files\n        filtered_files.sort()\n        \n        # Return as a comma-separated string\n        return \",\".join(filtered_files)\n    \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files based on the ignored pattern (if ignored is provided)\n        if ignored:\n            # If ignored is a list or string, filter files that match\n            ignored_files = [f for f in ignored_files if any(pattern in f for pattern in ignored)]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Return as a comma-separated string\n        return \",\".join(ignored_files)\n    \n    except subprocess.CalledProcessError as e:\n        # If git command fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files that match the ignored pattern (if ignored is provided)\n        if ignored:\n            ignored_files = [file for file in ignored_files if any(pattern in file for pattern in ignored)]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Return as a comma-separated string\n        return \",\".join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git command fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files based on the ignored parameter (assuming ignored is a list of patterns or file names)\n        # If ignored is a list of file names to check, filter accordingly\n        if ignored:\n            ignored_files = [f for f in ignored_files if any(pattern in f for pattern in ignored)]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Return as a comma-separated string\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git command fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files based on the ignored list (if ignored is provided)\n        if ignored:\n            ignored_files = [f for f in ignored_files if f in ignored]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Return as a comma-separated string\n        return \",\".join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git command fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 950, "num_retrieved": 3, "inference_time": 0.7312778830528259, "generate_results": ["import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # TODO: Implement this function\n    # Since no specific file types are defined in the context,\n    # we'll implement a basic check for common supported file types.\n    # This is a placeholder implementation - actual logic would depend on\n    # the specific requirements of the module.\n    \n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini'\n    }\n    \n    filename_lower = filename.lower()\n    for ext in supported_extensions:\n        if filename_lower.endswith(ext):\n            return True\n    \n    return False", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # TODO: Implement this function\n    # Since no specific file types are defined in the context,\n    # we'll implement a basic check for common supported file types.\n    # This is a placeholder implementation - actual logic would depend on\n    # the module's specific requirements.\n    \n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini'\n    }\n    \n    filename_lower = filename.lower()\n    for ext in supported_extensions:\n        if filename_lower.endswith(ext):\n            return True\n    \n    return False", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions (example list)\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini'\n    }\n    \n    # Convert filename to lowercase and extract extension\n    filename_lower = filename.lower()\n    file_extension = os.path.splitext(filename_lower)[1]\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # TODO: Implement this function\n    # Since no specific file types are defined in the context,\n    # we'll implement a basic check for common supported file types.\n    # This is a placeholder implementation - actual logic would depend on\n    # the module's specific requirements.\n    \n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini'\n    }\n    \n    filename_lower = filename.lower()\n    \n    for ext in supported_extensions:\n        if filename_lower.endswith(ext):\n            return True\n    \n    return False", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # TODO: Implement this function\n    # Since no specific file types are defined in the context,\n    # we return False by default. A real implementation would\n    # define supported file extensions or patterns.\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 914, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Define valid time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate: 30 days\n        'months': 2592000,\n        'year': 31536000,  # Approximate: 365 days\n        'years': 31536000\n    }\n\n    # Regular expression to match number and unit\n    match = re.match(r'^(\\d+)\\s*([a-z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unsupported time unit: {unit}\")\n\n    # Create timedelta with the appropriate multiplier\n    multiplier = unit_map[unit]\n    return datetime.timedelta(seconds=number * multiplier)", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Match number and unit (supporting common time units)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == \"weeks\":\n        return datetime.timedelta(weeks=number)\n    elif unit == \"days\":\n        return datetime.timedelta(days=number)\n    elif unit == \"hours\":\n        return datetime.timedelta(hours=number)\n    elif unit == \"minutes\":\n        return datetime.timedelta(minutes=number)\n    elif unit == \"seconds\":\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unsupported time unit: {unit}\")", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Define valid time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate: 30 days\n        'months': 2592000,\n        'year': 31536000,  # Approximate: 365 days\n        'years': 31536000\n    }\n\n    # Regular expression to match number and unit\n    match = re.match(r'^(\\d+)(\\s+)([a-z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(3).lower()\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unknown time unit: {unit}\")\n\n    # Create timedelta with the appropriate multiplier\n    multiplier = unit_map[unit]\n    return datetime.timedelta(seconds=number * multiplier)", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Define valid time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate: 30 days\n        'months': 2592000,\n        'year': 31536000,  # Approximate: 365 days\n        'years': 31536000\n    }\n\n    # Regular expression to match number and unit\n    match = re.match(r'^(\\d+)\\s*([a-z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unsupported time unit: {unit}\")\n\n    # Create timedelta based on the unit\n    total_seconds = number * unit_map[unit]\n    return datetime.timedelta(seconds=total_seconds)", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Define valid time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate: 30 days\n        'months': 2592000,\n        'year': 31536000,  # Approximate: 365 days\n        'years': 31536000\n    }\n\n    # Regular expression to match number and unit\n    match = re.match(r'^(\\d+)\\s*([a-z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unknown time unit: {unit}\")\n\n    # Create timedelta with the appropriate multiplier\n    multiplier = unit_map[unit]\n    return datetime.timedelta(seconds=number * multiplier)"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 922, "num_retrieved": 2, "inference_time": 0.7312778830528259, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 964, "num_retrieved": 4, "inference_time": 0.6683628708124161, "generate_results": ["import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    def transform_path(path):\n        # Check if the path already starts with a prefix like 'pp:' or 'sh:'\n        if path.startswith('sh:') or path.startswith('pp:'):\n            return path\n        \n        # Transform a simple path fragment into a glob pattern\n        # Using ** to match any directory depth, and * to match any filename\n        # Replace any '/' with '/*' to match subdirectories\n        # Add **/* at the beginning and */** at the end to match any directory depth\n        pattern = f\"sh:**/*{path.replace('/', '/*')}*/**\"\n        return pattern\n    \n    return [transform_path(path) for path in find_paths]", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('pp:') or path.startswith('sh:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a simple rule: replace dots with literal dots, and use **/*path*/** pattern\n            # This matches any directory containing the path fragment\n            # Replace any dot with literal dot to avoid glob expansion\n            pattern = path.replace('.', r'\\.')\n            # Create a glob pattern that matches any directory containing the path fragment\n            glob_pattern = f\"sh:**/*{pattern}*/**\"\n            result.append(glob_pattern)\n    return result", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    def transform_path(path):\n        # Check if the path already starts with a prefix like 'pp:' or 'sh:'\n        if path.startswith('sh:') or path.startswith('pp:'):\n            return path\n        \n        # Transform a simple path fragment into a glob pattern\n        # Using **/*path*/** to match anywhere in the directory tree\n        # Replace any '/' with '/*' to create a glob pattern\n        # Use a regex to ensure we don't break on existing glob patterns\n        pattern = re.sub(r'/', r'/*', path)\n        # Wrap with sh:**/* and **/ to match anywhere\n        return f'sh:**/*{pattern}*/**'\n    \n    return [transform_path(path) for path in find_paths]", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('pp:') or path.startswith('sh:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a simple rule: wrap with sh:**/*path*/** to match any directory containing the path\n            # Replace any '/' with '/*' to handle subpaths\n            # Use a regex to avoid issues with special characters\n            pattern = re.sub(r'/', r'/*', path)\n            result.append(f'sh:**/*{pattern}*/**')\n    return result", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('pp:') or path.startswith('sh:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a regex to match any path component and wrap with **/*...*/**\n            # Replace any path separator with / and add wildcards around\n            # For example, 'foo.txt' becomes 'sh:**/*foo.txt*/**'\n            # We assume the path is a simple fragment without wildcards\n            pattern = re.sub(r'(\\w+)', r'**/*\\1*/**', path)\n            # Ensure the pattern starts with sh: and ends with ** (to match any directory)\n            result.append(f'sh:**/*{path}*/**')\n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 981, "num_retrieved": 4, "inference_time": 0.6683628708124161, "generate_results": ["def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.environ.get('TERM', '') == 'xterm'", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.environ.get('TERM', '') == 'xterm'", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.environ.get('TERM', '') == 'xterm'", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.path.basename(os.environ.get('COMSPEC', '')).lower().startswith('cmd') and 'GIT_BASH' in os.environ.get('TERM', '')", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.path.basename(os.environ.get('COMSPEC', '')).lower().startswith('cmd') and 'GIT_BASH' in os.environ.get('TERM', '')"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 2, "inference_time": 0.6683628708124161, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file already exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 894, "num_retrieved": 1, "inference_time": 0.6683628708124161, "generate_results": ["def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.6683628708124161, "generate_results": ["import subprocess\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_spec = importlib.util.spec_from_file_location(module_name, None)\n    module = importlib.util.module_from_spec(module_spec)\n    \n    # Since we don't have a file path, we can't directly import the function\n    # Instead, we'll use a different approach: execute the function directly\n    # in a subprocess using the function's module and name\n    \n    # We need to reconstruct the module path or use a different method\n    # For now, we assume the function is available in the current module\n    # and we can call it directly via subprocess\n    \n    # Instead, we'll use a workaround: create a temporary script that imports\n    # and runs the function\n    \n    # Generate a temporary script\n    script_content = f\"\"\"\nimport sys\nimport {func.__module__}\nsys.path.insert(0, '{sys.path[0]}')\n{func.__module__}.{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    temp_script = Path(f\"/tmp/{module_name}.py\")\n    temp_script.write_text(script_content)\n    \n    # Set up environment\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the script in subprocess\n    try:\n        result = subprocess.run(\n            [sys.executable, str(temp_script)],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    finally:\n        # Clean up the temporary file\n        if temp_script.exists():\n            temp_script.unlink()", "import subprocess\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_spec = importlib.util.spec_from_file_location(module_name, None)\n    module = importlib.util.module_from_spec(module_spec)\n    \n    # Since we don't have a file path, we can't directly import the function\n    # Instead, we'll use a different approach: execute the function directly\n    # via subprocess using the function's module and name\n    \n    # We need to reconstruct the module path or use a different method\n    # Given the constraints, we assume the function is accessible in a known module\n    \n    # Instead, we'll use a more direct approach: run the function as a script\n    # by creating a temporary script that imports and calls the function\n    \n    # Create a temporary script that imports and runs the function\n    script_content = f\"\"\"\nimport sys\nimport {func.__module__}\nsys.path.insert(0, '{sys.path[0]}')\n{func.__module__}.{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    temp_script = Path(f\"/tmp/{module_name}.py\")\n    temp_script.write_text(script_content)\n    \n    # Set environment variables\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the script using subprocess\n    result = subprocess.run(\n        [sys.executable, str(temp_script)],\n        timeout=timeout,\n        env=env,\n        capture_output=True\n    )\n    \n    # Clean up the temporary file\n    temp_script.unlink(missing_ok=True)\n    \n    return result", "import subprocess\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_spec = importlib.util.spec_from_file_location(module_name, None)\n    module = importlib.util.module_from_spec(module_spec)\n    \n    # Since we don't have a file path, we can't directly import the function\n    # Instead, we'll use a different approach: execute the function directly\n    # via subprocess using the function's module and name\n    \n    # We need to reconstruct the module path or use a different method\n    # For now, we assume the function is available in a known module\n    # and we can call it directly via subprocess\n    \n    # This implementation assumes that the function can be called via a script\n    # that imports the module and calls the function\n    \n    # Create a temporary script that imports the function and calls it\n    script_content = f\"\"\"\nimport sys\nimport {func.__module__}\nsys.path.insert(0, '{sys.path[0]}')\n{func.__module__}.{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    temp_script = Path(f\"/tmp/{module_name}.py\")\n    temp_script.write_text(script_content)\n    \n    # Set up environment variables\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the script with subprocess\n    try:\n        result = subprocess.run(\n            [sys.executable, str(temp_script)],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    finally:\n        # Clean up the temporary file\n        if temp_script.exists():\n            temp_script.unlink()", "import subprocess\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_spec = importlib.util.spec_from_file_location(module_name, None)\n    module = importlib.util.module_from_spec(module_spec)\n    \n    # Since we don't have a file path, we need to create a dummy module\n    # and dynamically assign the function to it\n    module.func = func\n    \n    # Execute the function in a subprocess\n    # We need to create a script that imports the module and calls the function\n    script = f\"\"\"\nimport sys\nimport importlib\nimport subprocess\n\n# Import the module dynamically\nmodule_name = '{module_name}'\nmodule = importlib.import_module(module_name)\n\n# Call the function with provided arguments\nresult = module.func({', '.join([repr(arg) for arg in args])})\nprint(result)\n\"\"\"\n    \n    # Write the script to a temporary file\n    temp_script = Path(\"/tmp/subprocess_helper_script_{}.py\".format(hash(script)))\n    temp_script.write_text(script)\n    \n    # Set up environment\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the script in subprocess\n    try:\n        result = subprocess.run(\n            [sys.executable, str(temp_script)],\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            env=env\n        )\n        return result.stdout.strip() if result.stdout else None\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function execution timed out after {timeout} seconds\")\n    finally:\n        # Clean up the temporary script\n        if temp_script.exists():\n            temp_script.unlink()", "import subprocess\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the environment dictionary\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_path = Path(f\"/tmp/{module_name}.py\")\n    \n    # Write the function to a temporary file\n    module_code = f\"def {func.__name__}():\\n    {func.__code__}\\n\"\n    module_path.write_text(module_code)\n    \n    # Import the module\n    spec = importlib.util.spec_from_file_location(module_name, module_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    \n    # Run the function in a subprocess\n    command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n    \n    result = subprocess.run(\n        command,\n        env=env,\n        timeout=timeout,\n        capture_output=True,\n        text=True\n    )\n    \n    # Clean up the temporary file\n    module_path.unlink()\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 942, "num_retrieved": 3, "inference_time": 0.6683628708124161, "generate_results": ["import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(-([a-zA-Z0-9_]+))?(-:(.+))?$', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid variable format: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = None\n    if match.group(3) and match.group(5):\n        default_value = match.group(5)\n    \n    env_value = os.environ.get(var_name)\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value if env_value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name)\n        return env_value if env_value is not None else default_value\n    \n    env_value = os.environ.get(var_name)\n    if env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(-([a-zA-Z0-9_]+))?(-:(.+))?$', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = None\n    if match.group(3) and match.group(5):\n        default_value = match.group(6)\n    \n    env_value = os.environ.get(var_name)\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value if env_value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(-([a-zA-Z0-9_]+))?(-:(.+))?$', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = None\n    if match.group(3) and match.group(5):\n        default_value = match.group(6)\n    \n    env_value = os.environ.get(var_name)\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value if env_value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name)\n        return env_value if env_value is not None else default_value\n    \n    env_value = os.environ.get(var_name)\n    if env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 944, "num_retrieved": 3, "inference_time": 0.6683628708124161, "generate_results": ["import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    # Remove any leading or trailing whitespace\n    image_href = image_href.strip()\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http://'):\n        use_ssl = False\n        netloc = image_href[7:]\n    elif image_href.startswith('https://'):\n        use_ssl = True\n        netloc = image_href[8:]\n    else:\n        # If no protocol is specified, assume it's a local reference\n        # Try to extract the netloc from the path\n        if '/' in image_href:\n            # Extract the path part\n            path_parts = image_href.split('/', 1)\n            netloc = path_parts[0]\n            # If netloc is empty or just a dot, it's likely a local reference\n            if not netloc or netloc == '.':\n                raise ValueError(\"Invalid image href: missing protocol or netloc\")\n            use_ssl = False\n        else:\n            raise ValueError(\"Invalid image href: missing protocol or netloc\")\n\n    # Parse the netloc to extract the image_id\n    try:\n        # Split the netloc by '/' to get the domain and path\n        netloc_parts = netloc.split('/', 1)\n        if len(netloc_parts) == 1:\n            # If no path, assume the entire netloc is the domain\n            domain = netloc_parts[0]\n            path = \"\"\n        else:\n            domain = netloc_parts[0]\n            path = netloc_parts[1]\n\n        # Extract the image_id from the path\n        if path:\n            # Split the path by '/' and take the last part\n            path_parts = path.split('/', 1)\n            if len(path_parts) == 1:\n                image_id = path_parts[0]\n            else:\n                image_id = path_parts[1]\n        else:\n            # If no path, the image_id is not present\n            raise ValueError(\"Invalid image href: missing image ID\")\n\n        # Validate that image_id is not empty\n        if not", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    # Handle absolute URLs\n    if image_href.startswith('http'):\n        url = urllib.parse.urlparse(image_href)\n        netloc = url.netloc\n        use_ssl = url.scheme == 'https'\n        \n        # Extract image_id from path\n        path = url.path.lstrip('/')\n        if not path:\n            raise ValueError(\"Invalid image href: missing image ID in path\")\n            \n        image_id = path\n    else:\n        # Assume it's just an image ID\n        image_id = image_href\n        netloc = ''\n        use_ssl = False\n    \n    return (image_id, netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    # Remove any leading or trailing whitespace\n    image_href = image_href.strip()\n\n    # Check if the href starts with a scheme (http or https)\n    if image_href.startswith('http'):\n        scheme = image_href.split('://')[0]\n        rest = image_href.split('://')[1]\n    else:\n        # If no scheme, assume it's a relative path or just an ID\n        scheme = 'http'\n        rest = image_href\n\n    # Determine if SSL is used (https vs http)\n    use_ssl = scheme == 'https'\n\n    # Split the rest into netloc and path\n    try:\n        parsed = urllib.parse.urlparse(image_href)\n        netloc = parsed.netloc\n        path = parsed.path\n    except Exception:\n        raise ValueError(\"Invalid image href format\")\n\n    # Extract image_id from the path\n    if not path:\n        raise ValueError(\"Image href must have a path component\")\n\n    # Remove leading slash if present\n    path = path.lstrip('/')\n\n    # If path is empty, raise error\n    if not path:\n        raise ValueError(\"Image href path cannot be empty\")\n\n    # Extract image_id from path (assuming it's the last part)\n    parts = path.split('/')\n    if len(parts) == 0:\n        raise ValueError(\"Image href path cannot be empty\")\n    \n    image_id = parts[-1]\n\n    return (image_id, netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    # Remove any leading or trailing whitespace\n    image_href = image_href.strip()\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http://'):\n        use_ssl = False\n        netloc = image_href[7:]\n    elif image_href.startswith('https://'):\n        use_ssl = True\n        netloc = image_href[8:]\n    else:\n        # If no protocol is specified, assume it's a local ID\n        # Try to extract netloc from the URL\n        try:\n            parsed = urllib.parse.urlparse(image_href)\n            netloc = parsed.netloc\n            use_ssl = parsed.scheme == 'https'\n        except Exception:\n            # If parsing fails, assume it's a local ID\n            netloc = ''\n            use_ssl = False\n\n    # Extract the image_id from the netloc or path\n    if netloc:\n        # If netloc is present, extract the image_id from the path\n        try:\n            parsed = urllib.parse.urlparse(image_href)\n            path = parsed.path\n            if path:\n                # Remove leading slash and split by '/' to get the last part\n                image_id = path.split('/')[-1]\n            else:\n                image_id = ''\n        except Exception:\n            image_id = ''\n    else:\n        # If no netloc, assume the entire string is the image_id\n        image_id = image_href\n\n    # Validate that we have a non-empty image_id\n    if not image_id:\n        raise ValueError(\"Invalid image href: no image ID found\")\n\n    return (image_id, netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    # Remove any leading or trailing whitespace\n    image_href = image_href.strip()\n\n    # Check if the href starts with a scheme (http or https)\n    if image_href.startswith('http'):\n        scheme = image_href.split('://')[0]\n        rest = image_href.split('://')[1]\n    else:\n        # If no scheme, assume it's a relative path or just an ID\n        scheme = 'http'\n        rest = image_href\n\n    # Determine if SSL is used (https vs http)\n    use_ssl = scheme == 'https'\n\n    # Split the rest into netloc and path\n    try:\n        netloc, path = urllib.parse.splithost(rest)\n        if not netloc:\n            raise ValueError(\"Invalid image href: missing netloc\")\n        \n        # Extract image_id from the path\n        if path:\n            # If path exists, extract the last part as image_id\n            path_parts = path.split('/')\n            image_id = path_parts[-1] if path_parts else ''\n        else:\n            # If no path, the image_id is the netloc itself (unlikely)\n            image_id = netloc\n    except Exception:\n        raise ValueError(\"Invalid image href: unable to parse netloc or path\")\n\n    # Validate that image_id is not empty\n    if not image_id:\n        raise ValueError(\"Invalid image href: missing image_id\")\n\n    return (image_id, netloc, use_ssl)"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 889, "num_retrieved": 1, "inference_time": 0.6683628708124161, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(f\"All elements in input_list must be strings, found {type(item)}\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(f\"All elements in input_list must be strings, found {type(item)}\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(f\"All elements in input_list must be strings, got {type(item)}\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(f\"All elements in input_list must be strings, found {type(item)}\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1089, "num_retrieved": 8, "inference_time": 0.6683628708124161, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern and add word boundaries\n    pattern = re.escape(pattern)\n    # If pattern contains spaces, join with '\\s*' to match whitespace\n    if ' ' in pattern:\n        pattern = r'\\s*'.join(pattern.split())\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern and add word boundaries\n    pattern = re.escape(pattern)\n    # Ensure we have word boundaries around the pattern\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern and add word boundaries\n    pattern = re.escape(pattern)\n    # If pattern contains spaces, join with escaped space\n    if ' ' in pattern:\n        pattern = r'\\s*'.join(re.split(r'\\s+', pattern))\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern and add word boundaries\n    pattern = re.escape(pattern)\n    # Add word boundaries around the pattern\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern and add word boundaries\n    pattern = re.escape(pattern)\n    # Replace any whitespace with '\\s*' to match any number of whitespace characters\n    pattern = pattern.replace(' ', r'\\s*')\n    return pattern"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1002, "num_retrieved": 5, "inference_time": 0.6683628708124161, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            capture_output=True if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        if result.returncode != 0:\n            if hide_stderr:\n                print(f\"Command failed with return code {result.returncode}\")\n            else:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error: {result.stderr}\")\n                if result.stdout:\n                    print(f\"Output: {result.stdout}\")\n        return result.returncode\n    except subprocess.SubprocessError as e:\n        print(f\"Subprocess error: {e}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            capture_output=True if hide_stderr else False\n        )\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error: {result.stderr}\")\n            return result.returncode\n        return 0\n    except Exception as e:\n        if verbose:\n            print(f\"Exception occurred: {e}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            capture_output=True if not hide_stderr else None,\n            text=True,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None\n        )\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                print(f\"Stdout: {result.stdout}\")\n                print(f\"Stderr: {result.stderr}\")\n            return result.returncode\n        return 0\n    except subprocess.SubprocessError as e:\n        if verbose:\n            print(f\"Subprocess error: {e}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            capture_output=True if not hide_stderr else None,\n            text=True,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None\n        )\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                print(f\"Stdout: {result.stdout}\")\n                print(f\"Stderr: {result.stderr}\")\n            return result.returncode\n        return 0\n    except Exception as e:\n        if verbose:\n            print(f\"Error executing command: {e}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            capture_output=True if not hide_stderr else None,\n            text=True,\n            env=env,\n            timeout=None\n        )\n        if result.returncode != 0:\n            if hide_stderr:\n                print(f\"Command failed with return code {result.returncode}\")\n            else:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error: {result.stderr}\")\n            return result.returncode\n        return 0\n    except subprocess.TimeoutExpired:\n        print(\"Command timed out\")\n        return 1\n    except Exception as e:\n        print(f\"Command execution failed: {str(e)}\")\n        return 1"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 0.6683628708124161, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 885, "num_retrieved": 1, "inference_time": 0.6683628708124161, "generate_results": ["import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple (subject, prop, ?))\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., exists a triple (subject, prop, parent))\n        parent = None\n        for obj in graph.objects(subject, prop):\n            parent = obj\n            break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in a triple (subject, prop, ?))\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has no parent (i.e., no object in a triple (subject, prop, ?))\n        # We need to find if there's any triple (x, prop, subject) - if not, then subject is a root\n        has_parent = False\n        for obj in graph.objects(subject, prop):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in a (subject, prop, object) triple)\n    for subject in graph.subjects(prop, None):\n        parent = None\n        for obj in graph.objects(subject, prop):\n            parent = obj\n            break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in a triple (subject, prop, ?))\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., if there's a triple (subject, prop, parent))\n        parent = None\n        for obj in graph.objects(subject, prop):\n            parent = obj\n            break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in a triple (subject, prop, ?))\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has no parent (i.e., no object in a triple (subject, prop, ?))\n        # We need to find if there's any triple (x, prop, subject)\n        has_parent = False\n        for obj in graph.objects(subject, prop):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1098, "num_retrieved": 6, "inference_time": 0.6683628708124161, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1022, "num_retrieved": 6, "inference_time": 0.6683628708124161, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.6683628708124161, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 969, "num_retrieved": 4, "inference_time": 0.6683628708124161, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 2, "inference_time": 0.9668855518102646, "generate_results": ["import logging\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in the context\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file {config_filename} not found\",\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Permission denied when reading configuration file {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file {config_filename} validation error: {error}\",\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in the context\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} validation failed',\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in the context\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file {config_filename} not found\",\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Permission denied when reading configuration file {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file {config_filename} validation failed: {error}\",\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in the context\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file {config_filename} not found\",\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Permission denied when reading configuration file {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file {config_filename} failed validation: {error}\",\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in the function signature\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} failed validation',\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 930, "num_retrieved": 3, "inference_time": 0.9668855518102646, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 929, "num_retrieved": 2, "inference_time": 0.9668855518102646, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds = ticks // 1000000000\n    microseconds = (ticks % 1000000000) // 1000\n    \n    # Convert to time object\n    hour = seconds // 3600\n    minute = (seconds % 3600) // 60\n    second = seconds % 60\n    \n    # Create time with optional timezone\n    if tz is not None:\n        # For timezone, we need to create a time with timezone info\n        # Since Time in Neo4j doesn't store timezone in the same way as datetime,\n        # we just return a Time with the ticks and optional timezone offset\n        # The actual timezone handling is done at the hydration level\n        return cls(ticks, tz)\n    else:\n        return cls(ticks)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    seconds = ticks // 1000000000\n    remaining_ns = ticks % 1000000000\n    microseconds = remaining_ns // 1000\n    \n    # Convert seconds to hours, minutes, seconds\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create a time object with the calculated components\n    time_obj = time(hours, minutes, seconds, microseconds)\n    \n    # If timezone is provided, create a timezone-aware time\n    if tz is not None:\n        time_obj = timezone(tz)(time_obj)\n    \n    return cls(time_obj)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    seconds = ticks // 1000000000\n    remaining_ns = ticks % 1000000000\n    microseconds = remaining_ns // 1000\n    \n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create a time object with the calculated components\n    time_obj = time(hours, minutes, seconds, microseconds)\n    \n    # If timezone is provided, create a timezone-aware time\n    if tz is not None:\n        # Note: The Time class in Neo4j does not support timezone-aware times\n        # This is a simplified implementation assuming the time is timezone-neutral\n        # In practice, the timezone information would be stored separately\n        pass\n    \n    return cls(hours, minutes, seconds, microseconds)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    seconds = ticks // 1000000000\n    remaining_ns = ticks % 1000000000\n    microseconds = remaining_ns // 1000\n    \n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create a time object with the calculated components\n    time_obj = time(hours, minutes, seconds, microseconds)\n    \n    # If timezone is provided, create a timezone-aware time\n    if tz is not None:\n        # Note: The Time class in Neo4j does not support timezone-aware times\n        # This is a simplified implementation assuming the timezone is only used\n        # for the underlying representation\n        # In practice, the Time class in Neo4j is timezone-naive\n        pass\n    \n    return cls(time_obj)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    seconds = ticks // 1000000000\n    remaining_ns = ticks % 1000000000\n    microseconds = remaining_ns // 1000\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create a time object with the calculated components\n    time_obj = time(hours, minutes, seconds, microseconds)\n    \n    # If timezone is provided, create a timezone-aware time\n    if tz is not None:\n        time_obj = timezone(tz).localize(time_obj)\n    \n    return cls(time_obj)"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 874, "num_retrieved": 2, "inference_time": 0.9668855518102646, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol version must be a tuple of two integers\")\n        \n        # Check if the protocol version is supported\n        # Based on the context, we have AsyncBolt4x2 for version (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol_version must be a tuple of two integers\")\n        \n        # Check if the version is supported\n        # Assuming only AsyncBolt4x2 is defined in context and supports (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol_version must be a tuple of two integers\")\n        \n        # Check if the version is supported\n        # Based on context, we have AsyncBolt4x2 for (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2 or not all(isinstance(x, int) for x in protocol_version):\n            raise TypeError(\"protocol_version must be a tuple of two integers\")\n        \n        # Check if the protocol version is supported\n        # Assuming we have a known set of supported versions\n        # For example, in the context, we have AsyncBolt4x2 for (4, 2)\n        supported_versions = {\n            (4, 2): AsyncBolt4x2\n        }\n        \n        return {version: handler for version, handler in supported_versions.items() if version == protocol_version}\n    \n    # If no specific version is provided, return all supported versions\n    supported_versions = {\n        (4, 2): AsyncBolt4x2\n    }\n    \n    return supported_versions", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\" Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol version must be a tuple of two integers\")\n        \n        # Check if the version is supported by any known handler\n        # For this example, we assume only AsyncBolt4x2 supports (4, 2)\n        if protocol_version == (4, 2):\n            return {protocol_version: cls}\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {(4, 2): cls}"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1025, "num_retrieved": 5, "inference_time": 0.9668855518102646, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(values)", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(values)", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(values)", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(values)"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 882, "num_retrieved": 1, "inference_time": 0.9668855518102646, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    # Check if seq is already a sequence type (FillComputeSeq, FillRequestSeq, Source)\n    if isinstance(seq, (FillComputeSeq, FillRequestSeq, Source)):\n        return seq, type(seq).__name__\n    \n    # If seq is a generator or iterable, convert to a sequence\n    try:\n        # Try to create a sequence from the input\n        if hasattr(seq, '__iter__') and not isinstance(seq, (str, bytes)):\n            # For iterables, assume they are sequences\n            sequence = list(seq)\n            return sequence, \"iterable\"\n        else:\n            # For non-iterable, treat as a single value\n            return [seq], \"value\"\n    except Exception:\n        # If conversion fails, raise a type error\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    # Check if seq is already a sequence type (FillComputeSeq, FillRequestSeq, Source)\n    if isinstance(seq, (FillComputeSeq, FillRequestSeq, Source)):\n        return seq, type(seq).__name__\n    \n    # Otherwise, try to derive a sequence from seq\n    # This is a placeholder - actual implementation would depend on context\n    # For example, if seq is a generator or iterable, convert it to a sequence\n    try:\n        # Attempt to create a sequence from seq\n        if hasattr(seq, '__iter__') and not isinstance(seq, (str, bytes)):\n            # If it's an iterable, create a sequence from it\n            sequence = list(seq)\n            # Determine type based on context or default\n            # In absence of specific type info, default to \"fill_compute\"\n            return sequence, \"fill_compute\"\n        else:\n            # If not iterable, treat as a single value\n            # This is a fallback - actual behavior depends on context\n            return seq, \"fill_compute\"\n    except Exception:\n        # If conversion fails, raise a LenaTypeError\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    # Check if seq is already a sequence type (FillComputeSeq, FillRequestSeq, Source)\n    if isinstance(seq, (FillComputeSeq, FillRequestSeq, Source)):\n        return seq, type(seq).__name__\n    \n    # If not, try to derive a sequence from it\n    # This is a placeholder - actual implementation would depend on context\n    # For now, assume it's a valid sequence type\n    try:\n        # Attempt to get sequence and type from split._get_seq_with_type\n        # This is a fallback - actual logic would depend on the full context\n        return seq, \"unknown\"\n    except:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    # Check if seq is already a sequence type (FillComputeSeq, FillRequestSeq, Source)\n    if isinstance(seq, (FillComputeSeq, FillRequestSeq, Source)):\n        return seq, type(seq).__name__\n    \n    # If seq is a generator or iterable, wrap it appropriately\n    try:\n        iter(seq)\n        # If it's an iterable but not a sequence type, assume it's a data source\n        return seq, \"source\"\n    except TypeError:\n        # If it's not iterable, it's likely a single value or primitive\n        return seq, \"fill_compute\"", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    # Check if seq is already a sequence type (FillComputeSeq, FillRequestSeq, Source)\n    if isinstance(seq, (FillComputeSeq, FillRequestSeq, Source)):\n        return seq, type(seq).__name__\n    \n    # If seq is a generator or iterable, wrap it appropriately\n    try:\n        iter(seq)\n        # If it's an iterable but not a sequence type, assume it's a data source\n        return seq, \"source\"\n    except TypeError:\n        # If it's not iterable, it's likely a single value or primitive\n        return seq, \"fill_compute\""]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 865, "num_retrieved": 1, "inference_time": 0.9668855518102646, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            if not recompute:\n                raise ValueError(\"Scale not computed before, must recompute\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram scale is zero, cannot be rescaled\")\n        scale_factor = other / self._scale\n        self._scale = other\n        # Apply scale factor to histogram data\n        # Assuming histogram data is stored in self.data\n        # This is a placeholder - actual implementation depends on histogram internals\n        self._apply_scale_factor(scale_factor)\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Scale not computed before, compute it\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        # Update the histogram data if needed\n        self._update_histogram_data()\n    return None", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # If not recompute, don't compute if already computed\n                raise AttributeError(\"Scale not computed before\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        # Update histogram data if needed\n        self._update_histogram_data()\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # If not recompute, scale is not computed yet\n                return None\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Apply the scaling\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Scale not computed before, compute it\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        # Update histogram data if needed\n        self._update_histogram_data()\n    return None"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.9668855518102646, "generate_results": ["def scale(self, other=None):\n    if other is None:\n        return self._scale\n    elif other == 0:\n        raise ValueError(\"Cannot rescale to zero scale\")\n    elif other == float('inf'):\n        raise ValueError(\"Cannot rescale to infinite scale\")\n    \n    if self._scale is None or self._scale == 0:\n        raise ValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    # Only rescale the last coordinate\n    last_coord_idx = len(self.field_names) - 1\n    new_scale = other\n    \n    # Rescale the last coordinate and all associated errors\n    for i, value in enumerate(self.values):\n        if i == last_coord_idx:\n            self.values[i] *= new_scale\n        # Rescale errors if they exist\n        if hasattr(self, 'errors') and self.errors is not None:\n            if i == last_coord_idx:\n                self.errors[i] *= new_scale\n    \n    self._scale = new_scale\n    return self._scale", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    elif other == 0:\n        raise ValueError(\"Cannot rescale to zero scale\")\n    elif self._scale is None:\n        raise ValueError(\"Cannot rescale with unknown scale\")\n    \n    # Only rescale the last coordinate (field)\n    field_count = len(self.field_names)\n    if field_count == 0:\n        return self._scale\n    \n    last_field_index = field_count - 1\n    last_field_name = self.field_names[last_field_index]\n    \n    # Rescale the last coordinate and all associated errors\n    scale_factor = other / self._scale\n    \n    # Update the scale\n    self._scale = other\n    \n    # Rescale the last coordinate and all errors\n    for i, value in enumerate(self.values):\n        if i == last_field_index:\n            self.values[i] *= scale_factor\n        # Rescale errors if they exist\n        if hasattr(self, 'errors') and i < len(self.errors):\n            self.errors[i] *= scale_factor\n    \n    return self._scale", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    elif other == 0:\n        raise ValueError(\"Cannot rescale to zero scale\")\n    elif other == float('inf'):\n        raise ValueError(\"Cannot rescale to infinite scale\")\n    \n    if self._scale is None or self._scale == 0:\n        raise ValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    # Only rescale the last coordinate (field)\n    # and all associated errors\n    scale_factor = other / self._scale\n    \n    # Rescale the last coordinate and all errors\n    # Assuming the graph has fields, we only modify the last one\n    # and its errors\n    if hasattr(self, 'values'):\n        # Rescale the values (last coordinate)\n        last_coord = len(self.field_names) - 1\n        self.values = [val * scale_factor for val in self.values]\n    \n    # Rescale errors if they exist\n    if hasattr(self, 'errors'):\n        for i, err in enumerate(self.errors):\n            if err is not None:\n                self.errors[i] = err * scale_factor\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    \n    if self._scale == 0 or self._scale is None:\n        raise LenaValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    scale_factor = other / self._scale\n    \n    # Only rescale the last coordinate and its associated errors\n    # Assuming the graph has fields, and we want to rescale the last one\n    for i in range(len(self._fields) - 1, -1, -1):\n        field_name = self._fields[i]\n        if field_name.endswith('_error'):\n            # Rescale error field\n            error_value = getattr(self, field_name)\n            setattr(self, field_name, error_value * scale_factor)\n        else:\n            # Rescale the coordinate field\n            value = getattr(self, field_name)\n            setattr(self, field_name, value * scale_factor)\n    \n    self._scale = other", "def scale(self, other=None):\n    if other is None:\n        return self._scale\n    elif other == 0:\n        raise ValueError(\"Cannot rescale to zero scale\")\n    elif self._scale is None:\n        raise ValueError(\"Cannot rescale with unknown scale\")\n    \n    # Only rescale the last coordinate (field)\n    # and rescale all errors together with their coordinate\n    field_count = len(self.field_names)\n    if field_count == 0:\n        return self._scale\n    \n    last_field_index = field_count - 1\n    last_field_name = self.field_names[last_field_index]\n    \n    # Rescale the value and errors for the last coordinate\n    if hasattr(self, 'values'):\n        # Rescale values\n        self.values = [val * other for val in self.values]\n    \n    if hasattr(self, 'errors'):\n        # Rescale errors\n        self.errors = [err * other for err in self.errors]\n    \n    # Update the scale\n    self._scale = other\n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.9668855518102646, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with field names and scale\n    graph = Graph(\n        x_coords=x_coords,\n        y_values=y_values,\n        field_names=field_names,", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with appropriate field names\n    graph = Graph(\n        x=x_coords,\n        y=y_values,\n        field_names=field_names,\n        scale=scale", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    elif get_coordinate == \"middle\":\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    else:\n        raise ValueError(f\"Unknown get_coordinate value: {get_coordinate}\")\n    \n    # Compute y values using make_value\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with specified field names\n    graph = Graph(\n        x_coords=x_coords,\n        y_values=y_values,\n        field_names=field_names,\n        scale=scale if scale is not None else hist.scale\n    )\n    \n    return graph", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with specified field names\n    graph = Graph(\n        x=x_coords,\n        y=y_values,\n        field_names=field_names,\n        scale=scale", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create the graph points\n    points = []\n    for bin_idx, bin_data in enumerate(hist.bins):\n        # Get coordinate based on get_coordinate\n        if get_coordinate == \"left\":\n            x = bin_data[0]\n        elif get_coordinate == \"right\":\n            x = bin_data[-1]\n        elif get_coordinate == \"middle\":\n            x = (bin_data[0] + bin_data[-1]) / 2\n        \n        # Apply make_value to get y value(s)\n        y_value = make_value(bin_data)\n        \n        # Create point with field names\n        point = {\n            field_names[0]: x\n        }\n        if len(field_names) > 1:"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.9668855518102646, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                # Get the class of the candidate\n                cls = candidate.__class__\n                # Check if the class implements the interface\n                if not hasattr(cls, '__implements__'):\n                    errors.append(f\"candidate's class does not declare implementation of {iface!r}\")\n                else:\n                    # Check if the interface is in the implements list\n                    if iface not in cls.__implements__:\n                        errors.append(f\"candidate's class does not implement {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate's class does not support interface implementation checks\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else {}\n    except AttributeError:\n        # If interface has no methods, skip this check\n        pass\n    else", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                # Get the class of the candidate\n                cls = candidate.__class__\n                # Check if the class implements the interface\n                if not hasattr(cls, '__implements__'):\n                    errors.append(f\"candidate class does not declare implementation of {iface!r}\")\n                else:\n                    # Check if the interface is in the implements list\n                    if iface not in cls.__implements__:\n                        errors.append(f\"candidate class does not implement interface {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate class has no __implements__ attribute\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        methods = iface._methods\n    except AttributeError:\n        methods = []\n\n    for method_name in methods:\n        try:\n            method = getattr(candidate, method_name)\n            if not callable(method):\n                errors.append(f\"candidate missing method", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = getattr(candidate, '__providedBy__', None)\n            if provided_interfaces is None:\n                # Fall back to getObjectSpecification if needed\n                try:\n                    provided_interfaces = getObjectSpecification(candidate)\n                except:\n                    pass\n            if provided_interfaces is None:\n                errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n            else:\n                if iface not in provided_interfaces:\n                    errors.append(\"Candidate does not provide interface %s\" % iface)\n        except Exception as e:\n            errors.append(\"Error checking interface provision: %s\" % e)\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        iface_methods = getattr(iface, '__methods__', [])\n        if not iface_methods:\n            # No methods defined, so we're good\n            pass\n        else:\n            for method_name in iface_methods:\n                try:\n                    method = getattr(candidate, method_name)\n                    if method is None:\n                        errors.append(\"Missing required method %s\" % method_name)\n                except AttributeError:\n                    errors.append(\"", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                # Get the class of the candidate\n                cls = candidate.__class__\n                # Check if the class implements the interface\n                if not hasattr(cls, '__implements__'):\n                    errors.append(f\"candidate's class does not declare implementation of {iface!r}\")\n                else:\n                    # Check if the interface is in the implements set\n                    if iface not in cls.__implements__:\n                        errors.append(f\"candidate's class does not implement {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate's class does not support interface implementation checks\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        methods = iface._methods\n    except AttributeError:\n        methods = []\n\n    for method_name in methods:\n        try:\n            method = getattr(candidate, method_name)\n            if not callable(method):\n                errors.append(f\"candidate", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                # Get the class of the candidate\n                cls = candidate.__class__\n                # Check if the class implements the interface\n                if not hasattr(cls, '__implements__'):\n                    errors.append(f\"candidate class does not declare implementation of {iface!r}\")\n                else:\n                    # Check if the interface is in the implements list\n                    if iface not in cls.__implements__:\n                        errors.append(f\"candidate class does not implement interface {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate class has no __implements__ attribute\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        # Get the interface's methods\n        iface_methods = iface._methods\n        for method_name in iface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if not callable(method):\n                    errors.append(f\"candidate"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.9668855518102646, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # No __providedBy__ attribute, so we can't verify the interface claim\n            errors.append(f\"candidate has no __providedBy__ attribute\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        iface_methods = iface._methods if hasattr(iface, '_methods') else {}\n    except AttributeError:\n        # Interface doesn't have methods defined\n        pass\n\n    # Step 3: Check that methods have correct signatures\n    for method_name in iface_methods:\n        try:\n            method = getattr(candidate, method_name)\n            if not callable(method):\n                errors.append(f\"candidate missing method {method_name!r} or method is not callable\")\n            # Check method signature (simplified - actual signature checking would be more complex)\n            # This is a placeholder for actual signature validation\n        except AttributeError:\n            errors.append(f\"candidate missing method {method_name!r", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                # Get the class of the candidate\n                cls = candidate.__class__\n                # Check if the class implements the interface\n                if not hasattr(cls, '__implementedBy'):\n                    errors.append(f\"candidate's class does not declare implementation of {iface!r}\")\n                else:\n                    if iface not in cls.__implementedBy:\n                        errors.append(f\"candidate's class does not implement interface {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate's class does not support interface implementation checks\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else {}\n    except AttributeError:\n        # If interface doesn't have methods, skip\n        pass\n    else:\n        for method_name in interface_methods:\n            try:\n                method = getattr", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                # Get the class of the candidate\n                cls = candidate.__class__\n                # Check if the class implements the interface\n                if not hasattr(cls, '__implements__'):\n                    errors.append(f\"candidate class does not declare implementation of {iface!r}\")\n                else:\n                    # Check if the interface is in the implemented interfaces\n                    implemented_interfaces = getattr(cls, '__implements__', ())\n                    if iface not in implemented_interfaces:\n                        errors.append(f\"candidate class does not implement interface {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate class does not support interface implementation checks\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else {}\n    except AttributeError:\n        # If interface doesn't have methods, skip this check", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # No __providedBy__ attribute, so we can't verify the interface claim\n            errors.append(f\"candidate has no __providedBy__ attribute\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else {}\n    except AttributeError:\n        # Interface doesn't have methods defined\n        pass\n\n    # Step 3: Check that methods have correct signatures\n    for method_name, method_spec in interface_methods.items():\n        try:\n            candidate_method = getattr(candidate, method_name)\n            if not callable(candidate_method):\n                errors.append(f\"candidate missing method {method_name!r} or method is not callable\")\n            else:\n                # Check method signature (simplified - actual signature checking would be more complex)\n                # This is a placeholder for signature checking\n                pass\n        except AttributeError:\n            errors", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            provided_interfaces = candidate.__providedBy__\n            if iface not in provided_interfaces:\n                errors.append(f\"candidate does not provide interface {iface!r}\")\n        except AttributeError:\n            # Check if the class implements the interface\n            try:\n                if not hasattr(candidate.__class__, '__implementedBy'):\n                    errors.append(f\"candidate's class does not declare implementation of {iface!r}\")\n                else:\n                    implemented_interfaces = candidate.__class__.__implementedBy__\n                    if iface not in implemented_interfaces:\n                        errors.append(f\"candidate's class does not implement interface {iface!r}\")\n            except AttributeError:\n                errors.append(f\"candidate's class does not support interface implementation\")\n\n    # Step 2: Check that all necessary methods are defined\n    try:\n        methods = iface._methods\n    except AttributeError:\n        methods = []\n\n    for method_name in methods:\n        try:\n            method = getattr(candidate, method_name)\n            if not callable(method):\n                errors.append(f\"missing method {method_name!r} or method is not callable\")\n        except AttributeError:\n            errors.append(f\"missing method {method_name!r}\")"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 954, "num_retrieved": 3, "inference_time": 0.9668855518102646, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"candidate must be a class\")\n    \n    # Check if the candidate has the interface implemented\n    # This is a simplified implementation based on typical interface verification\n    # In real implementation, this would involve checking the class's MRO and interface implementation\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the interface is in the class's MRO\n        # This is a simplified version; actual implementation would depend on Zope interface system\n        try:\n            # Check if the interface is in the class's MRO\n            for base in candidate.__mro__:\n                if iface is base:\n                    return True\n            return False\n        except AttributeError:\n            return False\n    \n    # In strict mode, we need to ensure the interface is properly implemented\n    # This would typically involve checking the interface's implementedBy and MRO\n    try:\n        # Check if the interface is in the class's MRO\n        for base in candidate.__mro__:\n            if iface is base:\n                return True\n        return False\n    except AttributeError:\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if candidate is a subclass of iface or implements iface\n    try:\n        # Use the interface's implementedBy to check if candidate implements iface\n        from zope.interface import implementedBy\n        implements = implementedBy(candidate)\n        \n        # Check if iface is in the implemented interfaces of candidate\n        if iface not in implements:\n            return False\n        \n        # If tentative is True, we allow for more lenient verification\n        if tentative:\n            # In tentative mode, we only check that the candidate has the interface\n            # in its implemented interfaces, without strict C3 linearization\n            return True\n        \n        # In strict mode, we verify the full C3 linearization\n        # This is a simplified version - actual implementation would involve\n        # checking the MRO and ensuring the interface is properly inherited\n        return True\n        \n    except Exception:\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"candidate must be a class\")\n    \n    # Check if candidate is a subclass of iface\n    if not issubclass(candidate, iface):\n        return False\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check that the class is a subclass\n        # and that it has the interface in its MRO (Method Resolution Order)\n        try:\n            # Check if iface is in the MRO of candidate\n            if iface not in candidate.__mro__:\n                return False\n        except AttributeError:\n            return False\n        return True\n    \n    # In strict mode, we verify that the class is a direct or indirect subclass\n    # and that the interface is properly implemented\n    return issubclass(candidate, iface)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"candidate must be a class\")\n    \n    # Check if iface is an interface\n    if not hasattr(iface, '__implements__'):\n        raise TypeError(\"iface must be an interface\")\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the candidate has the interface\n        # in its MRO (Method Resolution Order) without strict verification\n        try:\n            # Check if iface is in the MRO of candidate\n            for base in candidate.__mro__:\n                if iface is base:\n                    return True\n            return False\n        except AttributeError:\n            return False\n    \n    # In strict mode, we need to verify that the candidate implements the interface\n    # by checking the MRO and ensuring the interface is properly implemented\n    try:\n        # Check if iface is in the MRO of candidate\n        for base in candidate.__mro__:\n            if iface is base:\n                return True\n        return False\n    except AttributeError:\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"candidate must be a class\")\n    \n    # Check if iface is an interface\n    if not hasattr(iface, '__implements__'):\n        raise TypeError(\"iface must be an interface\")\n    \n    # If tentative is True, we allow for the candidate to not implement the interface\n    # strictly, but still might provide it in a loose sense\n    if tentative:\n        # In tentative mode, we only check if the candidate has a method or attribute\n        # that might be related to the interface, but this is not a full verification\n        # For now, we just return True if the candidate is a class\n        return True\n    \n    # Otherwise, we need to verify that the candidate implements the interface\n    # This involves checking the class hierarchy for the interface\n    # We use the C3 linearization to determine the method resolution order\n    # and check if the interface is in the MRO\n    \n    # Get the method resolution order of the candidate\n    mro = candidate.__mro__\n    \n    # Check if the interface is in the MRO\n    for cls in mro:\n        if hasattr(cls, '__implements__'):\n            # Check if the interface is implemented by this class\n            if iface in cls.__implements__:\n                return True\n    \n    # If we reach here, the candidate does not implement the interface\n    return False"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1070, "num_retrieved": 7, "inference_time": 0.9668855518102646, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any of the bases have a metaclass defined\n    for base in bases:\n        # If the base is a class with a __metaclass__ attribute, use it\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass (type)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any of the bases provide a metaclass via __metaclass__\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass (type)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any of the bases have a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass (type)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any of the bases have a metaclass defined\n    for base in bases:\n        # If a base has a __metaclass__ attribute, use it\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass (type)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any of the bases have a metaclass defined\n    for base in bases:\n        # If base has a __metaclass__ attribute, use it\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass (type)\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.9668855518102646, "generate_results": ["def pop(self, key, default=__marker):\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 941, "num_retrieved": 3, "inference_time": 0.9668855518102646, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the context contains LRU and RR cache implementations, not LFU\n    # Since LFU is not implemented in the provided context, we cannot\n    # accurately implement popitem for LFU based on the given information.\n    # Therefore, we return a placeholder that would be appropriate for an\n    # LFU cache, though it's not fully specified in the context.\n    raise NotImplementedError(\"LFU cache popitem implementation not available in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the context contains LRU and RR cache implementations, not LFU\n    # Since LFU is not implemented in the provided context, we cannot\n    # accurately implement popitem for LFU based on the given information.\n    # Therefore, we return a placeholder that would be appropriate for an\n    # LFU cache, though it's not fully specified in the context.\n    raise NotImplementedError(\"LFU cache popitem not implemented in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LRU and RR caches have different popitem behaviors.\n    # Since the docstring specifies \"least frequently used\", this suggests\n    # an LFU (Least Frequently Used) cache, which is not fully described in the context.\n    # Without a specific LFU implementation details, we cannot accurately implement this.\n    # Therefore, we return a placeholder that would be appropriate for a missing LFU implementation.\n    raise NotImplementedError(\"LFU cache implementation not available in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the context contains LRU and RR caches, not LFU.\n    # Since LFU is not implemented in the provided code, we cannot\n    # accurately implement popitem for an LFU cache based on the given context.\n    # Therefore, we return a placeholder that would be appropriate\n    # if the LFU cache were implemented with a frequency counter.\n    raise NotImplementedError(\"LFU cache popitem not implemented in provided context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the context contains LRU and RR cache implementations, not LFU\n    # Since LFU is not implemented in the provided context, we cannot\n    # accurately implement popitem for LFU based on the given information.\n    # Therefore, we return a placeholder that would be correct in a proper LFU context.\n    raise NotImplementedError(\"LFU cache popitem implementation not available in context\")"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 901, "num_retrieved": 3, "inference_time": 0.9668855518102646, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the key with the least recent usage (LRU)\n    keys = list(self.keys())\n    key = keys[0]  # In LRU, the first key in the list is the least recently used\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the key with the least recent usage (first in LRU order)\n    keys = list(self.keys())\n    key = keys[0]  # In LRU, the first key is the least recently used\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the key with the least recent usage (least recently used)\n    keys = list(self.keys())\n    key = keys[0]\n    for k in keys:\n        if k not in self:\n            continue\n        # In LRU, we need to find the least recently used key\n        # This would typically be the one that was accessed the farthest back\n        # Since we don't have explicit access to access order, we assume\n        # the first key in the list is the least recently used\n        key = k\n        break\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the key with the least recent usage (first in LRU order)\n    keys = list(self.keys())\n    key = keys[0]  # In LRU, the first key is the least recently used\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Get the key with the least recent usage (LRU)\n    keys = list(self.keys())\n    key = keys[0]  # In LRU, the first key in the list is the least recently used\n    value = self.pop(key)\n    return (key, value)"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 900, "num_retrieved": 3, "inference_time": 0.7552242130041122, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 942, "num_retrieved": 3, "inference_time": 0.7552242130041122, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    import random\n    try:\n        key = random.choice(list(self.__order.keys()))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    import random\n    try:\n        key = random.choice(list(self.__order.keys()))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    import random\n    try:\n        key = random.choice(list(self.__order.keys()))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    import random\n    try:\n        key = random.choice(list(self.__order.keys()))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    import random\n    try:\n        key = random.choice(list(self.__order.keys()))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7552242130041122, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return self._in_regex", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return self._in_regex", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return re.compile(self._in_style.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return re.compile(self._in_style.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return self._in_regex"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 952, "num_retrieved": 3, "inference_time": 0.7552242130041122, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.7552242130041122, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 824, "num_retrieved": 0, "inference_time": 0.7552242130041122, "generate_results": ["def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    if not isinstance(timestr, str):\n        raise TypeError(\"Expected string input for timestr\")\n    \n    # Parse the string using the internal parser\n    result = self._parse(timestr, default=default, ignoretz=ignoretz, tzinfos=tzinfos, **kwargs)\n    \n    # If fuzzy_with_tokens is enabled, return a tuple\n    if hasattr(result, 'fuzzy_tokens') and result.fuzzy_tokens:\n        return (result, result.fuzzy_tokens)\n    \n    return result", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    if not isinstance(timestr, str):\n        raise TypeError(\"Expected string input for timestr\")\n    \n    # Parse the string using the internal parser\n    result = self._parse(timestr, default=default, ignoretz=ignoretz, tzinfos=tzinfos, **kwargs)\n    \n    # If fuzzy_with_tokens is True, return a tuple (datetime, tokens)\n    if hasattr(self, 'fuzzy_with_tokens') and self.fuzzy_with_tokens:\n        return result\n    else:\n        return result[0] if isinstance(result, tuple) else result", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    if not isinstance(timestr, str):\n        raise TypeError(\"Expected string input for timestr\")\n    \n    # Parse the string using the internal parser\n    parsed_dt = self._parse(timestr, default=default, ignoretz=ignoretz, tzinfos=tzinfos, **kwargs)\n    \n    # If fuzzy_with_tokens is True, return a tuple (datetime, tokens)\n    # Otherwise, return just the datetime object\n    if hasattr(parsed_dt, 'fuzzy_tokens') and parsed_dt.fuzzy_tokens:\n        return (parsed_dt, parsed_dt.fuzzy_tokens)\n    \n    return parsed_dt"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.7552242130041122, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Determine if the datetime is ambiguous and in a fold state\n    is_ambiguous = self._is_ambiguous(dt)\n    \n    if is_ambiguous:\n        # If ambiguous, we need to determine which fold to use\n        # The fold=0 is the first occurrence (chronologically earlier)\n        # The fold=1 is the second occurrence (chronologically later)\n        # Since we know it's unambiguous in UTC, we use fold=0 by default\n        # unless we have specific logic to determine fold state\n        dt_folded = enfold(dt, fold=0)\n    else:\n        dt_folded = dt\n    \n    # Convert to the new timezone\n    return dt_folded.astimezone(self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Determine if the datetime is ambiguous and in a fold state\n    is_ambiguous = self._is_ambiguous(dt)\n    \n    if is_ambiguous:\n        # If ambiguous, we need to determine which fold to use\n        # The fold=0 is the first occurrence (chronologically earlier)\n        # The fold=1 is the second occurrence (chronologically later)\n        # Since we know it's unambiguous in UTC, we use fold=0 by default\n        # unless we have specific logic to determine fold state\n        dt_folded = enfold(dt, fold=0)\n    else:\n        dt_folded = dt\n    \n    # Convert to the new timezone\n    return dt_folded.astimezone(self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Determine if the datetime is ambiguous in the current timezone\n    is_ambiguous = self._is_ambiguous(dt)\n    \n    # If the datetime is ambiguous, we need to determine which fold to use\n    # We use fold=0 by default (the first occurrence), but if the datetime\n    # is ambiguous and we're in a fold state, we need to handle it properly\n    if is_ambiguous:\n        # Use the first occurrence (fold=0) as it's the unambiguous case\n        # We assume the fold state is handled by the underlying tzinfo\n        dt = dt.replace(tzinfo=self)\n        # We need to ensure we're using the correct fold\n        # Since we're in fromutc, we're dealing with UTC time\n        # and converting to local time, so we use fold=0 by default\n        # unless the timezone specifically defines fold behavior\n        wall_0 = enfold(dt, fold=0)\n        wall_1 = enfold(dt, fold=1)\n        \n        # If both have the same offset and DST, then it's not ambiguous\n        # Otherwise, we pick the one that matches the original dt\n        if wall_0.utcoffset() == wall_1.utcoffset() and wall_0.dst() == wall_1.dst():\n            # Not ambiguous, use fold=0\n            return wall_0\n        else:\n            # Ambiguous, use fold=0 as the first occurrence\n            return wall_0\n    else:\n        # Not ambiguous, just convert directly\n        dt = dt.replace(tzinfo=self)\n        return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Determine if the datetime is ambiguous and in a fold state\n    is_ambiguous = self._is_ambiguous(dt)\n    \n    if is_ambiguous:\n        # If ambiguous, we need to determine which fold to use\n        # Use fold=0 by default (first occurrence)\n        # If fold=1 is available and preferred, use that\n        # For now, use fold=0 as the default\n        folded_dt = self._localize(dt.replace(tzinfo=None), fold=0)\n    else:\n        folded_dt = self._localize(dt.replace(tzinfo=None))\n    \n    return folded_dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Determine if the datetime is ambiguous in the current timezone\n    is_ambiguous = self._is_ambiguous(dt)\n    \n    # If the datetime is ambiguous, we need to determine which fold to use\n    # We use fold=0 by default (the first occurrence chronologically)\n    # If the datetime is ambiguous and we're in a fold state, we need to handle it\n    if is_ambiguous:\n        # Use the fold=0 version (first occurrence)\n        result = self._localize(dt.replace(tzinfo=None), fold=0)\n    else:\n        # Use the standard conversion\n        result = self._localize(dt.replace(tzinfo=None))\n    \n    return result"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.7552242130041122, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1007, "num_retrieved": 5, "inference_time": 0.7552242130041122, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.7552242130041122, "generate_results": ["def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message first\n    if request.get(\"type\") == \"public\" or request.get(\"message_type\") == \"public\":\n        return \"diaspora_public\"\n    \n    # Check if it's a private message\n    elif request.get(\"type\") == \"private\" or request.get(\"message_type\") == \"private\":\n        return \"diaspora_private\"\n    \n    # Check if it's a legacy payload\n    elif request.get(\"legacy\") == True or request.get(\"is_legacy\") == True:\n        return \"diaspora_legacy\"\n    \n    # Default case - not a Diaspora request\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message first\n    if request.get(\"type\") == \"public\" or request.get(\"message_type\") == \"public\":\n        return \"diaspora_public\"\n    \n    # Check if it's a private message\n    elif request.get(\"type\") == \"private\" or request.get(\"message_type\") == \"private\":\n        return \"diaspora_private\"\n    \n    # Check if it's a legacy payload\n    elif request.get(\"legacy\") == True or request.get(\"payload_type\") == \"legacy\":\n        return \"diaspora_legacy\"\n    \n    # Default case - not a Diaspora request\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message first\n    if request.get(\"type\") == \"public\" or request.get(\"message_type\") == \"public\":\n        return \"diaspora_public\"\n    \n    # Check if it's a private message\n    elif request.get(\"type\") == \"private\" or request.get(\"message_type\") == \"private\":\n        return \"diaspora_private\"\n    \n    # Check if it's a legacy payload\n    elif request.get(\"legacy\") == True or request.get(\"legacy\") == \"true\":\n        return \"diaspora_legacy\"\n    \n    # Default case - not a Diaspora request\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message first\n    if request.get(\"type\") == \"public_message\":\n        return \"diaspora_public\"\n    \n    # Check if it's a private message\n    elif request.get(\"type\") == \"private_message\":\n        return \"diaspora_private\"\n    \n    # Check if it's a legacy payload\n    elif request.get(\"legacy\", False):\n        return \"diaspora_legacy\"\n    \n    # Default case - not a Diaspora request\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message first\n    if request.get(\"type\") == \"public_message\":\n        return \"diaspora_public\"\n    \n    # Check if it's a private message\n    elif request.get(\"type\") == \"private_message\":\n        return \"diaspora_private\"\n    \n    # Check if it's a legacy payload\n    elif request.get(\"legacy\", False):\n        return \"diaspora_legacy\"\n    \n    # Default case - not a Diaspora request\n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 938, "num_retrieved": 3, "inference_time": 0.7552242130041122, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) is not None", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) is not None", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) is not None", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) is not None", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) is not None"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 892, "num_retrieved": 2, "inference_time": 0.7552242130041122, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser\n    import re\n    \n    # Ensure dt is a datetime object\n    if not isinstance(dt, datetime):\n        dt = parser.parse(dt)\n    \n    # Format the datetime in RFC 1123 format (e.g., \"Mon, 01 Jan 2023 12:00:00 GMT\")\n    # This is the format expected by D* nodes\n    fmt = '%a, %d %b %Y %H:%M:%S GMT'\n    return dt.strftime(fmt)", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser\n    import re\n    \n    # Ensure dt is a datetime object\n    if not isinstance(dt, datetime):\n        dt = parser.parse(dt)\n    \n    # Format according to RFC 1123: \"Mon, 02 Jan 2006 15:04:05 GMT\"\n    return dt.strftime('%a, %d %b %Y %H:%M:%S GMT')", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser\n    import re\n    \n    # Ensure dt is a datetime object\n    if not isinstance(dt, datetime):\n        dt = parser.parse(dt)\n    \n    # Format according to RFC 1123: \"Mon, 02 Jan 2006 15:04:05 GMT\"\n    return dt.strftime('%a, %d %b %Y %H:%M:%S GMT')", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser\n    import re\n    \n    # Ensure dt is a datetime object\n    if not isinstance(dt, datetime):\n        dt = parser.parse(dt)\n    \n    # Format the datetime in RFC 1123 format (e.g., \"Mon, 01 Jan 2020 12:34:56 GMT\")\n    # This is the format expected by D* nodes\n    fmt = '%a, %d %b %Y %H:%M:%S GMT'\n    return dt.strftime(fmt)", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser\n    import re\n    \n    # Ensure dt is a datetime object\n    if not isinstance(dt, datetime):\n        dt = parser.parse(dt)\n    \n    # Format the datetime in RFC 1123 format (e.g., \"Mon, 01 Jan 2020 12:00:00 GMT\")\n    # This is the format expected by D* nodes\n    return dt.strftime('%a, %d %b %Y %H:%M:%S GMT')"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.7552242130041122, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result_lines = []\n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        # Check if line starts a code block (with triple backticks or triple quotes)\n        if line.strip().startswith('```') or line.strip().startswith('\"\"\"'):\n            if in_code_block:\n                # End of code block\n                in_code_block = False\n            else:\n                # Start of code block\n                in_code_block = True\n                code_block_type = line.strip().strip('`\"')\n        elif in_code_block:\n            # Inside code block - skip tag processing\n            result_lines.append(line)\n        else:\n            # Outside code block - process tags\n            # Remove any leading/trailing whitespace\n            line = line.strip()\n            # Find tags (words starting with #)\n            import re\n            tag_matches = re.finditer(r'#(\\w+)', line)\n            for match in tag_matches:\n                tag = match.group(1)\n                tags.add(tag)\n                # Replace the tag with the replacer function if provided\n                if replacer:\n                    replacement = replacer(tag)\n                    line = line.replace(f'#{tag}', replacement)\n            result_lines.append(line)\n    \n    return tags, '\\n'.join(result_lines)", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result_lines = []\n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        if in_code_block:\n            # Check if we're exiting a code block\n            if line.strip().startswith('```'):\n                in_code_block = False\n                code_block_type = None\n            result_lines.append(line)\n        else:\n            # Check if we're entering a code block\n            if line.strip().startswith('```'):\n                in_code_block = True\n                code_block_type = line.strip()[3:].strip()\n                result_lines.append(line)\n            else:\n                # Process non-code block lines\n                # Remove any code block markers\n                line = line.strip()\n                if line.startswith('```'):\n                    continue\n                \n                # Check for tags (assuming tags are in the format #tag)\n                # We need to find #tag patterns, but ignore them inside code blocks\n                if not in_code_block:\n                    # Find all #tag patterns\n                    import re\n                    tag_pattern = r'#(\\w+)'\n                    matches = re.finditer(tag_pattern, line)\n                    for match in matches:\n                        tag = match.group(1)\n                        tags.add(tag)\n                        # Replace the tag with the replacer function if provided\n                        if replacer:\n                            line = line.replace(f'#{tag}', replacer(tag))\n                \n                result_lines.append(line)\n    \n    result_text = '\\n'.join(result_lines)\n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result = []\n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        if in_code_block:\n            # Inside code block, skip tag detection\n            result.append(line)\n            continue\n            \n        # Check for code block start (either triple backticks or triple quotes)\n        if line.strip().startswith(\"```\"):\n            in_code_block = True\n            code_block_type = line.strip().split(\" \")[1] if \" \" in line.strip() else None\n            result.append(line)\n            continue\n            \n        # Check for code block end\n        if line.strip().startswith(\"```\") and in_code_block:\n            in_code_block = False\n            result.append(line)\n            continue\n            \n        # Process line outside code block\n        # Look for tags (words starting with #)\n        parts = []\n        i = 0\n        while i < len(line):\n            if line[i:i+1] == \"#\":\n                # Found a potential tag\n                tag_start = i + 1\n                tag_end = tag_start\n                while tag_end < len(line) and line[tag_end] != \" \" and line[tag_end] != \"\\t\":\n                    tag_end += 1\n                tag = line[tag_start:tag_end]\n                if tag and tag[0] == \"#\":\n                    tags.add(tag)\n                    if replacer:\n                        # Replace the tag with the replacer function result\n                        line = line[:tag_start] + replacer(tag) + line[tag_end:]\n                else:\n                    # No valid tag found\n                    pass\n                i = tag_end\n            else:\n                parts.append(line[i])\n                i += 1\n                \n        result.append(\"\".join(parts))\n    \n    return tags, \"\".join(result)", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result_lines = []\n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        if in_code_block:\n            # Check if we're exiting a code block\n            if line.strip().startswith(('```', '``` ')):\n                in_code_block = False\n            result_lines.append(line)\n        else:\n            # Check if we're entering a code block\n            if line.strip().startswith(('```', '``` ')):\n                in_code_block = True\n                code_block_type = line.strip().split(' ')[1] if ' ' in line.strip() else None\n                result_lines.append(line)\n            else:\n                # Process non-code block line\n                # Remove any code block markers\n                line = line.strip()\n                # Check if line contains a tag (starts with #)\n                if line.startswith('#'):\n                    # Extract tag (everything after #)\n                    tag = line[1:].strip()\n                    if tag:\n                        tags.add(tag)\n                        # Apply replacer if provided\n                        if replacer:\n                            line = replacer(tag)\n                result_lines.append(line)\n    \n    return tags, '\\n'.join(result_lines)", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result_lines = []\n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        if in_code_block:\n            # Check if we're exiting a code block\n            if line.strip().startswith(('```', '``` ')):\n                in_code_block = False\n            result_lines.append(line)\n        else:\n            # Check if we're entering a code block\n            if line.strip().startswith(('```', '``` ')):\n                in_code_block = True\n                code_block_type = line.strip().split('`')[1] if '```' in line.strip() else None\n                result_lines.append(line)\n            else:\n                # Process non-code block line\n                # Remove code block markers\n                line = line.strip()\n                # Check for code block markers\n                if line.startswith('```'):\n                    in_code_block = True\n                    code_block_type = line.strip().split('`')[1] if '```' in line.strip() else None\n                    result_lines.append(line)\n                else:\n                    # Look for tags (words starting with #)\n                    # Only process outside code blocks\n                    # Use regex to find tags\n                    import re\n                    tag_pattern = r'#([a-zA-Z_][a-zA-Z0-9_]*)(?=\\s|$)'\n                    matches = re.finditer(tag_pattern, line)\n                    for match in matches:\n                        tag = match.group(1)\n                        tags.add(tag)\n                        # Replace tag with replacer if provided\n                        if replacer:\n                            line = line.replace(f'#{tag}', replacer(tag))\n                    result_lines.append(line)\n    \n    # Join lines back into text\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.7552242130041122, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in text with HTML links\n    # This is a simplified implementation - actual implementation would depend on\n    # the specific pattern matching and link formatting requirements\n    import re\n    \n    # Pattern to match URLs (basic pattern)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Convert to absolute URL if needed\n        if url.startswith('www.'):\n            url = 'https://' + url\n        \n        # Return HTML link with attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{url}</a>'\n    \n    # Replace all URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in text with HTML links\n    # This is a simplified implementation - actual implementation would depend on\n    # the specific pattern matching and link processing requirements\n    import re\n    \n    # Pattern to match URLs (basic pattern)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(\\.[a-zA-Z]{2,})+(/[^<>\"{}|\\\\^`[\\]]*)*)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Add attributes like target=\"_blank\", rel=\"noopener\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{url}</a>'\n    \n    # Replace URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in text with HTML anchor tags\n    # This is a simplified implementation - actual implementation would depend on\n    # the specific pattern matching and link processing requirements\n    import re\n    \n    # Pattern to match URLs (basic pattern)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(\\.[a-zA-Z]{2,})+([^\\s<>\"{}|\\\\^`[\\]]*)?)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Add attributes like target=\"_blank\" and rel=\"noopener\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{url}</a>'\n    \n    # Replace URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in text with HTML anchor tags\n    # This is a simplified implementation - actual implementation would depend on\n    # the specific link detection and formatting rules\n    import re\n    \n    # Pattern to match URLs (basic version)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(\\.[a-zA-Z]{2,})+(/[^<>\"{}|\\\\^`[\\]]*)*)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Add attributes like target=\"_blank\", rel=\"noopener\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{url}</a>'\n    \n    # Replace URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in text with HTML links\n    # This is a simplified implementation - actual implementation would depend on\n    # the specific pattern matching and link formatting requirements\n    import re\n    \n    # Pattern to match URLs (simple version)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(\\.[a-zA-Z]{2,})+(/[^<>\"{}|\\\\^`[\\]]*)*)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Add attributes like target=\"_blank\" and rel=\"nofollow\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"nofollow\">{url}</a>'\n    \n    # Replace all URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 914, "num_retrieved": 2, "inference_time": 0.7552242130041122, "generate_results": ["import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1051, "num_retrieved": 6, "inference_time": 0.7552242130041122, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or underscore and only contain letters, numbers, and underscores\n    # It should not be empty and should not start or end with a separator\n    if not tag or not tag[0].isalnum() and tag[0] != '_':\n        return False\n    if not tag[-1].isalnum() and tag[-1] != '_':\n        return False\n    for char in tag[1:-1]:\n        if not char.isalnum() and char != '_':\n            return False\n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or underscore and can contain letters, numbers, and underscores\n    # It should not be empty and should not contain spaces or special characters\n    if not tag or not tag.strip():\n        return False\n    \n    # Check if tag starts with a letter or underscore\n    if not (tag[0].isalpha() or tag[0] == '_'):\n        return False\n    \n    # Check if the rest of the tag contains only letters, numbers, and underscores\n    for char in tag[1:]:\n        if not (char.isalnum() or char == '_'):\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or underscore and only contain letters, numbers, and underscores\n    # It should not be empty and should not start or end with a separator\n    if not tag or not tag[0].isalnum() and tag[0] != '_':\n        return False\n    if not tag[-1].isalnum() and tag[-1] != '_':\n        return False\n    for char in tag[1:-1]:\n        if not char.isalnum() and char != '_':\n            return False\n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or underscore and only contain letters, numbers, and underscores\n    # It should not be empty and should not start or end with a separator\n    if not tag or not tag[0].isalnum() and tag[0] != '_':\n        return False\n    if not tag[-1].isalnum() and tag[-1] != '_':\n        return False\n    for char in tag[1:-1]:\n        if not char.isalnum() and char != '_':\n            return False\n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or underscore and can contain letters, numbers, and underscores\n    # It should not be empty and should not contain spaces or special characters\n    if not tag or not tag[0].isalnum() or tag[0] == ' ':\n        return False\n    \n    # Check if tag contains only alphanumeric characters and underscores\n    for char in tag:\n        if not char.isalnum() and char != '_':\n            return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1011, "num_retrieved": 5, "inference_time": 0.767255038022995, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 834, "num_retrieved": 1, "inference_time": 0.767255038022995, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 975, "num_retrieved": 4, "inference_time": 0.767255038022995, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = '/.well-known/nodeinfo'\n    \n    # Construct the full URL to fetch the NodeInfo document\n    nodeinfo_url = f\"{url}{document_path}\"\n    \n    # Fetch the document using the fetch_document function (assumed to be available)\n    doc, status_code, error = fetch_document(host=url, path=document_path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    # Return the parsed NodeInfo document\n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = '/.well-known/nodeinfo'\n    \n    # Construct the full URL to fetch the NodeInfo document\n    nodeinfo_url = f\"{url}{document_path}\"\n    \n    # Fetch the document using the fetch_document function (assumed to be available)\n    doc, status_code, error = fetch_document(host=url, path=document_path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = '/.well-known/nodeinfo'\n    \n    # Construct the full URL to fetch the NodeInfo document\n    nodeinfo_url = f\"{url}{document_path}\"\n    \n    # Fetch the document using the fetch_document function (assumed to be available)\n    doc, status_code, error = fetch_document(host=url, path=document_path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = '/.well-known/nodeinfo'\n    \n    # Construct the full URL to fetch the NodeInfo document\n    nodeinfo_url = f\"{url}{document_path}\"\n    \n    # Fetch the document using the fetch_document function (assumed to be available)\n    doc, status_code, error = fetch_document(host=url, path=document_path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    # Return the parsed document\n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = '/.well-known/nodeinfo'\n    \n    # Construct the full URL to fetch the NodeInfo document\n    nodeinfo_url = f\"{url}{document_path}\"\n    \n    # Fetch the document using the fetch_document function (assumed to be available)\n    doc, status_code, error = fetch_document(host=url, path=document_path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    # Return the parsed NodeInfo document\n    return doc"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 987, "num_retrieved": 4, "inference_time": 0.767255038022995, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from Crypto.PublicKey import RSA\n        from lxml import etree\n        \n        # Convert the XML document to a string\n        xml_string = etree.tostring(doc, encoding='unicode')\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_string.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(public_key).verify(hash_obj, signature)\n        return True\n        \n    except (ValueError, TypeError, Exception):\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from Crypto.PublicKey import RSA\n        from lxml import etree\n        \n        # Convert the XML document to a string\n        xml_string = etree.tostring(doc, encoding='unicode')\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_string.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(public_key).verify(hash_obj, signature)\n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from Crypto.PublicKey import RSA\n        from lxml import etree\n        \n        # Convert the XML document to a string\n        xml_string = etree.tostring(doc, encoding='unicode')\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_string.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(public_key).verify(hash_obj, signature)\n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from Crypto.PublicKey import RSA\n        from lxml import etree\n        \n        # Convert the XML document to a string\n        xml_string = etree.tostring(doc, encoding='unicode')\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_string.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(public_key).verify(hash_obj, signature)\n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from Crypto.PublicKey import RSA\n        from lxml import etree\n        \n        # Convert the XML document to a string\n        xml_string = etree.tostring(doc, encoding='unicode')\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_string.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(public_key).verify(hash_obj, signature)\n        return True\n    except (ValueError, TypeError, Exception):\n        return False"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 977, "num_retrieved": 4, "inference_time": 0.767255038022995, "generate_results": ["import re\nimport json\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    # XRD format is XML, so we need to extract relevant parts\n    # Look for the Alias element and links\n    # In Diaspora XRD, we expect elements like <Alias> and <link> tags\n    result = {\n        'handle': None,\n        'links': []\n    }\n    \n    # Extract handle from Alias element\n    alias_match = re.search(r'<Alias[^>]*>([^<]+)</Alias>', document, re.IGNORECASE)\n    if alias_match:\n        result['handle'] = alias_match.group(1)\n    \n    # Extract links from link elements\n    link_matches = re.findall(r'<link[^>]*rel=\"([^\"]+)\"[^>]*type=\"([^\"]+)\"[^>]*href=\"([^\"]+)\"[^>]*>', document, re.IGNORECASE)\n    for rel, typ, href in link_matches:\n        result['links'].append({\n            'rel': rel,\n            'type': typ,\n            'href': href\n        })\n    \n    return result", "import re\nimport json\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    # XRD format is XML, so we need to parse it manually\n    # Look for the Alias element which contains the user's handle\n    # and extract links from the links section\n    \n    # Simple parsing of XRD - look for Alias and Link elements\n    # This is a simplified version assuming standard structure\n    result = {\n        'handle': None,\n        'links': []\n    }\n    \n    # Extract handle from Alias element\n    handle_match = re.search(r'<Alias[^>]*>([^<]+)</Alias>', document)\n    if handle_match:\n        result['handle'] = handle_match.group(1)\n    \n    # Extract links from Link elements\n    link_matches = re.findall(r'<Link[^>]*rel=\"([^\"]+)\"[^>]*type=\"([^\"]+)\"[^>]*href=\"([^\"]+)\"[^>]*>', document)\n    for rel, type_, href in link_matches:\n        result['links'].append({\n            'rel': rel,\n            'type': type_,\n            'href': href\n        })\n    \n    return result", "import re\nimport json\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    # XRD format is XML, so we need to extract relevant parts\n    # Look for the Alias element and links\n    # In Diaspora XRD, we expect an Alias element and links with specific rels\n    try:\n        # Extract the Alias element (contains the handle)\n        handle_match = re.search(r'<Alias[^>]*>([^<]+)</Alias>', document, re.IGNORECASE)\n        if not handle_match:\n            return {}\n        \n        handle = handle_match.group(1)\n        \n        # Extract links with specific rels\n        links = []\n        rel_patterns = [\n            (r'href=\"([^\"]+)\"', 'href'),\n            (r'href=\"([^\"]+)\"', 'href'),\n            (r'href=\"([^\"]+)\"', 'href')\n        ]\n        \n        # Look for links with specific rels\n        rel_patterns = [\n            (r'<link[^>]+rel=\"http://microformats.org/profile/hcard\"[^>]*href=\"([^\"]+)\"[^>]*>', 'hcard'),\n            (r'<link[^>]+rel=\"http://joindiaspora.com/seed_location\"[^>]*href=\"([^\"]+)\"[^>]*>', 'seed_location'),\n            (r'<link[^>]+rel=\"http://joindiaspora.com/guid\"[^>]*href=\"([^\"]+)\"[^>]*>', 'guid')\n        ]\n        \n        for pattern, rel_type in rel_patterns:\n            match = re.search(pattern, document, re.IGNORECASE)\n            if match:\n                links.append({", "import re\nimport json\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {}\n            result['handle'] = data['subject']\n            result['links'] = []\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    # XRD format is XML, so we need to extract relevant parts\n    # Look for the Alias element and links\n    # In XRD, we expect elements like <element> and <link> tags\n    try:\n        # Extract the Alias element which contains the user's profile URL\n        alias_match = re.search(r'<Alias[^>]*>([^<]+)</Alias>', document, re.IGNORECASE)\n        if alias_match:\n            alias_url = alias_match.group(1)\n            # Extract links\n            links = []\n            link_matches = re.findall(r'<link[^>]*rel=\"([^\"]+)\"[^>]*type=\"([^\"]+)\"[^>]*href=\"([^\"]+)\"[^>]*>', document, re.IGNORECASE)\n            for rel, typ, href in link_matches:\n                links.append({\n                    'rel': rel,\n                    'type': typ,\n                    'href': href\n                })\n            \n            result = {\n                'handle': None,\n                'links': links\n            }\n            \n            # Try to extract handle from the alias URL\n            if '/' in alias_url:\n                parts = alias_url.split('/')\n                if len(parts) >= 3:\n                    # Extract the handle from the path\n                    # Example: https://example.com/people/abc123 -> user@domain.tld\n                    # We need to infer the handle from the structure\n                    # In Diaspora, the handle is typically the user part of the URL\n                    # We can extract it from the path\n                    user_part = parts[2]  # This", "import re\nimport json\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    # XRD format is XML, so we need to extract relevant elements\n    # Look for Alias element which contains the user handle\n    # and extract links from the XRD document\n    \n    # Extract Alias element (contains the user handle)\n    handle_match = re.search(r'<Alias[^>]*>([^<]+)</Alias>', document, re.IGNORECASE)\n    if not handle_match:\n        return {}\n    \n    handle = handle_match.group(1)\n    \n    # Extract links from the XRD document\n    links = []\n    link_matches = re.findall(r'<Link[^>]*rel=\"([^\"]+)\"[^>]*type=\"([^\"]+)\"[^>]*href=\"([^\"]+)\"[^>]*>', document, re.IGNORECASE)\n    for rel, typ, href in link_matches:\n        links.append({\n            'rel': rel,\n            'type': typ,\n            'href': href\n        })\n    \n    return {\n        'handle': handle,\n        'links': links\n    }"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 975, "num_retrieved": 4, "inference_time": 0.767255038022995, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Extract the host from the handle\n        if \"@\" not in handle:\n            return None\n        \n        host = handle.split(\"@\")[1]\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"http://{host}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, _, _ = fetch_document(webfinger_url)\n        \n        # Return the document if successfully retrieved\n        return document\n        \n    except Exception:\n        # Return None if any error occurs (does not raise)\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Extract host from handle (e.g., \"user@host\" -> \"host\")\n        if \"@\" not in handle:\n            return None\n        \n        host = handle.split(\"@\")[1]\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"http://{host}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, _, _ = fetch_document(webfinger_url)\n        \n        # Return the document if successfully retrieved\n        return document\n        \n    except Exception:\n        # Do not raise if retrieval fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Extract host from handle (e.g., \"user@host\" -> \"host\")\n        if \"@\" not in handle:\n            return None\n        \n        host = handle.split(\"@\")[1]\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"http://{host}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, _, _ = fetch_document(webfinger_url)\n        \n        # Return the document if successfully retrieved\n        return document\n        \n    except Exception:\n        # Do not raise if retrieval fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Extract host from handle (e.g., \"user@host\" -> \"host\")\n        if \"@\" not in handle:\n            return None\n        \n        host = handle.split(\"@\")[1]\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"http://{host}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, _, _ = fetch_document(webfinger_url)\n        \n        # Return the document if successfully retrieved\n        return document\n        \n    except Exception:\n        # Do not raise if retrieval fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Extract the host from the handle\n        if \"@\" not in handle:\n            return None\n        \n        host = handle.split(\"@\")[1]\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"http://{host}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, _, _ = fetch_document(webfinger_url)\n        \n        # Return the document if successfully retrieved\n        return document\n        \n    except Exception:\n        # Return None if any error occurs (does not raise)\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 972, "num_retrieved": 4, "inference_time": 0.767255038022995, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # Import the necessary module for Diaspora protocol utilities\n    import importlib\n    \n    # Import the Diaspora webfinger utility module\n    utils_module = importlib.import_module(\"federation.utils.diaspora\")\n    \n    # Call the webfinger retrieval and parsing function\n    return utils_module.retrieve_and_parse_webfinger(handle)", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # Import the necessary module for Diaspora protocol\n    import requests\n    \n    # Construct the webfinger endpoint URL for Diaspora\n    webfinger_url = f\"https://{handle}.diaspora.io/.well-known/webfinger\"\n    \n    try:\n        # Make the request to the webfinger endpoint\n        response = requests.get(webfinger_url, timeout=10)\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Extract and return the profile information if available\n        if 'subject' in data and 'links' in data:\n            profile_info = {\n                'subject': data['subject'],\n                'links': data['links']\n            }\n            return profile_info\n            \n        return {}\n        \n    except requests.exceptions.RequestException as e:\n        logger.warning(f\"Failed to retrieve webfinger for {handle}: {e}\")\n        return {}\n    except ValueError as e:\n        logger.warning(f\"Failed to parse webfinger response for {handle}: {e}\")\n        return {}", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # Import the necessary module for Diaspora protocol utilities\n    import importlib\n    \n    # Import the Diaspora webfinger utility module\n    utils_module = importlib.import_module(\"federation.utils.diaspora\")\n    \n    # Call the webfinger retrieval function from the Diaspora utils module\n    return utils_module.retrieve_webfinger(handle)", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # Import the necessary module for Diaspora protocol\n    import requests\n    \n    # Construct the webfinger endpoint URL for Diaspora\n    webfinger_url = f\"https://diaspora.com/.well-known/webfinger?resource=acct:{handle}\"\n    \n    try:\n        # Make the request to the webfinger endpoint\n        response = requests.get(webfinger_url, timeout=10)\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Extract the profile ID from the response\n        if 'links' in data and len(data['links']) > 0:\n            for link in data['links']:\n                if link['rel'] == 'http://diaspora.com/rel/profile':\n                    return {\n                        'profile_id': link['href'],\n                        'handle': handle\n                    }\n        \n        # If no profile link found, return None\n        return None\n        \n    except requests.exceptions.RequestException as e:\n        logger.warning(f\"Failed to retrieve webfinger for {handle}: {e}\")\n        return None\n    except (KeyError, ValueError) as e:\n        logger.warning(f\"Failed to parse webfinger response for {handle}: {e}\")\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # Import the necessary module for Diaspora webfinger operations\n    from federation.utils.diaspora import get_webfinger_document\n    \n    # Retrieve the webfinger document for the given handle\n    document = get_webfinger_document(handle)\n    \n    # Parse and return the document as a dictionary\n    if document:\n        return document\n    return {}"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1004, "num_retrieved": 5, "inference_time": 0.767255038022995, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger handle using the host\n    handle = f\"acct:{host}\"\n    \n    # Retrieve the webfinger document from the host\n    document = try_retrieve_webfinger_document(handle)\n    \n    if not document:\n        return None\n    \n    # Parse the webfinger document to extract the host-meta information\n    # Since we're dealing with Diaspora, we expect the webfinger response to contain\n    # a link to the host-meta document\n    from federation.hostmeta.generators import DiasporaHostMeta\n    \n    # Extract the webfinger response and parse it to get the host-meta\n    # In practice, this would involve parsing the XRD document from the webfinger response\n    # and extracting the relevant links\n    \n    # For now, we assume the webfinger response contains the necessary information\n    # and create a DiasporaHostMeta instance with the host\n    try:\n        hostmeta = DiasporaHostMeta(webfinger_host=host)\n        return hostmeta.render()\n    except Exception as e:\n        logger.error(\"Failed to generate Diaspora host-meta: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger handle using the host\n    handle = f\"user@{host}\"\n    \n    # Retrieve the webfinger document from the host\n    document = try_retrieve_webfinger_document(handle)\n    \n    if not document:\n        return None\n    \n    # Parse the webfinger document to extract the webfinger_host\n    # In a real implementation, parsing the document would extract the relevant fields\n    # For now, we assume the webfinger_host can be derived from the host parameter\n    # or from the document itself (this is a simplified version)\n    \n    # Since we don't have full parsing logic here, we'll use the host directly\n    # as the webfinger_host for generating the host-meta\n    webfinger_host = host\n    \n    # Generate the host-meta using the Diaspora template with webfinger_host\n    return generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger handle using the host\n    handle = f\"acct:{host}\"\n    \n    # Retrieve the webfinger document from the host\n    document = try_retrieve_webfinger_document(handle)\n    \n    if not document:\n        return None\n    \n    # Parse the webfinger document to extract the host-meta information\n    # Since we're dealing with Diaspora specifically, we need to extract the XRD content\n    # and generate the host-meta document using the template\n    try:\n        # Extract the webfinger response and parse it to get the necessary info\n        # For simplicity, assume the webfinger response contains the necessary link\n        # to the host-meta document\n        from federation.hostmeta.generators import generate_host_meta\n        \n        # Use the host as the webfinger_host to generate the host-meta\n        webfinger_host = host\n        hostmeta = generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)\n        return hostmeta\n    except Exception as e:\n        logger.error(\"Failed to retrieve or generate Diaspora host-meta: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger handle using the host\n    handle = f\"acct:{host}\"\n    \n    # Retrieve the webfinger document from the host\n    document = try_retrieve_webfinger_document(handle)\n    \n    if not document:\n        return None\n    \n    # Parse the webfinger document to extract the host-meta information\n    # Since we're dealing with Diaspora, we need to extract the XRD document\n    # from the webfinger response and return it as an XRD instance\n    try:\n        # Assuming the webfinger response contains an XRD document\n        # We would parse it here, but since the exact parsing logic is not provided,\n        # we return the document as an XRD instance (in practice, this would involve\n        # parsing the XML and creating an XRD object)\n        from federation.hostmeta.parsers import parse_xrd_document\n        xrd_instance = parse_xrd_document(document)\n        return xrd_instance\n    except Exception as e:\n        logger.error(\"Failed to parse XRD document: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger handle using the host\n    handle = f\"user@{host}\"\n    \n    # Retrieve the webfinger document from the host\n    document = try_retrieve_webfinger_document(handle)\n    \n    if not document:\n        return None\n    \n    # Parse the webfinger document to extract the webfinger_host\n    # In a real implementation, parsing the document would extract the relevant fields\n    # For now, we assume the webfinger_host can be derived from the host parameter\n    # or from the document structure (this would require actual parsing logic)\n    \n    # Since we don't have full parsing logic here, we'll use the host directly\n    # as the webfinger_host for generating the host-meta\n    webfinger_host = host\n    \n    # Generate the host-meta using the Diaspora template with webfinger_host\n    return generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.767255038022995, "generate_results": ["import requests\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "import requests\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "import requests\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "import requests\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "import requests\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.767255038022995, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Set the value for the first key\n    current[key] = val\n    # For each additional key in the chain, create nested dictionaries\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    return dic", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val\n    # For each subsequent key in the chain, create nested dictionaries\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    return dic", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Set the value for the first key\n    current[key] = val\n    # If there are additional keys, recursively set them\n    for k in keys:\n        if not isinstance(current, dict):\n            raise TypeError(\"Cannot insert into non-dictionary object\")\n        current = current.setdefault(k, {})\n    return dic", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Set the initial key\n    current[key] = val\n    # If there are additional keys, continue nesting\n    for k in keys:\n        if not isinstance(current, dict):\n            raise TypeError(\"Cannot insert into non-dict object\")\n        current = current.setdefault(k, {})", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val\n    # For each additional key in the chain, create nested dictionaries\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    return dic"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 956, "num_retrieved": 3, "inference_time": 0.767255038022995, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.inspector.inspector import IniType\n    \n    return IniType(\n        arg_name=spec_option['name'],\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('subcommand', ''),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.inspector.inspector import IniType\n    \n    return IniType(\n        arg_name=spec_option['name'],\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('subcommand', ''),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.inspector.inspector import IniType\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('subcommand', ''),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.inspector.inspector import IniType\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('subcommand', ''),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.inspector.inspector import IniType\n    \n    return IniType(\n        arg_name=spec_option['name'],\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('subcommand', ''),\n        spec_option=spec_option\n    )"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.767255038022995, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a class attribute that should be initialized once\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is stored as a class attribute, initialize it if not already\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a class attribute that should be initialized once\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a class attribute that should be initialized once\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a class attribute that should be initialized once\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 1004, "num_retrieved": 5, "inference_time": 0.767255038022995, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 997, "num_retrieved": 5, "inference_time": 0.767255038022995, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the class context and how the plugin manager is instantiated\n    # In a real scenario, this would return an instance of the PluginManager class\n    # based on the CoreServices or similar service registry\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the plugin manager instance is supposed to be.\n    # In a real scenario, this would return an instance of PluginManager.\n    from infrared.core import plugins_manager\n    return plugins_manager", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the class context and how the plugin manager is instantiated\n    # In a real scenario, this would return an instance of the PluginManager class\n    # based on the CoreServices or similar service registry\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual logic is not provided in the context\n    # In a real scenario, this would return an instance of the plugin manager\n    # For now, we return a mock implementation\n    from infrared.core import plugins_manager\n    return plugins_manager", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the plugin manager instance is supposed to be.\n    # In a real scenario, this would return an instance of PluginManager.\n    return CoreServices.plugins_manager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 893, "num_retrieved": 1, "inference_time": 0.767255038022995, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise cls.IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields that must be present in the spec\n    required_fields = [\n        'commands',\n        'description',\n        'name'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise cls.IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate that 'commands' is a list\n    if not isinstance(spec_data['commands'], list):\n        raise cls.IRValidatorException(\"'commands' must be a list\")\n\n    # Validate each command in commands list has required fields\n    for cmd in spec_data['commands']:\n        if not isinstance(cmd, dict):\n            raise cls.IRValidatorException(\"'commands' must contain dictionaries\")\n        \n        required_cmd_fields = ['name', 'options']\n        for field in required_cmd_fields:\n            if field not in cmd:\n                raise cls.IRValidatorException(f\"Missing required field in command '{cmd['name']}': {field}\")\n\n        # Validate options is a list of dictionaries\n        if not isinstance(cmd['options'], list):\n            raise cls.IRValidatorException(f\"Command '{cmd['name']}' must have 'options' as a list\")\n\n        for option in cmd['options']:\n            if not isinstance(option, dict):\n                raise cls.IRValidatorException(f\"Option in command '{cmd['name']}' must be a dictionary\")\n\n            required_option_fields = ['name']\n            for field in required_option_fields:\n                if field not in option:\n                    raise cls.IRValidatorException(f\"Missing required field in option '{option['name']}': {field}\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields (example - adjust based on actual spec structure)\n    required_fields = ['commands', 'options', 'description']  # Example required fields\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added here based on specific spec structure\n    # For example, validate commands, options, etc.\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields that must be present in the spec\n    required_fields = ['commands', 'description', 'version']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Validate that 'commands' is a list of dictionaries\n    if not isinstance(spec_data.get('commands'), list):\n        raise IRValidatorException(\"'commands' must be a list of command specifications\")\n    \n    # Validate each command has required fields\n    for cmd_idx, cmd in enumerate(spec_data['commands']):\n        cmd_required_fields = ['name', 'description', 'args']\n        for field in cmd_required_fields:\n            if field not in cmd:\n                raise IRValidatorException(f\"Command at index {cmd_idx} is missing required field: {field}\")\n        \n        # Validate args is a list of dictionaries\n        if not isinstance(cmd['args'], list):\n            raise IRValidatorException(f\"Command at index {cmd_idx} must have 'args' as a list of arguments\")\n        \n        # Validate each argument has required fields\n        for arg_idx, arg in enumerate(cmd['args']):\n            arg_required_fields = ['name', 'type']\n            for field in arg_required_fields:\n                if field not in arg:\n                    raise IRValidatorException(f\"Argument at index {arg_idx} in command {cmd['name']} is missing required field: {field}\")\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise cls.IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields that should be present in the spec\n    required_fields = ['commands', 'name', 'description']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise cls.IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Validate commands section\n    if 'commands' not in spec_data or not isinstance(spec_data['commands'], list):\n        raise cls.IRValidatorException(\"Commands section must be a non-empty list\")\n    \n    for cmd in spec_data['commands']:\n        if not isinstance(cmd, dict):\n            raise cls.IRValidatorException(\"Each command must be a dictionary\")\n        \n        cmd_name = cmd.get('name')\n        if not cmd_name:\n            raise cls.IRValidatorException(\"Command must have a 'name' field\")\n        \n        if 'options' not in cmd:\n            raise cls.IRValidatorException(f\"Command '{cmd_name}' is missing 'options' field\")\n        \n        if not isinstance(cmd['options'], list):\n            raise cls.IRValidatorException(f\"Command '{cmd_name}' options must be a list\")\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields (example - adjust based on actual spec structure)\n    required_fields = ['commands', 'description', 'version']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added based on specific spec structure\n    # For example, validate commands structure, options, etc.\n    \n    return spec_data"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 920, "num_retrieved": 2, "inference_time": 0.767255038022995, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Resolve the file path to absolute path\n    file_path = cls.resolve(yaml_file)\n\n    # Load the YAML file\n    with open(file_path, 'r') as f:\n        data = yaml.safe_load(f)\n\n    # Validate that the data is a dictionary and has required fields\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n\n    # Check for required fields (example - adjust based on actual requirements)\n    required_fields = ['required_field_1', 'required_field_2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Resolve the file path to absolute path\n    file_path = cls.resolve(yaml_file)\n\n    # Load the YAML file\n    with open(file_path, 'r') as f:\n        data = yaml.safe_load(f)\n\n    # Validate that the data is a dictionary and contains required fields\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n\n    # Check for mandatory fields (example - adjust based on actual requirements)\n    required_fields = ['required_field1', 'required_field2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise ValueError(\"yaml_file parameter is required\")\n\n    # Resolve the file path using the FileType class\n    file_path = FileType().resolve(yaml_file)\n\n    # Load the YAML file\n    try:\n        with open(file_path, 'r') as f:\n            data = yaml.safe_load(f)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Failed to load YAML file {file_path}: {str(e)}\")\n\n    # Validate that the file has required fields (example: check for top-level list or dict)\n    # This is a placeholder - actual validation logic should be defined based on required fields\n    if data is None:\n        raise exceptions.IRValidatorException(f\"YAML file {file_path} is empty or invalid\")\n\n    # Additional validation logic can be added here based on required fields\n    # For example:\n    # if not isinstance(data, (list, dict)):\n    #     raise exceptions.IRValidatorException(f\"YAML file {file_path} must contain a list or dict\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Resolve the file path to absolute path\n    file_path = cls.resolve(yaml_file)\n\n    # Load the YAML file\n    with open(file_path, 'r') as f:\n        data = yaml.safe_load(f)\n\n    # Validate that the data is a dictionary and contains required fields\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n\n    # Check for required fields (example - adjust based on actual requirements)\n    required_fields = ['required_field_1', 'required_field_2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise ValueError(\"yaml_file parameter is required\")\n\n    # Resolve the file path using the FileType class\n    file_path = FileType().resolve(yaml_file)\n\n    # Load the YAML file\n    try:\n        with open(file_path, 'r') as f:\n            data = yaml.safe_load(f)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Failed to load YAML file {file_path}: {str(e)}\")\n\n    # Validate that the file has required fields (example: check for top-level list or dict)\n    # This is a placeholder - actual validation logic depends on required fields\n    if data is None:\n        raise exceptions.IRValidatorException(f\"YAML file {file_path} is empty or invalid\")\n\n    # Validate structure (example: ensure it's a list or dict with required keys)\n    # Add specific validation rules based on required fields\n    if not isinstance(data, (list, dict)):\n        raise exceptions.IRValidatorException(f\"YAML file {file_path} must contain a list or dictionary\")\n\n    return data"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 985, "num_retrieved": 4, "inference_time": 0.7242447882890701, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group in include_groups:\n        if isinstance(group, dict) and 'name' in group:\n            group_name = group['name']\n            shared_groups.append(group_name)\n        elif isinstance(group, str):\n            shared_groups.append(group)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group in include_groups:\n        if isinstance(group, dict) and 'name' in group:\n            group_name = group['name']\n            shared_groups.append(group_name)\n        elif isinstance(group, str):\n            shared_groups.append(group)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group in include_groups:\n        if isinstance(group, dict) and 'name' in group:\n            group_name = group['name']\n            shared_groups.append(group_name)\n        elif isinstance(group, str):\n            shared_groups.append(group)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', {})\n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    \n    for group_name, group_config in include_groups.items():\n        if group_name in shared_groups:\n            continue\n            \n        # Add the group to shared groups if it's not already present\n        shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group in include_groups:\n        if isinstance(group, dict) and 'name' in group:\n            group_name = group['name']\n            shared_groups.append(group_name)\n        elif isinstance(group, str):\n            shared_groups.append(group)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 917, "num_retrieved": 3, "inference_time": 0.7242447882890701, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec if available\n    if self.spec_option and 'defaults' in self.spec_option:\n        defaults.update(self.spec_option['defaults'])\n    \n    # Get defaults from environment variables\n    env_vars = {}\n    for key, value in defaults.items():\n        if isinstance(value, str) and value.startswith('$'):\n            env_var_name = value[1:]\n            env_vars[env_var_name] = os.getenv(env_var_name)\n            if env_vars[env_var_name] is None:\n                env_vars[env_var_name] = value  # fallback to original value\n    \n    # Merge environment variable defaults into the result\n    defaults.update(env_vars)\n    \n    # Resolve file path arguments using auto propagation if applicable\n    if hasattr(self, 'files_path') and self.files_path:\n        file_names = ListOfFileNames()\n        for key, value in defaults.items():\n            if isinstance(value, str) and value.startswith('file:'):\n                file_path = value[5:]\n                if file_path:\n                    file_names.validate(file_path)\n                    defaults[key] = file_path\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec if available\n    if self.spec_option and 'defaults' in self.spec_option:\n        defaults.update(self.spec_option['defaults'])\n    \n    # Get defaults from plugin path and lookup_dir if available\n    if hasattr(self, 'plugin_path') and hasattr(self, 'lookup_dir'):\n        files_path = os.path.join(self.plugin_path, self.lookup_dir)\n        if os.path.exists(files_path):\n            files = os.listdir(files_path)\n            # Process file names and extract defaults if needed\n            for file in files:\n                file_name = os.path.splitext(file)[0]\n                # Add file name as default if specified in spec\n                if file_name in self.spec_option.get('defaults', {}):\n                    defaults[file_name] = self.spec_option['defaults'][file_name]\n    \n    # Resolve values using any auto-propagation logic from supported types\n    for key, value in defaults.items():\n        if hasattr(self, 'resolve'):\n            try:\n                resolved_value = self.resolve(value)\n                defaults[key] = resolved_value\n            except Exception as e:\n                logging.warning(f\"Failed to resolve value {value}: {e}\")\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec if available\n    if self.spec_option:\n        spec_defaults = self.spec_option.get('defaults', {})\n        defaults.update(spec_defaults)\n    \n    # Get defaults from plugin path and lookup_dir if available\n    if hasattr(self, 'plugin_path') and hasattr(self, 'lookup_dir'):\n        files_path = os.path.join(self.plugin_path, self.lookup_dir)\n        if os.path.exists(files_path):\n            files = [f for f in os.listdir(files_path) if os.path.isfile(os.path.join(files_path, f))]\n            # Process file names if needed\n            for file in files:\n                file_name = os.path.splitext(file)[0]\n                if file_name not in defaults:\n                    defaults[file_name] = None\n    \n    # Resolve any file name arguments using ListOfFileNames logic\n    if hasattr(self, 'file_names'):\n        file_names = self.file_names\n        if isinstance(file_names, str):\n            file_names = file_names.split(',')\n        for file_name in file_names:\n            if file_name not in defaults:\n                try:\n                    # Validate the file name exists\n                    self.validate(file_name)\n                    defaults[file_name] = None\n                except exceptions.IRFileNotFoundException:\n                    pass\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec if available\n    if self.spec_option:\n        spec_defaults = self.spec_option.get('defaults', {})\n        defaults.update(spec_defaults)\n    \n    # Get defaults from plugin path and lookup_dir if available\n    if hasattr(self, 'plugin_path') and hasattr(self, 'lookup_dir'):\n        files_path = os.path.join(self.plugin_path, self.lookup_dir)\n        if os.path.exists(files_path):\n            files = os.listdir(files_path)\n            # Process file names if needed\n            for file in files:\n                file_name = os.path.splitext(file)[0]\n                if file_name not in defaults:\n                    defaults[file_name] = None\n    \n    # Resolve any values using validation logic\n    for key, value in defaults.items():\n        if isinstance(value, str) and self.ARG_SEPARATOR:\n            try:\n                defaults[key] = value.split(self.ARG_SEPARATOR)\n            except Exception:\n                pass\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec if available\n    if self.spec_option:\n        spec_defaults = self.spec_option.get('defaults', {})\n        defaults.update(spec_defaults)\n    \n    # Get defaults from environment variables\n    env_vars = self.env_vars or {}\n    for key, value in env_vars.items():\n        if key not in defaults:\n            defaults[key] = value\n    \n    # Get defaults from configuration files or settings\n    if self.settings:\n        for key, value in self.settings.items():\n            if key not in defaults:\n                defaults[key] = value\n    \n    # Resolve file path arguments using auto propagation if applicable\n    if hasattr(self, 'files_path') and self.files_path:\n        file_args = []\n        for key in defaults.keys():\n            if key.endswith('_file') or key.endswith('_path'):\n                file_args.append(key)\n        \n        for arg in file_args:\n            if arg in defaults:\n                # Validate and resolve file path\n                if hasattr(self, 'lookup_dir') and self.lookup_dir:\n                    path = os.path.join(self.plugin_path, self.lookup_dir)\n                    if os.path.exists(path):\n                        file_names = [f for f in os.listdir(path) if f.endswith('.yaml') or f.endswith('.yml')]\n                        defaults[arg] = file_names[0] if file_names else None\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.7242447882890701, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 848, "num_retrieved": 1, "inference_time": 0.7242447882890701, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 926, "num_retrieved": 2, "inference_time": 0.7242447882890701, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    # Assuming options are stored in a structure accessible via spec_helper\n    # This is a placeholder implementation based on context and typical pattern\n    options_spec = []\n    # Retrieve options from spec_helper for the given command_name\n    # In real implementation, this would query the actual spec storage\n    return options_spec", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    # Assuming options are stored in a spec structure accessible via self.spec_helper\n    # This is a placeholder implementation based on context and typical pattern\n    return self.spec_helper.get_command_options(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    # Assuming options are stored in a spec structure accessible via self.spec_helper\n    # This is a placeholder implementation based on context and typical pattern\n    return self.spec_helper.get_command_options(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    # Assuming options are stored in a spec structure accessible via self.spec_helper\n    # This is a placeholder implementation based on context and typical pattern\n    return self.spec_helper.get_command_options(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    # Assuming options are stored in a spec structure that can be accessed via command_name\n    # This is a placeholder implementation based on typical inspector pattern\n    # In real implementation, this would retrieve from a configuration or spec store\n    return self.spec_helper.get_command_options(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 925, "num_retrieved": 2, "inference_time": 0.7242447882890701, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    spec_options = self.spec_helper.get_parser_option_specs(command_name)\n    for spec_option in spec_options:\n        if spec_option['name'] == argument_name:\n            return spec_option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    spec_options = self.spec_helper.get_parser_option_specs(command_name)\n    for spec_option in spec_options:\n        if spec_option['name'] == argument_name:\n            return spec_option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    spec_options = self.spec_helper.get_parser_option_specs(command_name)\n    for spec_option in spec_options:\n        if spec_option['name'] == argument_name:\n            return spec_option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    spec_options = self.spec_helper.get_parser_option_specs(command_name)\n    for spec_option in spec_options:\n        if spec_option['name'] == argument_name:\n            return spec_option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    spec_options = self.spec_helper.get_parser_option_specs(command_name)\n    for spec_option in spec_options:\n        if spec_option['name'] == argument_name:\n            return spec_option\n    return None"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 950, "num_retrieved": 3, "inference_time": 0.7242447882890701, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    # Assuming silent arguments are those that start with 'silent' or are explicitly marked\n    # This is a placeholder implementation based on common patterns\n    for arg in args:\n        if arg.startswith('silent'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    # Assuming silent arguments are those that start with 'silent' or are explicitly marked\n    # This is a placeholder implementation based on typical behavior\n    for arg in args:\n        if arg.startswith('silent'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    # Assuming silent arguments are those that start with 'silent' or are explicitly marked\n    # This is a placeholder implementation based on typical behavior\n    for arg in args:\n        if arg.startswith('silent'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    # Assuming silent arguments are those that start with 'silent' or are explicitly marked\n    # This is a placeholder implementation based on typical behavior\n    for arg in args:\n        if arg.startswith('silent'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    # Assuming silent arguments are those that start with 'silent' or are explicitly marked\n    # This is a placeholder implementation based on typical behavior\n    for arg in args:\n        if arg.startswith('silent'):\n            silent_args.append(arg)\n    return silent_args"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 982, "num_retrieved": 4, "inference_time": 0.7242447882890701, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 952, "num_retrieved": 3, "inference_time": 0.7242447882890701, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    required_args = []\n    for option in options_spec:\n        if 'required_when' in option:\n            condition = option['required_when']\n            # Check if the condition is met based on the provided args\n            condition_met = False\n            for arg_name, arg_value in args.items():\n                if arg_name in condition:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option['name'])\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    required_args = []\n    for option in options_spec:\n        if 'required_when' in option and option['required_when']:\n            # Extract the condition from required_when\n            condition = option['required_when']\n            # Check if the condition is met based on args\n            condition_met = False\n            for arg_name, arg_value in args.items():\n                if arg_name in condition:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option['name'])\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    required_args = []\n    for option in options_spec:\n        if 'required_when' in option:\n            condition = option['required_when']\n            # Check if the condition is met based on the provided args\n            condition_met = False\n            for arg_name, arg_value in args.items():\n                if arg_name in condition:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option['name'])\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    required_args = []\n    for option in options_spec:\n        if 'required_when' in option:\n            condition = option['required_when']\n            # Check if the condition is met based on the provided args\n            condition_met = False\n            for arg_name, arg_value in args.items():\n                if arg_name in condition:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option['name'])\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    required_args = []\n    for option in options_spec:\n        if 'required_when' in option:\n            condition = option['required_when']\n            # Check if the condition is met based on the provided args\n            condition_met = False\n            for arg_name, arg_value in args.items():\n                if arg_name in condition:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option['name'])\n    return required_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 991, "num_retrieved": 4, "inference_time": 0.7242447882890701, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length specification is provided in a nested structure or via a default\n            # We need to determine the expected length from context, but since it's not provided,\n            # we'll assume the length is specified in the args structure or via a convention\n            # For now, we'll just validate if the string is too long based on a hypothetical length\n            # This is a placeholder - actual implementation would require knowing the expected length\n            pass\n    \n    # Since the expected length is not provided in the context, we cannot fully implement\n    # the validation without additional information. The function would typically check\n    # against a specified length for each argument.\n    \n    # Placeholder implementation - in real use, lengths would be defined elsewhere\n    # For example, if lengths were stored in a config or passed in, we'd compare:\n    # if len(arg_value) > expected_length:\n    #     invalid_options.append((arg_name, arg_value, expected_length))\n    \n    # If no invalid options found, return\n    if not invalid_options:\n        return\n    \n    # Raise exception if invalid options found\n    raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length specification is provided in a nested structure or via a default\n            # Since no specific length is defined in the context, we'll assume it's stored in args\n            # or passed via a convention. We'll check if length is specified in args\n            # For example, if args has a key like 'max_length' for each argument\n            # But since no such structure is defined, we'll assume a default length or skip\n            # This is a placeholder implementation based on context\n            pass\n    \n    # Based on the context, we see IRInvalidLengthException is used for invalid lengths\n    # and _validate_args checks for required arguments\n    # We'll implement a generic validation that checks string lengths against a specified limit\n    # If length is not specified, we skip or raise an error\n    \n    # Since no specific length is provided in the function signature, we assume\n    # that length information is stored in args or passed separately\n    # We'll use a default length of 256 as a placeholder\n    \n    max_length = 256\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str) and len(arg_value) > max_length:\n            invalid_options.append((arg_name, arg_value, max_length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length specification is provided in a nested structure or via a default\n            # We need to determine the expected length from context, but since it's not provided,\n            # we'll assume that length is specified in the args structure or via a convention\n            # For now, we'll just check if the value is too long based on a hypothetical length\n            # This is a placeholder implementation - actual length validation would require\n            # knowing the expected maximum length for each argument\n            pass\n    \n    # Since the context does not specify how lengths are defined or where they are stored,\n    # we cannot fully implement this function without additional information.\n    # The following is a minimal implementation that would need to be adapted based on actual requirements.\n    \n    # Example: if lengths were stored in a separate dictionary or in the args structure\n    # For instance, if args had a structure like {'arg1': {'value': '...', 'max_length': 10}}\n    # We would iterate over such entries and validate.\n    \n    # Placeholder implementation - this would need to be adjusted based on actual data structure\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assume max_length is stored in a separate key, e.g., 'max_length' or in a nested dict\n            # This is a simplified version - actual implementation would depend on input structure\n            max_length = getattr(self, 'max_length', 100)  # Default max length\n            if len(arg_value) > max_length:\n                invalid_options.append((arg_name, arg_value, max_length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length is specified in the args structure, e.g., args[arg_name]['length']\n            length_spec = args.get(arg_name, {}).get('length')\n            if length_spec is not None:\n                if len(arg_value) > length_spec:\n                    invalid_options.append((arg_name, arg_value, length_spec))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length is specified in the args structure, e.g., args[arg_name]['length']\n            length_spec = args.get(arg_name, {}).get('length')\n            if length_spec is not None:\n                if len(arg_value) > length_spec:\n                    invalid_options.append((arg_name, arg_value, length_spec))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 987, "num_retrieved": 4, "inference_time": 0.7242447882890701, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_specs in self.get_option_specs().items():\n        for option_name, option_spec in option_specs.items():\n            if option_spec.get('choices'):\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_spec['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if option_data.get('choices'):\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Assuming the available choices are stored in a dictionary within args\n    # with keys as command names and option names, and values as tuples of (option_name, value, available_choices)\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Check if this option has a defined choice list\n            # This would typically come from a configuration or option specification\n            # For this implementation, we assume available_choices is stored in a predefined structure\n            # or passed as part of the option definition\n            # Since no specific structure is provided in context, we'll simulate based on typical use\n            # In a real implementation, available_choices would be retrieved from option specs\n            \n            # Example: if available_choices were stored in a dict like:\n            # option_specs = {'option_name': {'choices': ['choice1', 'choice2']}}\n            # We would check if option_value is in choices\n            \n            # Placeholder: simulate available_choices lookup\n            # In actual implementation, this would come from a configuration or option spec\n            available_choices = self.get_option_spec(command_name, option_name)\n            if available_choices and 'choices' in available_choices:\n                if option_value not in available_choices['choices']:\n                    invalid_options.append((option_name, option_value, available_choices['choices']))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if option_data.get('choices'):\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_specs in self.get_option_specs().items():\n        for option_name, option_spec in option_specs.items():\n            if option_spec.get('choices'):\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_spec['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 988, "num_retrieved": 4, "inference_time": 0.7242447882890701, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, (int, float)):\n            min_val = self.module.params.get(f'{key}_min')\n            max_val = self.module.params.get(f'{key}_max')\n            \n            if min_val is not None and max_val is not None:\n                min_val = float(min_val)\n                max_val = float(max_val)\n                value = float(value)\n                \n                if value < min_val or value > max_val:\n                    raise UtilError(f\"Value of '{key}' ({value}) is not between min ({min_val}) and max ({max_val})\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, (int, float)):\n            min_val = self.module.params.get(f'{key}_min')\n            max_val = self.module.params.get(f'{key}_max')\n            \n            if min_val is not None and max_val is not None:\n                min_val = float(min_val)\n                max_val = float(max_val)\n                value = float(value)\n                \n                if not (min_val <= value <= max_val):\n                    raise UtilError(f\"Value of '{key}' ({value}) is not between min ({min_val}) and max ({max_val})\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, (int, float)):\n            min_val = self.module.params.get(f'{key}_min')\n            max_val = self.module.params.get(f'{key}_max')\n            \n            if min_val is not None and max_val is not None:\n                min_val = float(min_val)\n                max_val = float(max_val)\n                value = float(value)\n                \n                if value < min_val or value > max_val:\n                    raise UtilError(f\"Value of {key} ({value}) is not between min ({min_val}) and max ({max_val})\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, (int, float)):\n            min_val = self.module.params.get(f'{key}_min')\n            max_val = self.module.params.get(f'{key}_max')\n            \n            if min_val is not None and max_val is not None:\n                min_val = float(min_val)\n                max_val = float(max_val)\n                value = float(value)\n                \n                if value < min_val or value > max_val:\n                    raise UtilError(f\"Value of '{key}' ({value}) is not between min ({min_val}) and max ({max_val})\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, (int, float)):\n            min_val = self.module.params.get(f'{key}_min')\n            max_val = self.module.params.get(f'{key}_max')\n            \n            if min_val is not None and max_val is not None:\n                min_val = float(min_val)\n                max_val = float(max_val)\n                value = float(value)\n                \n                if value < min_val or value > max_val:\n                    raise UtilError(f\"Value of '{key}' ({value}) is not between min ({min_val}) and max ({max_val})\")"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 927, "num_retrieved": 3, "inference_time": 0.7242447882890701, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    return ComplexType(arg_name=option_name,\n                       settings_dirs=None,\n                       sub_command_name=subcommand,\n                       spec_option=spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    return ComplexType(arg_name=option_name,\n                       settings_dirs=self.settings_dirs,\n                       sub_command_name=subcommand,\n                       spec_option=spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    return ComplexType(arg_name=option_name,\n                       settings_dirs=None,\n                       sub_command_name=subcommand,\n                       spec_option=spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    return ComplexType(arg_name=option_name,\n                       settings_dirs=None,\n                       sub_command_name=subcommand,\n                       spec_option=spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    return ComplexType(arg_name=option_name,\n                       settings_dirs=None,\n                       sub_command_name=subcommand,\n                       spec_option=spec_option)"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.7242447882890701, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith(\"--additional-args\"):\n            # Parse additional-args format: option1=value1;option2=value2\n            value = arg[len(\"--additional-args\"):].strip()\n            if value:\n                additional_args = value.split(';')\n                for additional_arg in additional_args:\n                    additional_arg = additional_arg.strip()\n                    if '=' in additional_arg:\n                        name, value = additional_arg.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[additional_arg] = None\n        else:\n            # Default to control arguments if not additional-args\n            # Split on '=' to handle key=value format\n            if '=' in arg:\n                name, value = arg.split('=', 1)\n                control_args[name] = value\n            else:\n                control_args[arg] = None\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            value = arg.split('=', 1)[1] if '=' in arg else ''\n            if value:\n                additional_args = value.split(';')\n                for additional_arg in additional_args:\n                    if '=' in additional_arg:\n                        name, value = additional_arg.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[additional_arg] = True\n        else:\n            # Default to control arguments if not matching additional-args\n            # Split on '=' to handle key=value format\n            if '=' in arg:\n                name, value = arg.split('=', 1)\n                control_args[name] = value\n            else:\n                control_args[arg] = True\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            value = arg.split('=', 1)[1] if '=' in arg else ''\n            if value:\n                additional_args = value.split(';')\n                for additional_arg in additional_args:\n                    if '=' in additional_arg:\n                        name, value = additional_arg.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[additional_arg] = None\n        else:\n            # Default to control arguments if not matching additional-args\n            # Split on first '=' to handle key=value format\n            if '=' in arg:\n                name, value = arg.split('=', 1)\n                control_args[name] = value\n            else:\n                control_args[arg] = None\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith(\"--additional-args\"):\n            # Parse additional-args format: option1=value1;option2=value2\n            value = arg[len(\"--additional-args\"):].strip()\n            if value:\n                additional_args = value.split(';')\n                for additional_arg in additional_args:\n                    additional_arg = additional_arg.strip()\n                    if '=' in additional_arg:\n                        name, value = additional_arg.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[additional_arg] = None\n        else:\n            # Assume all other arguments are control arguments\n            # Split on '=' to handle key=value format\n            if '=' in arg:\n                name, value = arg.split('=', 1)\n                control_args[name] = value\n            else:\n                control_args[arg] = None\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith(\"--additional-args\"):\n            # Parse additional-args format: option1=value1;option2=value2\n            value = arg[len(\"--additional-args\"):].strip()\n            if value:\n                additional_args = value.split(';')\n                for additional_arg in additional_args:\n                    additional_arg = additional_arg.strip()\n                    if '=' in additional_arg:\n                        name, value = additional_arg.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[additional_arg] = None\n        else:\n            # Default to control arguments if not additional-args\n            # Split on '=' to handle both key=value and key cases\n            if '=' in arg:\n                name, value = arg.split('=', 1)\n                control_args[name] = value\n            else:\n                control_args[arg] = None\n    \n    return control_args, nested_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 915, "num_retrieved": 2, "inference_time": 0.7242447882890701, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n\n    result = vars_dict.copy()\n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            result.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                result[key] = value\n            except ValueError:\n                # Invalid format, skip\n                pass\n    return result", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if '=' in extra_var:\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key (no value)\n            vars_dict[extra_var] = True\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    result = vars_dict.copy()\n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            result.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                result[key] = value\n            except ValueError:\n                # Invalid format, skip\n                pass\n    return result", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            vars_dict[extra_var] = True\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    result = vars_dict.copy()\n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            result[key] = value\n        else:\n            # Handle case where extra_var is just a key (no value)\n            result[extra_var] = True\n    \n    return result"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 885, "num_retrieved": 1, "inference_time": 0.7242447882890701, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Prepare the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(str(extra_vars))])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend(['--{}'.format(key), str(value)])\n    \n    # Set up environment and execute\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Ensure workspace is active\n    workspace_manager = CoreServices.workspace_manager()\n    active_workspace = workspace_manager.get_active_workspace()\n    if not active_workspace:\n        active_workspace = workspace_manager.create()\n        workspace_manager.activate(active_workspace.name)\n        LOG.warning(\"There are no workspaces. New workspace added: %s\", active_workspace.name)\n\n    # Parse plugin specification\n    if ir_plugin.specification is None:\n        raise Exception(\"Unable to create specification for '{}' plugin. Check plugin config and settings folders\".format(ir_plugin.name))\n    \n    # Parse arguments from plugin specification\n    parsed_args = ir_plugin.specification.parse_args(None, {})\n    if parsed_args is None:\n        return None\n\n    nested_args, control_args, custom_args = parsed_args\n\n    # Set debug level if requested\n    if control_args.get('debug', None):\n        logger.LOG.setLevel(logging.DEBUG)\n\n    # Generate vars dict from workspace and extra_vars\n    vars_dict = VarsDictManager.generate_settings(\n        ir_workspace=ir_workspace,\n        extra_vars=extra_vars,\n        control_args=control_args,\n        custom_args=custom_args\n    )\n\n    # Prepare ansible arguments\n    ansible_cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', str(extra_vars)])\n    \n    # Add custom ansible arguments\n    if ansible_args:\n        for key, value in ansible_args.items():\n            ansible_cmd.extend(['--' + key, str(value)])\n\n    # Execute ansible playbook\n    import subprocess\n    try:\n        result = subprocess.run(ansible_cmd, capture_output=True, text=True)\n        if result.returncode != 0:", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Prepare the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(str(extra_vars))])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend(['--{}'.format(key), str(value)])\n    \n    # Set up environment and execute\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero if an error occurs", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Prepare the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars', str(extra_vars)])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend(['--' + key, str(value)])\n    \n    # Set up workspace and plugin context\n    workspace_manager = CoreServices.workspace_manager()\n    active_workspace = workspace_manager.get_active_workspace()\n    if not active_workspace:\n        active_workspace = workspace_manager.create()\n        workspace_manager.activate(active_workspace.name)\n        LOG.warning(\"There are no workspaces. New workspace added: %s\", active_workspace.name)\n    \n    # Parse arguments using the plugin's specification\n    if ir_plugin.specification is None:\n        raise Exception(\"Unable to create specification for '{}' plugin. Check plugin config and settings folders\".format(ir_plugin.name))\n    \n    parsed_args = ir_plugin.specification.parse_args(None, None)\n    if parsed_args is None:\n        return None\n    \n    nested_args, control_args, custom_args = parsed_args\n    \n    # Set debug level if requested\n    if control_args.get('debug', None):\n        logger.LOG.setLevel(logging.DEBUG)\n    \n    # Generate vars dict from workspace and plugin settings\n    vars_dict = VarsDictManager.generate_settings(\n        nested_args, control_args, custom_args\n    )\n    \n    # Add vars to ansible command if vars_dict is not empty\n    if vars_dict:\n        vars_str = ' '.join(['--extra-vars',", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Ensure the workspace is active\n    workspace_manager = CoreServices.workspace_manager()\n    active_workspace = workspace_manager.get_active_workspace()\n    if not active_workspace:\n        active_workspace = workspace_manager.create()\n        workspace_manager.activate(active_workspace.name)\n        LOG.warning(\"There are no workspaces. New workspace added: %s\", active_workspace.name)\n    \n    # Prepare the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars', ' '.join(f'{k}={v}' for k, v in extra_vars.items())])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend(['--' + key, str(value)])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True, check=False)\n        if result.returncode == 0:\n            return 0\n        else:\n            LOG.error(\"Ansible playbook failed with exit code %d: %s\", result.returncode, result.stderr)\n            return result.returncode\n    except Exception as e:\n        LOG.error(\"Error executing ansible-playbook: %s\", str(e))\n        return 1"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.9107822974522909, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    import subprocess\n    import json\n\n    # Prepare the ansible command\n    ansible_cmd = ['ansible', 'playbook']\n    \n    # Add CLI arguments\n    ansible_cmd.extend(cli_args)\n    \n    # Add extra vars if vars_dict is not empty\n    if vars_dict:\n        vars_json = json.dumps(vars_dict)\n        ansible_cmd.extend(['-e', vars_json])\n    \n    # Execute ansible command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result\n    except subprocess.CalledProcessError as e:\n        return e", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    import subprocess\n    import json\n\n    # Prepare the ansible command\n    ansible_cmd = ['ansible', 'playbook']\n    \n    # Add CLI arguments\n    ansible_cmd.extend(cli_args)\n    \n    # Add extra vars if vars_dict is not empty\n    if vars_dict:\n        vars_json = json.dumps(vars_dict)\n        ansible_cmd.extend(['-e', vars_json])\n    \n    # Execute ansible command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result\n    except subprocess.CalledProcessError as e:\n        return e", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    import subprocess\n    import json\n\n    # Prepare the ansible command\n    ansible_cmd = ['ansible', 'playbook']\n    \n    # Add CLI arguments\n    ansible_cmd.extend(cli_args)\n    \n    # Add extra vars if vars_dict is not empty\n    if vars_dict:\n        vars_json = json.dumps(vars_dict)\n        ansible_cmd.extend(['-e', vars_json])\n    \n    # Execute ansible command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result\n    except subprocess.CalledProcessError as e:\n        return e", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    import subprocess\n    import json\n\n    # Prepare the ansible command\n    ansible_cmd = ['ansible', 'playbook']\n    \n    # Add CLI arguments\n    ansible_cmd.extend(cli_args)\n    \n    # Add extra vars if vars_dict is not empty\n    if vars_dict:\n        vars_json = json.dumps(vars_dict)\n        ansible_cmd.extend(['-e', vars_json])\n    \n    # Execute ansible command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result\n    except subprocess.CalledProcessError as e:\n        return e", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    import subprocess\n    import json\n\n    # Prepare the ansible command\n    ansible_cmd = ['ansible', 'playbook']\n    \n    # Add CLI arguments\n    ansible_cmd.extend(cli_args)\n    \n    # Add extra vars if vars_dict is not empty\n    if vars_dict:\n        vars_json = json.dumps(vars_dict)\n        ansible_cmd.extend(['-e', vars_json])\n    \n    # Execute ansible command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result\n    except subprocess.CalledProcessError as e:\n        return e"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 928, "num_retrieved": 3, "inference_time": 0.9107822974522909, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.type in ['int', 'float', 'bool']:\n            if option.type == 'int':\n                values_dict[option.name] = int(values_dict[option.name])\n            elif option.type == 'float':\n                values_dict[option.name] = float(values_dict[option.name])\n            elif option.type == 'bool':\n                values_dict[option.name] = values_dict[option.name].lower() in ['true', '1', 'yes']\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.type in ['int', 'float']:\n            try:\n                values_dict[option.name] = option.type(values_dict[option.name])\n            except (ValueError, TypeError):\n                pass", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.type in ['int', 'float']:\n            try:\n                values_dict[option.name] = option.type(values_dict[option.name])\n            except (ValueError, TypeError):\n                pass", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.type in ['int', 'float']:\n            try:\n                values_dict[option.name] = option.type(values_dict[option.name])\n            except (ValueError, TypeError):\n                pass", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.type in ['int', 'float', 'bool']:\n            if option.type == 'int':\n                values_dict[option.name] = int(values_dict[option.name])\n            elif option.type == 'float':\n                values_dict[option.name] = float(values_dict[option.name])\n            elif option.type == 'bool':\n                values_dict[option.name] = values_dict[option.name].lower() in ['true', '1', 'yes', 'on']\n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 991, "num_retrieved": 4, "inference_time": 0.9107822974522909, "generate_results": ["import os\nimport yaml\nfrom collections import OrderedDict\nimport dict_utils\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(plugin_spec_file):\n        return {}\n    \n    with open(plugin_spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\nimport dict_utils\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(plugin_spec_file):\n        return {}\n    \n    with open(plugin_spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\nimport dict_utils\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(plugin_spec_file):\n        return {}\n    \n    with open(plugin_spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\nimport dict_utils\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(plugin_spec_file):\n        return {}\n    \n    with open(plugin_spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\nimport dict_utils\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(plugin_spec_file):\n        return {}\n    \n    with open(plugin_spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flattened = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flattened[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flattened[key] = value\n    \n    return flattened"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 892, "num_retrieved": 1, "inference_time": 0.9107822974522909, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    import os\n    config_path = os.environ.get('INFRARED_CONFIG')\n    if config_path is None:\n        # Set the environment variable for config path\n        os.environ['INFRARED_CONFIG'] = self.config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    import os\n    config_path = os.environ.get('INFRARED_CONFIG')\n    if config_path is None:\n        # Assuming the config path is derived from core_settings or a default\n        # This is a placeholder - actual implementation may vary based on context\n        # For example, it might use self.core_settings.config_path or similar\n        config_path = self.core_settings.config_path if hasattr(self, 'core_settings') and self.core_settings else None\n        if config_path:\n            os.environ['INFRARED_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    import os\n    config_path = os.environ.get('INFRARED_CONFIG')\n    if config_path is None:\n        # Set the environment variable for config path\n        os.environ['INFRARED_CONFIG'] = self.config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    import os\n    config_path = os.environ.get('INFRARED_CONFIG')\n    if config_path is None:\n        # Assuming the config path should be set to a default or from some configuration\n        # This is a placeholder - actual implementation may vary based on context\n        os.environ['INFRARED_CONFIG'] = self.config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    import os\n    config_path = os.environ.get('INFRARED_CONFIG')\n    if config_path is None:\n        # Set the environment variable for config path\n        os.environ['INFRARED_CONFIG'] = self.config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 948, "num_retrieved": 3, "inference_time": 0.9107822974522909, "generate_results": ["def extend_cli(self, subparser):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a parser for the spec\n    parser = self.parser\n    # Add the spec options to the subparser\n    parser.add_argument('--spec', help='Specify the spec to run')\n    # Add any additional arguments defined in the spec dictionary\n    for arg_name, arg_info in self.spec_helper.get_args().items():\n        if arg_info['type'] == 'str':\n            subparser.add_argument('--' + arg_name, help=arg_info['help'])\n        elif arg_info['type'] == 'bool':\n            subparser.add_argument('--' + arg_name, action='store_true', help=arg_info['help'])\n        elif arg_info['type'] == 'int':\n            subparser.add_argument('--' + arg_name, type=int, help=arg_info['help'])\n        elif arg_info['type'] == 'choice':\n            subparser.add_argument('--' + arg_name, choices=arg_info['choices'], help=arg_info['help'])", "def extend_cli(self, subparser):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a parser for the spec\n    spec_parser = self.parser\n    # Add the spec-specific arguments to the subparser\n    spec_parser.add_argument(\n        '--spec-name',\n        help='Name of the spec to execute',\n        required=True\n    )\n    # Add any additional arguments defined in the spec dictionary\n    for arg_name, arg_info in self.spec_helper.spec_dict.items():\n        if arg_info.get('type') in ['str', 'int', 'bool', 'float']:\n            arg_type = arg_info.get('type')\n            help_text = arg_info.get('help', '')\n            default = arg_info.get('default')\n            if arg_type == 'str':\n                subparser.add_argument(\n                    '--' + arg_name,\n                    help=help_text,\n                    default=default,\n                    type=str\n                )\n            elif arg_type == 'int':\n                subparser.add_argument(\n                    '--' + arg_name,\n                    help=help_text,\n                    default=default,\n                    type=int\n                )\n            elif arg_type == 'bool':\n                subparser.add_argument(\n                    '--' + arg_name,\n                    help=help_text,\n                    default=default,\n                    type=bool\n                )\n            elif arg_type == 'float':\n                subparser.add_argument(\n                    '--' + arg_name,\n                    help=help_text,\n                    default=default,\n                    type=float\n                )\n        elif arg_info.get('type') == 'list':\n            subparser.add_argument(\n                '--' + arg_name,\n                help=help_text,\n                default=default,\n                type=str,\n                action='append'\n            )\n    return subparser", "def extend_cli(self, subparser):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This method should extend the provided subparser with CLI options\n    # based on the spec definition. Since the actual implementation details\n    # are not provided in the context, this is a placeholder that follows\n    # the expected pattern.\n    pass", "def extend_cli(self, subparser):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a parser for the spec\n    parser = self.parser\n    # Add the spec options to the subparser\n    parser.add_argument('--spec', help='Specify the spec to run')\n    # Add any additional arguments defined in the spec dictionary\n    for arg_name, arg_info in self.spec_helper.get_args().items():\n        if arg_info['type'] == 'str':\n            subparser.add_argument('--' + arg_name, help=arg_info['help'])\n        elif arg_info['type'] == 'bool':\n            subparser.add_argument('--' + arg_name, action='store_true', help=arg_info['help'])\n        elif arg_info['type'] == 'int':\n            subparser.add_argument('--' + arg_name, type=int, help=arg_info['help'])\n        elif arg_info['type'] == 'choice':\n            choices = arg_info['choices']\n            subparser.add_argument('--' + arg_name, choices=choices, help=arg_info['help'])", "def extend_cli(self, subparser):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a parser for the spec\n    spec_parser = self.parser\n    spec_parser.add_argument(\n        '--spec',\n        help='Specify the spec to run',\n        required=True\n    )\n    # Add other options as defined in the spec dictionary\n    for option in self.spec_helper.get_options():\n        option_name = option['name']\n        option_type = option.get('type', str)\n        option_help = option.get('help', '')\n        if option.get('action') == 'store_true':\n            subparser.add_argument(\n                '--' + option_name,\n                action='store_true',\n                help=option_help\n            )\n        else:\n            subparser.add_argument(\n                '--' + option_name,\n                type=option_type,\n                help=option_help\n            )"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 1024, "actual_tokens": 972, "num_retrieved": 5, "inference_time": 0.9107822974522909, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"Remove root from path, throw exception on failure.\"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"Remove root from path, throw exception on failure.\"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"Remove root from path, throw exception on failure.\"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"Remove root from path, throw exception on failure.\"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]"]}
