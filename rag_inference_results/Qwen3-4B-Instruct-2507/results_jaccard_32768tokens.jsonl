{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38625, "num_retrieved": 194, "inference_time": 4.4492135643959045, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37830, "num_retrieved": 169, "inference_time": 4.4492135643959045, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: the timedelta value to dehydrate\n    :type value: timedelta\n    :returns: a value of type understood by packstream\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: the timedelta value to dehydrate\n    :type value: timedelta\n    :returns: a value of type understood by packstream\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: the timedelta value to dehydrate\n    :type value: timedelta\n    :returns: a value of type understood by packstream\n    \"\"\"\n    nanoseconds = int(value.microseconds * 1000)\n    return Structure(b\"t\", value.days, value.seconds, nanoseconds)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38503, "num_retrieved": 192, "inference_time": 4.4492135643959045, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return Structure(b\"T\", value.srid, *value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return Structure(b\"T\", value.srid, *value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return Structure(b\"T\", value.srid, *value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.srid, *value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return Structure(b\"T\", value.srid, *value)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37770, "num_retrieved": 165, "inference_time": 4.4492135643959045, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 40220, "num_retrieved": 245, "inference_time": 4.4492135643959045, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34919, "num_retrieved": 74, "inference_time": 4.4492135643959045, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Bolt 3 handlers\n    handlers[(3, 5)] = Bolt3\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Bolt 3 handlers\n    handlers[(3, 5)] = Bolt3\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Bolt 3 handlers\n    handlers[(3, 5)] = Bolt3\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Bolt 3 handlers\n    handlers[(3, 5)] = Bolt3\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Bolt 3 handlers\n    handlers[(3, 5)] = Bolt3\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34197, "num_retrieved": 51, "inference_time": 4.4492135643959045, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\n    \n    For example, a timeout may be applied:\n    \n    from neo4j import unit_of_work\n    \n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n    \n    :param metadata: a dictionary with metadata. Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures. It will also get logged to the ``query.log``. This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n    \n    :param timeout: the transaction timeout in seconds. Transactions that execute longer than the configured timeout will be terminated by the database. This functionality allows to limit query/transaction execution time. Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting. Value should not represent a negative duration. A zero duration will make the transaction execute indefinitely. None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\n    \n    For example, a timeout may be applied:\n    \n    from neo4j import unit_of_work\n    \n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n    \n    :param metadata: a dictionary with metadata. Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures. It will also get logged to the ``query.log``. This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n    \n    :param timeout: the transaction timeout in seconds. Transactions that execute longer than the configured timeout will be terminated by the database. This functionality allows to limit query/transaction execution time. Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting. Value should not represent a negative duration. A zero duration will make the transaction execute indefinitely. None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\n    \n    For example, a timeout may be applied:\n    \n    from neo4j import unit_of_work\n    \n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n    \n    :param metadata: a dictionary with metadata. Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures. It will also get logged to the ``query.log``. This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n    \n    :param timeout: the transaction timeout in seconds. Transactions that execute longer than the configured timeout will be terminated by the database. This functionality allows to limit query/transaction execution time. Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting. Value should not represent a negative duration. A zero duration will make the transaction execute indefinitely. None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n        from neo4j import unit_of_work\n\n        @unit_of_work(timeout=100)\n        def count_people_tx(tx):\n            result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n            record = result.single()\n            return record[\"persons\"]\n\n    :param metadata:\n        a dictionary with metadata.\n        Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n        It will also get logged to the ``query.log``.\n        This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n\n    :param timeout:\n        the transaction timeout in seconds.\n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n        Value should not represent a negative duration.\n        A zero duration will make the transaction execute indefinitely.\n        None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\n    \n    For example, a timeout may be applied:\n    \n    from neo4j import unit_of_work\n    \n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n    \n    :param metadata: a dictionary with metadata. Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures. It will also get logged to the ``query.log``. This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n    \n    :param timeout: the transaction timeout in seconds. Transactions that execute longer than the configured timeout will be terminated by the database. This functionality allows to limit query/transaction execution time. Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting. Value should not represent a negative duration. A zero duration will make the transaction execute indefinitely. None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38969, "num_retrieved": 207, "inference_time": 4.4492135643959045, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        raise KeyError(f\"{key} not found in keys\")", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        raise KeyError(f\"{key} not found in keys\")", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        raise KeyError(f\"{key} not found in keys\")", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        raise KeyError(f\"{key} not found in keys\")", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        raise KeyError(f\"{key} not found in keys\")"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35816, "num_retrieved": 104, "inference_time": 4.4492135643959045, "generate_results": ["def values(self, *keys: _TResultKey) -> t.List[t.Any]:\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.value(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.value(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.value(*keys) for record in self]", "def values(self, *keys: _TResultKey) -> t.List[t.Any]:\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.value(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.value(*keys) for record in self]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34983, "num_retrieved": 77, "inference_time": 4.4492135643959045, "generate_results": ["def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    record_dict = {}\n    for key in keys:\n        if isinstance(key, int):\n            try:\n                record_dict[key] = self[key]\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            record_dict[key] = self[key]\n    \n    # Fill in missing keys with None\n    for key in self.keys():\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises IndexError: if an out-of-bounds index is specified\n    \"\"\"\n    record_keys = list(self.keys())\n    record_values = [self.value(i) for i in range(len(record_keys))]\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_keys):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[record_keys[key]] = record_values[key]\n        else:\n            # key is a string (field name)\n            if key not in record_keys:\n                result[key] = None\n            else:\n                result[key] = self.value(key)\n    \n    # If no keys were provided, return all values\n    if not keys:\n        result = dict(zip(record_keys, record_values))\n    \n    return result", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    record_dict = {}\n    for key in keys:\n        if isinstance(key, int):\n            try:\n                record_dict[key] = self[key]\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            record_dict[key] = self[key]\n    \n    # Fill in missing keys with None\n    for key in self.keys():\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    record_dict = {}\n    for key in keys:\n        if isinstance(key, int):\n            try:\n                record_dict[key] = self[key]\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            record_dict[key] = self[key]\n    \n    # Fill in missing keys with None\n    for key in self.keys():\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    record_dict = {}\n    for key in keys:\n        if isinstance(key, int):\n            try:\n                record_dict[key] = self[key]\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            record_dict[key] = self[key]\n    \n    # Fill in missing keys with None\n    for key in self.keys():\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37532, "num_retrieved": 159, "inference_time": 4.4492135643959045, "generate_results": ["def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned 16-bit integer\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = (self.data[-2] << 8) | self.data[-1]\n    # Remove the last two bytes\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned 16-bit integer\n    value = (self.data[-2] << 8) | self.data[-1]\n    # Remove the last two bytes\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned 16-bit integer\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = self.data[-2] << 8 | self.data[-1]\n    # Remove the two bytes from the data\n    self.data = self.data[:-2]\n    return value"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35649, "num_retrieved": 98, "inference_time": 4.4492135643959045, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(b\"\\x0E\", fields=(n, qid), dehydration_hooks=dehydration_hooks,\n                 response=Response(self, \"discard\", hydration_hooks, **handlers))", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(b\"\\x0E\", fields=(n, qid), dehydration_hooks=dehydration_hooks,\n                 response=Response(self, \"discard\", hydration_hooks, **handlers))", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(b\"\\x0E\", fields=(n, qid), dehydration_hooks=dehydration_hooks,\n                 response=Response(self, \"discard\", hydration_hooks, **handlers))", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(b\"\\x0D\", fields=(n, qid), dehydration_hooks=dehydration_hooks,\n                 response=Response(self, \"discard\", hydration_hooks, **handlers))", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(b\"\\x0D\", fields=(n, qid), dehydration_hooks=dehydration_hooks,\n                 response=Response(self, \"discard\", hydration_hooks, **handlers))"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34499, "num_retrieved": 61, "inference_time": 4.4492135643959045, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\"\"\"\n    if not mode:\n        mode = \"w\"\n    if not metadata:\n        metadata = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x09\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\"\"\"\n    if not mode:\n        mode = \"w\"\n    if not metadata:\n        metadata = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x02\", fields, \n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\"\"\"\n    if not mode:\n        mode = \"w\"\n    if not metadata:\n        metadata = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x09\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\"\"\"\n    if not mode:\n        mode = \"w\"\n    if not metadata:\n        metadata = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x02\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\"\"\"\n    if not mode:\n        mode = \"w\"\n    if not metadata:\n        metadata = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x02\", fields, \n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38081, "num_retrieved": 177, "inference_time": 4.4492135643959045, "generate_results": ["def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    >>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n    \n    :param n: number to round\n    :return: rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    >>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n    \n    :param n: number to round\n    :return: rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    >>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n    \n    :param n: number to round\n    :return: rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    >>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n    \n    :param n: number to round\n    :return: rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    >>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n    \n    :param n: number to round\n    :return: rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38358, "num_retrieved": 186, "inference_time": 4.4492135643959045, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the metaclass for the Point subclass\n    class PointMeta(type):\n        def __call__(cls, *args, **kwargs):\n            # Ensure the instance is properly initialized\n            instance = super().__call__(*args, **kwargs)\n            # Set the srid attribute if provided in fields\n            if 'srid' in fields:\n                instance.srid = fields['srid']\n            return instance\n    \n    # Create the new Point subclass\n    class_name = name\n    class_dict = {\n        '__module__': 'neo4j.spatial',\n        '__doc__': 'A Point subclass for %s' % name,\n        '__slots__': (),\n        '__init__': lambda self, *coordinates: setattr(self, 'coordinates', coordinates),\n        'srid': None,\n        'dim': len(fields.get('coordinates', [])),\n    }\n    \n    # Add the srid mapping to the class\n    class_dict['srid_table'] = srid_map\n    \n    # Create the class using the metaclass\n    PointClass = type(class_name, (Point,), class_dict)\n    \n    # Update the class to use the correct srid_table\n    PointClass.srid_table = srid_map\n    \n    # Override the hydrate_point function to use the new class\n    def hydrate_point(srid, *coordinates):\n        try:\n            point_class, dim = srid_table[srid]\n        except KeyError:\n            point = Point(coordinates)\n            point.srid = srid\n            return point\n        else:\n            if len(coordinates) != dim:\n                raise ValueError(\"SRID %d requires %d coordinates (%d provided)\" % (srid, dim, len(coordinates)))\n            return point_class(coordinates)\n    \n    # Add the hydrate_point function to the class\n    PointClass.hydrate_point = hydrate_point\n    \n    return PointClass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            return super().__new__(cls)\n        \n        def __init__(self, *coordinates):\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            super().__init__(coordinates)\n            self._fields = fields\n    \n    # Register the point type with the srid map\n    srid_map[PointSubclass.__name__] = (PointSubclass, len(fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            return super().__new__(cls)\n        \n        def __init__(self, *coordinates):\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            super().__init__(coordinates)\n            self._fields = fields\n    \n    # Register the point type in the srid_map\n    for srid, point_class in srid_map.items():\n        if point_class == PointSubclass:\n            srid_map[srid] = PointSubclass\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Determine the SRID based on the coordinates\n            srid = None\n            if len(coordinates) >= 2:\n                # Try to get SRID from the first coordinate if it's a tuple\n                if isinstance(coordinates[0], tuple) and len(coordinates[0]) >= 2:\n                    srid = coordinates[0][1]\n            \n            # If no SRID is found, use the default SRID from srid_map\n            if srid is None:\n                srid = srid_map.get(name, 4326)\n            \n            # Create the point instance with the determined SRID\n            if srid is not None:\n                # Use the hydrate_point function to create the actual point instance\n                return hydrate_point(srid, *coordinates)\n            else:\n                # Fallback to the base Point class\n                return Point(*coordinates)\n    \n    # Set the class name and fields\n    PointSubclass.__name__ = name\n    PointSubclass.__doc__ = f\"Point subclass for {name} with {fields} fields\"\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Determine the SRID based on the coordinates\n            srid = srid_map.get(len(coordinates))\n            if srid is None:\n                # If no SRID is found, use the base Point class\n                return super().__new__(cls, *coordinates)\n            \n            # Use the hydrate_point function to create the point instance\n            return hydrate_point(srid, *coordinates)\n        \n        def __init__(self, *coordinates):\n            # Initialize the point with coordinates\n            super().__init__(*coordinates)\n    \n    # Set the class name and add it to the module\n    PointSubclass.__name__ = name\n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38647, "num_retrieved": 196, "inference_time": 4.4492135643959045, "generate_results": ["def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        \n        return wrapper\n    \n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        \n        return wrapper\n    \n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37294, "num_retrieved": 160, "inference_time": 2.8459603786468506, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Common R options that might be needed for pre-commit hooks\n    r_options = [\n        '--vanilla',  # Use vanilla R without user-specific options\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-restore',  # Ensure workspace is not restored\n        '--no-readline',  # Disable readline for consistency\n        '--no-restore',  # Ensure no restore of workspace\n    ]\n    \n    # Insert R options at the beginning of the code\n    # This ensures that the R options are applied before any R code is executed\n    return f'library(R.utils); options({\", \".join(r_options)}); {code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Common R options that might be needed for pre-commit hooks\n    r_options = [\n        '--vanilla',  # Use vanilla R without user options\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-restore',  # Don't restore the workspace (duplicate)\n        '--no-readline',  # Don't use readline for input\n        '--no-threads',  # Don't use threads\n    ]\n    \n    # Join the options with spaces and prepend to the code\n    options_str = ' '.join(r_options)\n    return f'{code} {options_str}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Common R options that might be needed for pre-commit hooks\n    r_options = [\n        '--vanilla',  # Use vanilla R without user-specific settings\n        '--no-save',  # Don't save the R session\n        '--no-restore',  # Don't restore the R session\n        '--no-restore',  # Ensure no restore behavior\n        '--no-readline',  # Disable readline for better compatibility\n        '--no-echo',  # Don't echo commands\n        '--no-site-file',  # Don't load site-specific files\n    ]\n    \n    # Construct the R command with options\n    r_command = f'R --slave --no-restore --no-save --no-echo --no-site-file {code}'\n    \n    # Return the modified code with R options\n    return r_command", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = [\n        '--vanilla',  # Use vanilla R without user-specific settings\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-readline',  # Disable readline for better compatibility\n    ]\n    \n    # Insert R options at the beginning of the code\n    setup_code = '\\n'.join([\n        'options(',\n        '  prompt = \">\",',\n        '  continue = \">\",',\n        '  width = 70,',\n        '  digits = 6,',\n        '  scipen = 999,',\n        '  show.signif.stars = FALSE,',\n        '  stringsAsFactors = FALSE,',\n        '  warn = 1,',\n        '  error = \"stop\",',\n        '  message = \"warning\",',\n        '  verbose = FALSE,',\n        '  max.print = 1000,',\n        '  max.rows = 1000,',\n        '  max.col = 50,',\n        '  max.size = 1000000,',\n        '  max.depth = 100,',\n        '  max.na = 1000,',\n        '  max.iter = 1000,',\n        '  max.time = 3600,',\n        '  max.memory = 2000000000,',\n        '  max.file.size = 1000000000,',\n        '  max.file.count = 1000,',\n        '  max.file.name.length = 256,',\n        '  max.file.path.length = 256,',\n        '  max.file.path.depth = 10,',\n        '  max.file.path.components = 100,',\n        '  max.file.path.separator = \"/\",',\n        '  max.file.path.encoding"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37055, "num_retrieved": 153, "inference_time": 2.8459603786468506, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run in serial mode\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For parallel execution, use a subprocess with xargs\n    # This is a simplified version that doesn't fully implement the xargs behavior\n    # in all cases, but follows the general pattern\n    \n    # Create a command that will run the given command on each argument\n    xargs_cmd = cmd + varargs\n    \n    # Use subprocess to run the command with the given arguments\n    result = subprocess.run(xargs_cmd, capture_output=True)\n    \n    return result.returncode, result.stdout", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency <= 0:\n        target_concurrency = 1\n    \n    # Determine the actual number of jobs to use\n    jobs = target_concurrency\n    \n    # If we have no files, just run the command once\n    if not varargs:\n        result = subprocess.run(cmd, capture_output=True, text=True, **kwargs)\n        return result.returncode, result.stdout.encode() if result.stdout else b''\n    \n    # Shuffle the files to ensure even distribution\n    import random\n    shuffled_files = list(varargs)\n    random.shuffle(shuffled_files)\n    \n    # Split files into chunks for parallel processing\n    chunk_size = max(1, len(shuffled_files) // jobs)\n    file_chunks = []\n    for i in range(0, len(shuffled_files), chunk_size):\n        file_chunks.append(shuffled_files[i:i + chunk_size])\n    \n    # Run each chunk in parallel\n    results = []\n    for chunk in file_chunks:\n        # Create a new command with the current chunk of files\n        new_cmd = cmd + chunk\n        result = subprocess.run(new_cmd, capture_output=True, text=True, **kwargs)\n        results.append(result)\n    \n    # Combine results and determine final exit code\n    final_exit_code = 0\n    for result in results:\n        if result.returncode != 0:\n            final_exit_code = result.returncode\n            break\n    \n    # Return the final exit code and combined output\n    combined_output = b''\n    for result in results:\n        combined_output += result.stdout.encode() if result.stdout else b''\n    \n    return final_exit_code, combined_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run in serial mode\n        result = subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True)\n        return result.returncode, result.stdout\n    \n    # For parallel execution, use a subprocess with xargs\n    # This is a simplified version that assumes xargs is available\n    # In a real implementation, this would use a proper xargs implementation\n    # or spawn multiple processes with the appropriate concurrency\n    \n    # Create a list of commands to run in parallel\n    # This is a simplified version that just runs the command with the files\n    # In practice, this would use a proper parallel execution mechanism\n    \n    # Use the target_concurrency to determine how many processes to spawn\n    # This is a simplified implementation that just runs the command with all files\n    # In a real implementation, this would use a proper parallel execution mechanism\n    \n    # For now, just run the command with all files in a single process\n    result = subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True)\n    return result.returncode, result.stdout", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency <= 0:\n        target_concurrency = 1\n        \n    # Determine the actual number of jobs to use\n    jobs = target_concurrency\n    \n    # If we have no files to process, return immediately\n    if not varargs:\n        return 0, b''\n        \n    # Shuffle the files to ensure even distribution across partitions\n    import random\n    shuffled_files = list(varargs)\n    random.shuffle(shuffled_files)\n    \n    # Use the xargs command to process files in parallel\n    # This is a simplified version that would typically use subprocess.Popen\n    # with a pool of processes to run the command on each file\n    import subprocess\n    import os\n    \n    # Build the command with the files as arguments\n    full_cmd = list(cmd) + shuffled_files\n    \n    # Execute the command with the specified concurrency\n    # In a real implementation, this would use a process pool or similar\n    # to run the command in parallel across the files\n    try:\n        result = subprocess.run(\n            full_cmd,\n            capture_output=True,\n            text=True,\n            timeout=300,  # 5 minute timeout\n            env=os.environ.copy(),\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode() if result.stdout else b''\n    except subprocess.TimeoutExpired:\n        return 1, b'Timeout while running xargs command\\n'\n    except Exception as e:\n        return 1, f'Error running xargs: {str(e)}\\n'.encode()", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency <= 0:\n        target_concurrency = 1\n    \n    # Determine the actual number of jobs to use\n    jobs = target_concurrency\n    \n    # If we have no files, just run the command once\n    if not varargs:\n        result = subprocess.run(cmd, capture_output=True, text=True, **kwargs)\n        return result.returncode, result.stdout.encode() if result.stdout else b''\n    \n    # Shuffle the files to ensure even distribution\n    import random\n    shuffled_files = list(varargs)\n    random.shuffle(shuffled_files)\n    \n    # Split files into chunks for parallel processing\n    chunk_size = max(1, len(shuffled_files) // jobs)\n    file_chunks = []\n    for i in range(0, len(shuffled_files), chunk_size):\n        file_chunks.append(shuffled_files[i:i + chunk_size])\n    \n    # Run each chunk in parallel\n    results = []\n    for chunk in file_chunks:\n        # Create a command with the current chunk of files\n        chunk_cmd = cmd + chunk\n        result = subprocess.run(chunk_cmd, capture_output=True, text=True, **kwargs)\n        results.append(result)\n    \n    # Combine results\n    final_returncode = 0\n    final_output = b''\n    \n    for result in results:\n        if result.returncode != 0:\n            final_returncode = result.returncode\n        if result.stdout:\n            final_output += result.stdout.encode() + b'\\n'\n    \n    # If we have color output, make a pty if supported\n    if color and sys.stdout.isatty():\n        # This would typically involve setting up a pseudo-terminal\n        # For simplicity, we just pass through the output\n        pass\n    \n    return final_returncode, final_output"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 28187, "num_retrieved": 116, "inference_time": 2.8459603786468506, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle every time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37926, "num_retrieved": 182, "inference_time": 2.8459603786468506, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36839, "num_retrieved": 147, "inference_time": 2.8459603786468506, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure the command is properly formatted\n    # to handle shebangs and path resolution correctly\n    if sys.platform == 'win32':\n        # Convert any relative paths to absolute paths\n        # and ensure the command is properly formatted for Windows\n        result = []\n        for arg in cmd:\n            # Handle paths by ensuring they are absolute\n            if os.path.isabs(arg):\n                result.append(arg)\n            else:\n                # For relative paths, we need to ensure they are properly resolved\n                # This is a simplified version that just ensures the path is absolute\n                result.append(os.path.abspath(arg))\n        return tuple(result)\n    else:\n        # For non-Windows platforms, return the original command\n        return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure the command is properly formatted\n    # to handle shebangs and path resolution correctly\n    if sys.platform == 'win32':\n        # For Windows, we need to ensure the first argument is a proper executable path\n        # and that any shebangs are properly handled by using the system's shebang parser\n        # This function will normalize the command to ensure it's in a format that\n        # can be executed on Windows, particularly handling deep-path shebangs\n        # and ensuring the command is properly quoted and formatted\n        return tuple(shlex.quote(arg) if arg.startswith('#') else arg for arg in cmd)\n    \n    # For non-Windows platforms, we just return the original command\n    # as the shebang handling is already handled by the system\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure that the command is properly normalized\n    # to handle shebangs and path issues that arise from deep paths\n    if sys.platform == 'win32':\n        # Convert any paths that might be in a shebang to use forward slashes\n        # and ensure they are properly formatted for Windows\n        normalized_cmd = []\n        for arg in cmd:\n            # Replace backslashes with forward slashes for consistency\n            # This helps with shebang parsing and path handling\n            normalized_arg = arg.replace('\\\\', '/')\n            normalized_cmd.append(normalized_arg)\n        return tuple(normalized_cmd)\n    \n    # For non-Windows platforms, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure the command is properly formatted\n    # to handle shebangs and path resolution correctly\n    if sys.platform == 'win32':\n        # For Windows, we need to ensure the first argument is a proper executable path\n        # and that any shebangs are properly resolved\n        # This handles the case where a script might have a shebang that Windows can't parse\n        # by ensuring we have a proper executable path\n        if isinstance(cmd, tuple) and cmd and isinstance(cmd[0], str):\n            # Normalize the command path for Windows\n            # Ensure it's a full path or a valid executable\n            first_arg = cmd[0]\n            # If the first argument is a shebang, we need to resolve it properly\n            if first_arg.startswith('#!'):\n                # Replace the shebang with the actual executable path\n                # This handles deep-path shebangs that Windows can't parse\n                # by ensuring we have a valid executable path\n                # We use the first argument as the executable path\n                # and remove the shebang\n                new_cmd = (first_arg.split('#!')[1],) + cmd[1:]\n                return new_cmd\n            # For regular Windows paths, ensure they are properly formatted\n            # by ensuring they are absolute paths\n            if not os.path.isabs(first_arg):\n                first_arg = os.path.abspath(first_arg)\n            # Ensure the path is properly formatted for Windows\n            # by replacing backslashes with forward slashes\n            first_arg = first_arg.replace(os.sep, os.altsep)\n            # Replace the first argument with the properly formatted path\n            cmd = (first_arg,) + cmd[1:]\n    \n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure the command is properly formatted\n    # to handle shebangs and path resolution correctly\n    if sys.platform == 'win32':\n        # For Windows, we need to ensure the command is properly quoted\n        # and that any shebangs are handled correctly\n        # This is a simplified version that ensures the command is\n        # properly formatted for Windows execution\n        return tuple(shlex.quote(str(c)) for c in cmd)\n    \n    # For non-Windows systems, we can use the standard shebang handling\n    # which is already handled by the parse_shebang module\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10175, "num_retrieved": 54, "inference_time": 2.8459603786468506, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[cache_key]\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n            \n            return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[cache_key]\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n            \n            return result\n        \n        # Add a lock if provided for thread safety\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[key_value]\n                return result\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_value] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            import functools\n            wrapper = functools.wraps(func)(wrapper)\n            wrapper.__lock__ = lock\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10517, "num_retrieved": 52, "inference_time": 2.8459603786468506, "generate_results": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    from cachetools.keys import hashkey, typedkey\n    \n    key_func = typedkey if typed else hashkey\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    from cachetools.keys import hashkey, typedkey\n    \n    key_func = typedkey if typed else hashkey\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 2.8459603786468506, "generate_results": ["def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    def decorator(func):\n        cache = MRUCache(maxsize=maxsize)\n        \n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 2.8459603786468506, "generate_results": ["def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    from .decorators import cached\n    \n    return cached(LRUCache(maxsize), key=key)", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10041, "num_retrieved": 49, "inference_time": 2.8459603786468506, "generate_results": ["def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                # Function not in cache, compute and store result\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                # Function not in cache, compute and store result\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU) algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            key = args + tuple(sorted(kwargs.items())) if not typed else args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + tuple(type(arg) for arg in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                # Function not in cache, compute and store result\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(a) for a in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                # Function not in cache, compute and store result\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            # Call the original function\n            result = func(*args, **kwargs)\n            \n            # Store the result in cache\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 8936, "num_retrieved": 45, "inference_time": 2.8459603786468506, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11144, "num_retrieved": 57, "inference_time": 2.8459603786468506, "generate_results": ["def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11342, "num_retrieved": 59, "inference_time": 2.8459603786468506, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11327, "num_retrieved": 60, "inference_time": 2.8459603786468506, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the key for this call\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    cache[k] = v\n                    return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss, compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    cache[k] = v\n                    return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the key for this call\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    cache[k] = v\n                    return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on cache internals\n                # For now, we just return a placeholder\n                return _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Create a wrapper function that will handle caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    return cache[k]\n                except KeyError:\n                    # Cache miss - call the original method and store the result\n                    result = method(self, *args, **kwargs)\n                    cache[k] = result\n                    return result\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to access the cache's internal state\n                # For simplicity, we return a placeholder\n                return _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original method's metadata\n        functools.update_wrapper(wrapper, method)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Create a wrapper function that will handle caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    # Try to get the value from cache\n                    return cache[k]\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    value = method(self, *args, **kwargs)\n                    cache[k] = value\n                    return value\n        \n        # Add cache info and clear methods to the wrapper\n        wrapper.cache_info = lambda: cache.cache_info() if hasattr(cache, 'cache_info') else None\n        wrapper.cache_clear = lambda: cache.cache_clear() if hasattr(cache, 'cache_clear') else None\n        \n        # Update the wrapper with the original method's metadata\n        functools.update_wrapper(wrapper, method)\n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 7109, "num_retrieved": 23, "inference_time": 2.8459603786468506, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback information\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_lines = []\n        for frame in tb:\n            tb_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb_str = \"\\n\".join(tb_lines)\n        \n        # Combine everything\n        result = f\"{exc_type}: {exc_msg}\\nTraceback (most recent call last):\\n{tb_str}\"\n        \n        return result\n        \n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            trace_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Return formatted string\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            trace_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11406, "num_retrieved": 43, "inference_time": 2.8459603786468506, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    import errno\n    \n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode or write mode if overwrite is True\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            # Write the bytes buffer to file\n            file.write(text_buffer)\n        \n        # Return the number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except OSError as e:\n        if e.errno == errno.EACCES:\n            # Permission denied\n            return -1\n        elif e.errno == errno.ENOENT:\n            # Directory not found\n            return -1\n        else:\n            # Other OS error\n            return -1\n    except Exception as e:\n        # Any other exception\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11524, "num_retrieved": 44, "inference_time": 3.3578641414642334, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11387, "num_retrieved": 44, "inference_time": 3.3578641414642334, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 5115, "num_retrieved": 16, "inference_time": 3.3578641414642334, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's per-thread and managed by the local instance\n    \n    # Optionally, reset any platform-specific state if needed\n    # This is a placeholder - actual implementation depends on what needs to be reset\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    if hasattr(ContextFilter, 'LOC'):\n        ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state found in context, so no additional reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    if hasattr(ContextFilter, 'LOC'):\n        ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state found in context, so no additional reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's managed by the gevent context\n    \n    # Reinitialize any platform-specific logging if needed\n    # This is a placeholder - actual implementation depends on application needs\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in the context, so this is minimal)"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37376, "num_retrieved": 166, "inference_time": 3.3578641414642334, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35246, "num_retrieved": 92, "inference_time": 3.3578641414642334, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _merge_orderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    result = []\n    positions = {}\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, find its position in the result\n        # and ensure that the suffix starting with that element is preserved\n        for i, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = len(result)\n            else:\n                # If the element already exists, ensure that the suffix is preserved\n                # by checking if the current ordering has a suffix that matches\n                # the existing suffix\n                pass\n    \n    # Sort the orderings by the first element to ensure a consistent order\n    # and merge them in a way that preserves the within-ordering order\n    sorted_orderings = sorted(orderings, key=lambda x: x[0])\n    \n    # Start with the first ordering\n    result = sorted_orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in sorted_orderings[1:]:\n        # Find the position where the current ordering should be inserted\n        # such that the suffix starting with each element is preserved\n        for i, element in enumerate(ordering):\n            # Find the position of the element in the result\n            try:\n                pos = result.index(element)\n            except ValueError:\n                # Element not found, insert it at the end\n                result.append(element)\n                break\n            else:\n                # Element found, ensure that the suffix is preserved\n                # by checking if the suffix starting with the element is in the result\n                suffix = ordering[i:]\n                # Check if the suffix is already", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _merge_orderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by the first element that appears in multiple orderings\n    # and then by the order in which they appear in the original list\n    sorted_orderings = sorted(orderings, key=lambda x: (len(x), x[0]))\n    \n    # Build the result ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # For each element in the ordering, if it's not already in the result,\n        # add it to the result\n        for element in ordering:\n            if element not in seen:\n                result.append(element)\n                seen.add(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _merge_orderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of tuples to represent the orderings\n    result = []\n    \n    # For each ordering, we need to find the position of each element\n    # and merge them in a way that preserves the order within each ordering\n    # and respects the constraint that if an element appears in multiple orderings,\n    # the suffix starting with that element must be in all orderings\n    \n    # First, create a list of all elements and their positions in each ordering\n    elements = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in elements:\n                elements[element] = []\n            elements[element].append(i)\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that we process the orderings in a way that respects the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Now merge the orderings\n    # We'll use a dictionary to track the current position of each element\n    current_positions = {}\n    \n    # Process each ordering\n    for ordering in sorted_orderings:\n        # For each element in the ordering, ensure it appears in the result\n        # and that the suffix starting with that element is preserved\n        for i, element in enumerate(ordering):\n            if element not in current_positions:\n                current_positions[element] = len(result)\n            # Add the element to the result\n            result.insert(current_positions[element], element)\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    final_result = []\n    for item in result:\n        if item not", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n    \n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n    \n    For example:\n    \n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of tuples to represent the orderings\n    result = []\n    \n    # For each ordering, we need to find the position of each element\n    # and merge them in a way that preserves the order within each ordering\n    \n    # First, create a list of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # This will help us determine the order in which elements should appear\n    element_positions = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in element_positions:\n                element_positions[element] = []\n            element_positions[element].append(i)\n    \n    # Sort the elements by their first appearance in any ordering\n    # This will give us the order in which elements should appear in the result\n    sorted_elements = sorted(all_elements, key=lambda x: min(element_positions[x]) if x in element_positions else float('inf'))\n    \n    # Now, we need to merge the orderings in a way that preserves the order within each ordering\n    # We'll do this by creating a new ordering that includes all elements\n    # and then ensuring that the order within each original ordering is preserved\n    \n    # Create a list to store the result\n    result = []\n    \n    # For each element in the sorted list, add it to the result\n    # We need to ensure that if an element appears in multiple orderings,\n    # then the suffix that begins with that element must be in both orderings\n    \n    #", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _merge_orderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of lists to represent the orderings\n    result = []\n    \n    # For each ordering, we need to find the position of each element\n    # and merge them in a way that preserves the order within each ordering\n    \n    # First, create a list of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a list of all elements in the orderings\n    # We'll use a dictionary to store the position of each element in each ordering\n    element_positions = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in element_positions:\n                element_positions[element] = []\n            element_positions[element].append(i)\n    \n    # Now, we need to merge the orderings\n    # We'll create a result list and add elements to it in the order they appear\n    # in the orderings\n    \n    # Create a list of all elements in the orderings\n    # We'll use a dictionary to store the position of each element in each ordering\n    # and then merge them\n    \n    # Create a list of all elements in the orderings\n    # We'll use a dictionary to store the position of each element in each ordering\n    # and then merge them\n    \n    # Create a list of all elements in the orderings\n    # We'll use a dictionary to store the position of each element in each ordering\n    # and then merge them\n    \n    # Create a list of all elements in the orderings\n    # We'll use a dictionary to store the position of each element"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38093, "num_retrieved": 192, "inference_time": 3.3578641414642334, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38613, "num_retrieved": 206, "inference_time": 3.3578641414642334, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by MRO to get the minimal ordered equivalent\n    # This is a simplified version - in practice, this would need to\n    # consider the actual MRO and inheritance hierarchy\n    return tuple(unique_classes)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by MRO to ensure consistent ordering\n    # This is a simplified version - in practice, this would need to consider\n    # the actual MRO and inheritance hierarchy\n    return tuple(unique_classes)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track interfaces that are already in the result\n    result = []\n    seen = set()\n    \n    # Process classes in the order they appear in the input list\n    for cls in classes:\n        # Skip if already seen\n        if cls in seen:\n            continue\n            \n        # Add to result and mark as seen\n        result.append(cls)\n        seen.add(cls)\n        \n        # Add any base classes of this class that aren't already in the result\n        # This is a simplified version - in a real implementation, we'd need\n        # to handle the full MRO and interface resolution\n        for base in cls.__bases__:\n            if base not in seen:\n                seen.add(base)\n                result.append(base)\n    \n    # Remove duplicates by converting to tuple and back to list\n    # This ensures the result is ordered and minimal\n    unique_result = []\n    for cls in result:\n        if cls not in unique_result:\n            unique_result.append(cls)\n    \n    return tuple(unique_result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Process classes in the order they appear\n    for cls in classes:\n        # If we haven't seen this class before, add it to the result\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro to ensure consistent ordering\n    # This is a simplified version - in practice, you'd need to consider\n    # the actual MRO and inheritance hierarchy\n    return tuple(unique_classes)"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39312, "num_retrieved": 231, "inference_time": 3.3578641414642334, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names = self.names()\n            descriptions = []\n            for name in names:\n                desc = self.direct(name)\n                if desc is not None:\n                    descriptions.append((name, desc))\n            return descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names = self.names(all=True)\n            descriptions = []\n            for name in names:\n                desc = self.direct(name)\n                if desc is not None:\n                    descriptions.append((name, desc))\n            return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.names(all=False)]\n        else:\n            # Return all attributes including those defined by base classes\n            return [(name, desc) for name, desc in self.names(all=True)]", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.names(all=False)]\n        else:\n            # Return all attributes including those defined by base classes\n            return [(name, desc) for name, desc in self.names(all=True)]", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.names(all=False)]\n        else:\n            # Return all attributes including those defined by base classes\n            return [(name, desc) for name, desc in self.names(all=True)]", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.names(all=False)]\n        else:\n            # Return all attributes including those defined by base interfaces\n            return [(name, desc) for name, desc in self.names(all=True)]"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39660, "num_retrieved": 243, "inference_time": 3.3578641414642334, "generate_results": ["def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Return all attribute names including those from base interfaces\n        names = []\n        for base in self.__bases__:\n            names.extend(base.names(all=True))\n        names.extend(list(self.__dict__.keys()))\n        return list(set(names))", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37880, "num_retrieved": 182, "inference_time": 3.3578641414642334, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36012, "num_retrieved": 120, "inference_time": 3.3578641414642334, "generate_results": ["def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36075, "num_retrieved": 122, "inference_time": 3.3578641414642334, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, in which case we don't attempt C optimizations\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Default to attempting C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Otherwise, attempt C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, in which case we don't attempt C optimizations\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Otherwise, attempt C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39386, "num_retrieved": 233, "inference_time": 3.3578641414642334, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36937, "num_retrieved": 152, "inference_time": 3.3578641414642334, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34867, "num_retrieved": 84, "inference_time": 3.3578641414642334, "generate_results": ["def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = [self.initial_value] * self.nbins\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = [self.initial_value] * self.nbins\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = [self.initial_value] * self.nbins\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = [self.initial_value] * self.nbins\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = [self.initial_value] * self.nbins\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34157, "num_retrieved": 58, "inference_time": 3.3578641414642334, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use ToCSV class instead for converting graphs to CSV\n    if header is not None:\n        yield header\n    \n    # For a graph, we need to iterate through points and format them\n    # The actual implementation would depend on the graph structure\n    # and how coordinates and values are stored\n    # Since this is a deprecated function, no specific implementation\n    # is provided as the recommended approach is to use ToCSV\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and replaced by ToCSV\n    # Convert graph points to CSV format\n    if header is not None:\n        yield header\n    \n    # For each point in the graph, format the coordinates and values\n    for point in self:\n        # Convert coordinates to string with separator\n        coords_str = separator.join(str(coord) for coord in point.coords)\n        # Convert values to string with separator\n        values_str = separator.join(str(value) for value in point.values)\n        yield coords_str + separator + values_str", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and replaced by ToCSV\n    # Convert graph points to CSV format\n    if header is not None:\n        yield header\n    \n    # For each point in the graph, format the coordinates and values\n    for point in self:\n        # Convert coordinates to string\n        coords_str = separator.join(str(coord) for coord in point.coords)\n        # Convert values to string\n        values_str = separator.join(str(value) for value in point.values)\n        # Combine coordinates and values\n        yield coords_str + separator + values_str", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use ToCSV class for converting graphs to CSV\n    if header is not None:\n        yield header\n    \n    # For a graph, we need to iterate through points and format them\n    # The actual implementation would depend on the graph structure\n    # and how coordinates and values are stored\n    # Since this is a deprecated function, we simply return nothing\n    # and recommend using ToCSV instead\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use ToCSV class instead for converting graphs to CSV\n    if header is not None:\n        yield header\n    \n    # For a graph, we need to iterate through points and format them\n    # The actual implementation would depend on the graph structure\n    # and how coordinates and values are stored\n    \n    # Since this is a deprecated function, we simply yield nothing\n    # and recommend using ToCSV instead\n    pass"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36844, "num_retrieved": 157, "inference_time": 5.063642740249634, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    error_x_ind = 0\n    error_y_ind = 0\n    \n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            if coord_name == \"x\":\n                error_x_ind = error_ind\n            elif coord_name == \"y\":\n                error_y_ind = error_ind\n    \n    return error_x_ind, error_y_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    error_x_ind = 0\n    error_y_ind = 0\n    \n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            if err[1] == \"x\":\n                error_x_ind = error_ind\n            elif err[1] == \"y\":\n                error_y_ind = error_ind\n    \n    return error_x_ind, error_y_ind"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 33897, "num_retrieved": 48, "inference_time": 5.063642740249634, "generate_results": ["def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Initialize error context\n    error_context = {}\n    \n    # Check for error fields in the graph\n    for i, field_name in enumerate(field_names):\n        # Check if the field name ends with \"_low\" or \"_high\" to identify error fields\n        if field_name.endswith(\"_low\") or field_name.endswith(\"_high\"):\n            # Extract the base field name (without \"_low\" or \"_high\")\n            base_field = field_name[:-4] if field_name.endswith(\"_low\") or field_name.endswith(\"_high\") else field_name\n            \n            # Map base field to coordinate (x, y, z)\n            coord_map = {\"x\": 0, \"y\": 1, \"z\": 2}\n            coord = \"x\"\n            \n            # Determine the coordinate based on the base field\n            if base_field == \"x\" or base_field == \"y\" or base_field == \"z\":\n                coord = base_field\n            \n            # Create error context entry\n            error_name = f\"{coord}_{field_name[-4:]}\" if field_name.endswith(\"_low\") or field_name.endswith(\"_high\") else f\"{coord}_{field_name}\"\n            error_context[error_name] = {\"index\": i}\n    \n    # Update context with error information\n    if error_context:\n        context.update({\"error\": error_context})\n    \n    # Ensure value context is preserved\n    # Existing values in context.value and its subcontexts are not removed", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Initialize error context\n    error_context = {}\n    \n    # Check for error fields in the graph\n    for i, field_name in enumerate(field_names):\n        # Check if the field name ends with \"_low\" or \"_high\" to identify error fields\n        if field_name.endswith(\"_low\") or field_name.endswith(\"_high\"):\n            # Extract the base field name (without \"_low\" or \"_high\")\n            base_field = field_name[:-4] if field_name.endswith(\"_low\") or field_name.endswith(\"_high\") else field_name\n            \n            # Determine the coordinate (x, y, z) based on position\n            coord_index = i\n            \n            # Map coordinate index to x, y, z\n            coord_name = \"x\" if coord_index == 0 else \"y\" if coord_index == 1 else \"z\"\n            \n            # Store the error index\n            error_context[coord_name + (\"_low\" if field_name.endswith(\"_low\") else \"_high\")] = {\"index\": i}\n    \n    # Update context with error information\n    if error_context:\n        context.update({\"error\": error_context})", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Initialize error context\n    error_context = {}\n    \n    # Check for error fields in the graph\n    for i, field_name in enumerate(field_names):\n        # Check if the field name ends with \"_low\" or \"_high\" to identify errors\n        if field_name.endswith(\"_low\") or field_name.endswith(\"_high\"):\n            # Extract the base field name (without \"_low\" or \"_high\")\n            base_field = field_name[:-4] if field_name.endswith(\"_low\") or field_name.endswith(\"_high\") else field_name\n            \n            # Map error field names to coordinate names (x, y, z)\n            if base_field == \"x\" or base_field == \"y\" or base_field == \"z\":\n                # Use the coordinate name as the key\n                coord_name = base_field\n                error_key = f\"{coord_name}_{field_name[-4:]}\"  # e.g., \"x_low\"\n                \n                # Add the error index to the error context\n                if \"error\" not in error_context:\n                    error_context[\"error\"] = {}\n                error_context[\"error\"][error_key] = {\"index\": i}\n    \n    # Update the context with the error information\n    # Existing values are not removed from context.value and its subcontexts\n    if error_context:\n        lena.context.update_recursively(context, error_context)", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Initialize error context\n    error_context = {\"error\": {}}\n    \n    # Check for error fields (x, y, z) and their indices\n    for i, field_name in enumerate(field_names):\n        # Check if the field name ends with an error suffix\n        if field_name.endswith(\"_low\") or field_name.endswith(\"_high\"):\n            # Determine the base field name (without _low or _high)\n            base_field = field_name[:-4] if field_name.endswith(\"_low\") or field_name.endswith(\"_high\") else field_name\n            \n            # Map to x, y, z based on position\n            coord_index = i\n            \n            # Create error name (x, y, z) based on position\n            error_name = \"x\" if coord_index == 0 else \"y\" if coord_index == 1 else \"z\"\n            \n            # Add to error context\n            error_context[\"error\"][f\"{error_name}_{field_name.split('_')[-1]}\"] = {\"index\": i}\n    \n    # Update context with error information\n    # Existing values are not removed from context.value and its subcontexts\n    if \"error\" not in context:\n        context[\"error\"] = {}\n    \n    # Append the error context to existing error context\n    for key, value in error_context[\"error\"].items():\n        context[\"error\"][key] = value\n    \n    # Ensure the value context is preserved\n    # This function only updates error context, not value context", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Initialize error context\n    error_context = {}\n    \n    # Check for error fields in the graph\n    for i, field_name in enumerate(field_names):\n        # Error fields are named with suffixes like \"_low\", \"_high\", etc.\n        # We look for common error suffixes\n        error_suffixes = [\"_low\", \"_high\", \"_error\"]\n        \n        for suffix in error_suffixes:\n            error_field_name = field_name + suffix\n            if error_field_name in field_names:\n                # Determine which coordinate this error corresponds to\n                # x, y, z correspond to the first three fields\n                coord_index = i % 3\n                error_key = f\"{['x', 'y', 'z'][coord_index]}_{suffix}\"\n                \n                # Append the index of the error field\n                if error_key not in error_context:\n                    error_context[error_key] = {\"index\": i}\n    \n    # Update context with error information\n    if error_context:\n        # Append to existing error context without removing existing values\n        if \"error\" in context:\n            # Merge with existing error context\n            for error_key, error_data in error_context.items():\n                context[\"error\"][error_key] = error_data\n        else:\n            context[\"error\"] = error_context\n    \n    # Ensure value context is preserved\n    # Existing values in context.value and its subcontexts are not removed"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35694, "num_retrieved": 115, "inference_time": 5.063642740249634, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier manipulation\n    edges = np.array(edges)\n    \n    # Calculate the volume of each bin\n    # For each dimension, the bin volume is (edge[i+1] - edge[i])\n    # For a multidimensional histogram, we multiply the volumes of all dimensions\n    volumes = np.diff(edges, axis=0) if edges.ndim > 1 else np.diff(edges, axis=0)\n    \n    # For 1D case, we need to handle the edges properly\n    if edges.ndim == 1:\n        # For 1D histogram, the bin volume is (edges[i+1] - edges[i])\n        bin_volumes = np.diff(edges)\n        # The integral is the sum of bins[i] * bin_volumes[i]\n        return np.sum(bins * bin_volumes)\n    \n    # For multidimensional histogram, we need to compute the volume of each cell\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # For each cell, the volume is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh\n    # The volume of each cell is the product of the differences in each dimension\n    # We need to compute the volume for each cell in the mesh", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier manipulation\n    edges = np.array(edges)\n    \n    # Calculate the volume of each bin\n    # For each dimension, the volume is (edge[i+1] - edge[i]) * (edge[i+1] - edge[i])\n    # For 1D, it's simply (edge[i+1] - edge[i])\n    # For multi-dimensional, it's the product of the differences\n    \n    # If edges is a list of arrays (multi-dimensional), we need to compute the volume of each bin\n    if isinstance(edges, (list, tuple)):\n        # For multi-dimensional histogram, compute the volume of each bin\n        # The volume of each bin is the product of the differences between consecutive edges\n        volumes = []\n        for i in range(len(edges)):\n            # For each dimension, compute the width of the bin\n            width = edges[i][1:] - edges[i][:-1]\n            volumes.append(width)\n        \n        # Compute the volume of each bin as the product of the widths\n        # This is equivalent to the product of the differences between consecutive edges\n        bin_volumes = np.prod(volumes, axis=0)\n        \n        # Compute the integral as the sum of bins * bin_volumes\n        integral_value = np.sum(np.array(bins) * bin_volumes)\n        \n        return integral_value\n    \n    else:\n        # For 1D histogram, compute the integral as the sum of bins * (edges[i+1] - edges[i])\n        bin_widths = edges[1:] - edges[:-1]\n        integral_value = np.sum(np.array(bins) * bin_widths)\n        \n        return integral_value", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier manipulation\n    edges = np.array(edges)\n    \n    # Calculate the volume of each bin\n    # For each dimension, the bin volume is (edge[i+1] - edge[i])\n    # For a multidimensional histogram, we need to compute the product of bin volumes\n    # along each dimension\n    \n    # Get the bin volumes for each dimension\n    bin_volumes = []\n    for i in range(len(edges) - 1):\n        bin_volumes.append(edges[i+1] - edges[i])\n    \n    # Calculate the total integral by summing the product of bin values and bin volumes\n    total_integral = 0.0\n    for i in range(len(bins)):\n        # For each bin, multiply the bin value by the product of bin volumes\n        # This assumes bins are stored in a format where bins[i] corresponds to the i-th bin\n        bin_value = bins[i]\n        bin_volume = 1.0\n        for j in range(len(bin_volumes)):\n            bin_volume *= bin_volumes[j]\n        \n        total_integral += bin_value * bin_volume\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier manipulation\n    edges = np.array(edges)\n    \n    # Calculate the volume of each bin\n    # For each dimension, the bin volume is (edge[i+1] - edge[i])\n    # For a multidimensional histogram, we need to compute the product of bin volumes\n    # across all dimensions\n    \n    # Get the bin volumes for each dimension\n    bin_volumes = []\n    for i in range(len(edges) - 1):\n        bin_volumes.append(edges[i+1] - edges[i])\n    \n    # Calculate the total integral by summing the product of bin values and bin volumes\n    total_integral = 0.0\n    for i in range(len(bins)):\n        # For each bin, multiply the bin value by the product of bin volumes\n        # The bin value is at index i in bins\n        bin_value = bins[i]\n        \n        # Calculate the product of bin volumes for this bin\n        # This is a simplification - in a multidimensional histogram, each bin has its own volume\n        # We need to calculate the volume of each bin based on its position in the mesh\n        bin_volume = 1.0\n        for j in range(len(edges) - 1):\n            # Get the bin index for this dimension\n            bin_index = i // (len(edges) - 1) if len(edges) > 1 else i\n            # This is a simplified approach - in reality, we need to calculate the bin volume\n            # based on the actual bin indices\n            bin_volume *= (edges[j+1] - edges[j])\n        \n        total_integral += bin_value * bin_volume\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier manipulation\n    edges = np.array(edges)\n    \n    # Calculate the volume of each bin\n    # For each dimension, the volume is (edge[i+1] - edge[i]) * (edge[i+1] - edge[i])\n    # For 1D: volume = edge[i+1] - edge[i]\n    # For 2D+: volume = (edge[i+1] - edge[i]) * (edge[j+1] - edge[j])\n    \n    # If edges is a single array (1D), compute the integral as sum of bin values * bin_width\n    if isinstance(edges, (list, tuple)) and len(edges) == 1:\n        bin_width = edges[0][1] - edges[0][0]\n        return np.sum(bins) * bin_width\n    \n    # For multidimensional case, compute the volume of each bin\n    # The volume of each bin is the product of the differences between consecutive edges\n    # in each dimension\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        # Calculate the volume for each bin\n        volumes = []\n        for i in range(len(edges)):\n            # For each dimension, calculate the width of the bin\n            width = edges[i][1] - edges[i][0]\n            volumes.append(width)\n        \n        # Calculate the total volume of each bin\n        bin_volumes = np.prod(volumes)\n        \n        # Compute the integral as the sum of bin values * bin volume\n        return np.sum(bins) * bin_volumes\n    \n    # Default case - if edges is not properly formatted\n    # Return sum of bins as the integral\n    return np.sum(bins)"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34585, "num_retrieved": 76, "inference_time": 5.063642740249634, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq contains a FillRequest element\n    # This requires checking the internal structure\n    # Since we don't have direct access to the internal structure,\n    # we rely on the check_sequence_type module which is imported\n    # and used in the context.\n    \n    # Based on the context, we know that:\n    # - FillRequestSeq is created from a sequence containing a FillRequest element\n    # - The check_sequence_type module has a function is_fill_request_el\n    # - The _get_seq_with_type function checks for FillRequestSeq\n    \n    # Since we don't have direct access to the internal elements,\n    # we can only check if it's not a Source and if it has a FillRequest element\n    # based on the type checking.\n    \n    # The context shows that is_fill_request_el is used in the initialization\n    # of FillRequestSeq, so we can assume that if the sequence has a FillRequest\n    # element, it can be converted to FillRequestSeq.\n    \n    # We need to check if seq has a FillRequest element\n    # This is done by checking if it's a FillRequestSeq or if it contains one\n    # through the check_sequence_type module.\n    \n    # Since we don't have direct access to the internal elements,\n    # we rely on the fact that the context shows that is_fill_request_el\n    # is used to check for FillRequest elements.\n    \n    # Therefore, we can implement this function as:\n    try:\n        from .check_sequence_type import is_fill_request_el\n        if is_fill_request_el(seq):\n            return True\n    except ImportError:\n        pass\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq contains a FillRequest element\n    try:\n        from .check_sequence_type import is_fill_request_el\n        if is_fill_request_el(seq):\n            return True\n    except ImportError:\n        pass\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq contains a FillRequest element\n    try:\n        from .check_sequence_type import is_fill_request_el\n        if is_fill_request_el(seq):\n            return True\n    except ImportError:\n        pass\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq contains a FillRequest element\n    try:\n        from .check_sequence_type import is_fill_request_el\n        if is_fill_request_el(seq):\n            return True\n    except ImportError:\n        pass\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq contains a FillRequest element\n    try:\n        from .check_sequence_type import is_fill_request_el\n        if is_fill_request_el(seq):\n            return True\n    except ImportError:\n        pass\n    \n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36409, "num_retrieved": 144, "inference_time": 5.063642740249634, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36728, "num_retrieved": 154, "inference_time": 5.063642740249634, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36475, "num_retrieved": 146, "inference_time": 5.063642740249634, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34054, "num_retrieved": 54, "inference_time": 5.063642740249634, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion - check all keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif level == 1 and d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level - 1)\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion - check all keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif level == 1 and d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level - 1)\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion - check all keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif level == 1 and d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level - 1)\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion - check all keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif level == 1 and d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level - 1)\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion - check all keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif level == 1 and d1[key] != d2[key]:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level - 1)\n    \n    return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36122, "num_retrieved": 129, "inference_time": 5.063642740249634, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if coordinates are within the edges\n    if not isinstance(edges, (list, tuple)):\n        # Handle 1D case\n        if coord < edges[0] or coord >= edges[1]:\n            return\n    \n    # For multidimensional case, check each coordinate\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        for i, edge in enumerate(edges):\n            if not isinstance(edge, (list, tuple)):\n                # 1D case\n                if coord < edge[0] or coord >= edge[1]:\n                    return\n            else:\n                # Multidimensional case\n                if len(coord) != len(edge):\n                    return\n                for j, val in enumerate(coord):\n                    if val < edge[j][0] or val >= edge[j][1]:\n                        return\n    \n    # Find the bin index for the given coordinate\n    bin_index = []\n    for i, edge in enumerate(edges):\n        if isinstance(edge, (list, tuple)):\n            # Find the bin index for this dimension\n            if len(coord) > 1:\n                bin_index.append(self._find_bin_index(coord[i], edge))\n            else:\n                bin_index.append(self._find_bin_index(coord, edge))\n        else:\n            # 1D case\n            bin_index.append(self._find_bin_index(coord, edge))\n    \n    # Update the bin content with the weight\n    bin_content = bins[bin_index]\n    bin_content += weight\n    bins[bin_index] = bin_content", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if coordinates are within the histogram edges\n    if not isinstance(edges, (list, tuple)):\n        # Handle 1D case\n        if not (edges[0] <= coord <= edges[1]):\n            return  # Coordinate outside edges, ignore\n    \n    # For multidimensional case, check each coordinate\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        for i, edge in enumerate(edges):\n            if not isinstance(edge, (list, tuple)):\n                # 1D case\n                if not (edge[0] <= coord[i] <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n            else:\n                # Multidimensional case\n                if not (edge[0] <= coord[i] <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n    \n    # Find the bin index for the given coordinate\n    bin_index = get_bin_on_index(coord, bins)\n    \n    # Add the weight to the bin\n    if bin_index is not None:\n        bin_index += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if coordinates are within the histogram edges\n    if not isinstance(edges, (list, tuple)):\n        # Handle 1D case\n        if not (edges[0] <= coord <= edges[1]):\n            return  # Coordinate outside edges, ignore\n    \n    # For multidimensional case, check each coordinate\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        for i, edge in enumerate(edges):\n            if not isinstance(edge, (list, tuple)):\n                # 1D case\n                if not (edge[0] <= coord <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n            else:\n                # Multi-dimensional case\n                if not (edge[0] <= coord <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n    \n    # Find the bin index for the given coordinate\n    bin_index = get_bin_on_index(coord, bins)\n    \n    # Add the weight to the bin content\n    if bin_index is not None:\n        bin_index[0] += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if coordinates are within the histogram edges\n    if not isinstance(edges, (list, tuple)):\n        # Handle 1D case\n        if not (edges[0] <= coord <= edges[1]):\n            return  # Coordinate outside edges, ignore\n    \n    # For multidimensional case, check each coordinate\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        for i, edge in enumerate(edges):\n            if not isinstance(edge, (list, tuple)):\n                # 1D case\n                if not (edge[0] <= coord <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n            else:\n                # Multidimensional case\n                if not (edge[0] <= coord <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n    \n    # Find the bin index for the given coordinate\n    bin_index = get_bin_on_index(coord, bins)\n    \n    # Add the weight to the bin content\n    if bin_index is not None:\n        bin_index[0] += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if coordinates are within the histogram edges\n    if not isinstance(edges, (list, tuple)):\n        # Handle 1D case\n        if not (edges[0] <= coord <= edges[1]):\n            return  # Coordinate outside edges, ignore\n    \n    # For multidimensional case, check each coordinate\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        for i, edge in enumerate(edges):\n            if not isinstance(edge, (list, tuple)):\n                # 1D case\n                if not (edge[0] <= coord[i] <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n            else:\n                # Multidimensional case\n                if not (edge[0] <= coord[i] <= edge[1]):\n                    return  # Coordinate outside edges, ignore\n    \n    # Determine the bin index for the given coordinate\n    bin_index = []\n    for i, edge in enumerate(edges):\n        if isinstance(edge, (list, tuple)):\n            # Find the bin index for this dimension\n            if len(edge) > 1:\n                # Find the bin that contains the coordinate\n                bin_idx = 0\n                for j in range(1, len(edge) - 1):\n                    if edge[j] > coord[i]:\n                        bin_idx = j\n                        break\n                bin_index.append(bin_idx)\n        else:\n            # 1D case\n            bin_index.append(0)\n    \n    # Update the bin content with the weight\n    if bin_index:\n        # Get the bin at the calculated index\n        bin_content = get_bin_on_index(bin_index, bins)\n        if bin_content is not None:\n            bin_content += weight"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34567, "num_retrieved": 68, "inference_time": 5.063642740249634, "generate_results": ["def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Tuple\n    \n    # Define regular expressions for label keys and values\n    # Label keys must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])? (as per Kubernetes label key rules)\n    KEY_REGEX = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    # Label values must match: [a-z0-9]([a-z0-9._-]*[a-z0-9])? (as per Kubernetes label value rules)\n    VALUE_REGEX = re.compile(r'^[a-z0-9]([a-z0-9._-]*[a-z0-9])?$')\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex\n        if not KEY_REGEX.match(key):\n            errors.append({key: f\"Label key '{key}' does not match the regex\"})", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Tuple\n    \n    # Define regular expressions for label keys and values\n    # Label keys must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])? (as per Kubernetes label key rules)\n    KEY_REGEX = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    # Label values must match: [a-z0-9]([a-z0-9._-]*[a-z0-9])? (as per Kubernetes label value rules)\n    VALUE_REGEX = re.compile(r'^[a-z0-9]([a-z0-9._-]*[a-z0-9])?$')\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a valid string\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n            continue\n            \n        # Check if key matches the regex\n        if not KEY_REGEX.match(key):\n            errors.append({key: f\"Label key '{key}' does not match the regex [a", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Tuple\n    \n    # Define regular expressions for label keys and values\n    # Label keys must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])? (as per Kubernetes label key rules)\n    KEY_REGEX = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    # Label values must match: [a-z0-9]([a-z0-9._-]*[a-z0-9])? (as per Kubernetes label value rules)\n    VALUE_REGEX = re.compile(r'^[a-z0-9]([a-z0-9._-]*[a-z0-9])?$')\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a valid string\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n            continue\n            \n        # Check if key matches the regex\n        if not KEY_REGEX.match(key):\n            errors.append({key: f\"Label key '{key}' does not match the regex [a", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Tuple\n    \n    # Regular expressions for label keys and values\n    # Label keys must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])? (as per Kubernetes label key rules)\n    KEY_REGEX = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    # Label values must match: [a-z0-9]([a-z0-9._-]*[a-z0-9])? (as per Kubernetes label value rules)\n    VALUE_REGEX = re.compile(r'^[a-z0-9]([a-z0-9._-]*[a-z0-9])?$')\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        if not KEY_REGEX.match(key):\n            errors.append({key: f\"Label key '{key}' does not match the regex [a-z0-9]([a-z0-9.-]*[a-z0-9])", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Tuple\n    \n    # Define regular expressions for label keys and values\n    # Label keys must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])? (as per Kubernetes label key rules)\n    KEY_REGEX = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    # Label values must match: [a-z0-9]([a-z0-9._-]*[a-z0-9])? (as per Kubernetes label value rules)\n    VALUE_REGEX = re.compile(r'^[a-z0-9]([a-z0-9._-]*[a-z0-9])?$')\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is valid\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not KEY_REGEX.match(key):\n            errors.append({key: f\"Label key '{key}' does not match the regex [a-z0-9]([a-z0-9.-]*["]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36596, "num_retrieved": 133, "inference_time": 5.063642740249634, "generate_results": ["def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*$\"\n    return re.compile(pattern)", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*$\"\n    return re.compile(pattern)", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    pattern = r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\"\n    return re.compile(pattern)", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*$\"\n    return re.compile(pattern)", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\"\n    return re.compile(pattern)"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35936, "num_retrieved": 114, "inference_time": 5.063642740249634, "generate_results": ["def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is likely defined in the context but not fully implemented\n    # Based on the context, it appears to be part of a validation system\n    # that uses regular expressions to validate values\n    # Since the actual regex pattern is not provided in the context,\n    # we implement a placeholder that would validate against a common pattern\n    # In a real implementation, this would use the specific regex pattern\n    # defined in the system\n    \n    # Example validation: ensure value is not empty and contains only alphanumeric characters\n    if not value:\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # Validate against a common pattern (alphanumeric with optional hyphens, underscores, dots)\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern\")", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is likely defined in the context but not fully implemented\n    # Based on the context, it appears to be part of a validation system\n    # that uses regular expressions to validate values\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string\")\n    \n    # Placeholder implementation - actual regex would be defined elsewhere\n    # in the validation system\n    # For example, if there's a predefined regex pattern for the value\n    # we would check against it here\n    return True", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is likely defined in the context but not fully implemented\n    # Based on the context, it appears to be part of a validation system\n    # that uses regular expressions to validate values\n    # Since the specific regex pattern is not provided in the context,\n    # we implement a placeholder that would validate against a common pattern\n    # In a real implementation, this would use the specific regex defined elsewhere\n    \n    # Example validation: ensure value is not empty and contains only alphanumeric characters\n    if not value:\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # Validate against a common pattern (alphanumeric with optional hyphens, underscores, dots)\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern\")\n    \n    return True", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is likely defined in the context but not fully implemented\n    # Based on the context, it appears to be part of a validation system\n    # that uses regular expressions to validate values\n    # Since the specific regex pattern is not provided in the context,\n    # we implement a placeholder that would validate against a common pattern\n    # such as alphanumeric with optional underscores and hyphens\n    \n    # Common validation patterns (example)\n    import re\n    \n    # Validate against a basic pattern: alphanumeric, underscores, hyphens, and dots\n    pattern = r'^[a-zA-Z0-9_\\-\\.]+$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern\")\n    \n    return True", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is likely defined in the context but not fully implemented\n    # Based on the context, it appears to be part of a validation system\n    # that uses regular expressions to validate values\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string\")\n    \n    # Placeholder implementation - actual regex would be defined elsewhere\n    # in the codebase based on specific validation rules\n    # For now, we just ensure it's a string and would validate against\n    # specific patterns defined in the system\n    return True"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35901, "num_retrieved": 113, "inference_time": 5.063642740249634, "generate_results": ["def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # This function is referenced in the context but not fully implemented\n    # The context shows it's used in constraint parsing but doesn't provide\n    # the actual regex validation logic\n    # Based on the context, it appears to validate key format according to a regex\n    # Since the actual regex pattern is not provided in the context, \n    # we implement a basic validation that checks for valid key format\n    if not isinstance(key, str):\n        raise ValidationError(\"Key must be a string\")\n    \n    # Basic validation: keys should not contain invalid characters\n    # This is a placeholder - actual implementation would depend on the specific regex\n    if not key or not key.strip():\n        raise ValidationError(\"Key cannot be empty or whitespace only\")\n    \n    # Check for invalid characters (common in key names)\n    # This is a simplified check - actual implementation would use the specific regex\n    if not all(c.isalnum() or c in '-_' for c in key):\n        raise ValidationError(\"Key contains invalid characters\")\n    \n    # Additional validation could be added based on specific requirements\n    return True", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # This function is referenced in the context but not fully implemented\n    # The context shows it's used in parse method of constraints.py\n    # However, no actual regex pattern or validation logic is provided\n    # Based on the context, we can infer it validates key format\n    if not key or not isinstance(key, str):\n        raise ValidationError(\"Key must be a non-empty string\")\n    \n    # Simple validation: check for valid key characters (alphanumeric, hyphen, underscore)\n    # This is a placeholder implementation based on typical key validation\n    if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # This function is referenced in the context but not fully implemented\n    # Based on the context, it appears to be part of a constraint parsing system\n    # that validates keys against a regular expression pattern\n    import re\n    \n    # Assuming a common key validation pattern (e.g., alphanumeric with hyphens and underscores)\n    # This is a placeholder implementation - actual pattern would depend on the specific requirements\n    pattern = r'^[a-zA-Z0-9_\\-]+$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}. Key must match pattern {pattern}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # This function is referenced in the context but not fully implemented\n    # Based on the context, it appears to be part of a constraint parsing system\n    # that validates keys against a regular expression pattern\n    import re\n    \n    # Assuming a common key validation pattern (e.g., alphanumeric with hyphens and underscores)\n    # This is a placeholder implementation - actual pattern would be defined elsewhere\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # This function is referenced in the context but not fully implemented\n    # The context shows it's used in parse method of constraints.py\n    # However, no actual regex pattern or validation logic is provided\n    # Based on the context, we can infer it validates key format\n    if not key or not isinstance(key, str):\n        raise ValidationError(\"Key must be a non-empty string\")\n    \n    # Simple validation: check for valid key characters (alphanumeric, hyphens, underscores)\n    # This is a placeholder implementation based on typical key validation\n    if not key.isalnum() and not any(c in key for c in \"-_\"):\n        raise ValidationError(f\"Invalid key format: {key}\")\n    \n    return True"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34521, "num_retrieved": 65, "inference_time": 5.063642740249634, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of\n    a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = result[\"metadata\"] = {}\n            metadata[\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except (KeyError, AssertionError):\n            pass\n            \n        # Remove other fields from manifest_dict that are not part of observer_schema\n        for key, value in manifest_dict.items():\n            if key not in [\"apiVersion\", \"kind\", \"metadata\"]:\n                if isinstance(value, dict):\n                    result[key] = generate_default_observer_schema_dict(value, first_level=False)\n                elif isinstance(value, list):\n                    result[key] = generate_default_observer_schema_list(value)\n                else:\n                    result[key] = None\n    else:\n        for key, value in manifest_dict.items():\n            if isinstance(value, dict):\n                result[key] = generate_default_observer_schema_dict(value, first_level=False)\n            elif isinstance(value, list):\n                result[key] = generate_default_observer_schema_list(value)\n            else:\n                result[key] = None\n                \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of\n    a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = result[\"metadata\"] = {}\n            metadata[\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except (KeyError, AssertionError):\n            pass\n            \n        # For other fields, replace non-list and non-dict values with None\n        for key, value in manifest_dict.items():\n            if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n                continue\n            if isinstance(value, dict):\n                result[key] = generate_default_observer_schema_dict(value, first_level=False)\n            elif isinstance(value, list):\n                result[key] = generate_default_observer_schema_list(value)\n            else:\n                result[key] = None\n    else:\n        for key, value in manifest_dict.items():\n            if isinstance(value, dict):\n                result[key] = generate_default_observer_schema_dict(value, first_level=False)\n            elif isinstance(value, list):\n                result[key] = generate_default_observer_schema_list(value)\n            else:\n                result[key] = None\n                \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of\n    a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = result[\"metadata\"] = {}\n            metadata[\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except (KeyError, AssertionError):\n            pass\n            \n        # Remove other fields from manifest_dict that are not part of observer_schema\n        for key, value in manifest_dict.items():\n            if key not in [\"apiVersion\", \"kind\", \"metadata\"]:\n                result[key] = None\n                \n    else:\n        # For non-first-level dictionaries, only copy metadata.name if present\n        try:\n            result[\"metadata\"] = {}\n            result[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except KeyError:\n            pass\n            \n        # Replace all non-list and non-dict values with None\n        for key, value in manifest_dict.items():\n            if isinstance(value, dict):\n                result[key] = generate_default_observer_schema_dict(value, first_level=False)\n            elif isinstance(value, list):\n                result[key] = generate_default_observer_schema_list(value)\n            else:\n                result[key] = None\n                \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of\n    a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = result[\"metadata\"] = {}\n            metadata[\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except (KeyError, AssertionError):\n            pass\n            \n        # Remove other fields from manifest_dict that are not part of observer_schema\n        for key, value in manifest_dict.items():\n            if key not in [\"apiVersion\", \"kind\", \"metadata\"]:\n                if isinstance(value, dict):\n                    result[key] = generate_default_observer_schema_dict(value, first_level=False)\n                elif isinstance(value, list):\n                    result[key] = generate_default_observer_schema_list(value)\n                else:\n                    result[key] = None\n    else:\n        for key, value in manifest_dict.items():\n            if isinstance(value, dict):\n                result[key] = generate_default_observer_schema_dict(value, first_level=False)\n            elif isinstance(value, list):\n                result[key] = generate_default_observer_schema_list(value)\n            else:\n                result[key] = None\n                \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of\n    a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = result[\"metadata\"] = {}\n            metadata[\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except (KeyError, AssertionError):\n            pass\n            \n        # Remove other fields from manifest_dict that are not part of observer_schema\n        for key, value in manifest_dict.items():\n            if key not in [\"apiVersion\", \"kind\", \"metadata\"]:\n                result[key] = None\n                \n    else:\n        # For non-first-level dictionaries, only copy metadata.name if present\n        try:\n            result[\"metadata\"] = {}\n            result[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n        except KeyError:\n            pass\n            \n        # Replace all non-list and non-dict values with None\n        for key, value in manifest_dict.items():\n            if isinstance(value, dict):\n                result[key] = generate_default_observer_schema_dict(value, first_level=False)\n            elif isinstance(value, list):\n                result[key] = generate_default_observer_schema_list(value)\n            else:\n                result[key] = None\n                \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34814, "num_retrieved": 72, "inference_time": 5.063642740249634, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        observed_resource = observer_schema[idx]\n        \n        # Check if the resource is already in last_applied_manifest\n        if any(\n            r.get(\"apiVersion\") == resource.get(\"apiVersion\") and\n            r.get(\"kind\") == resource.get(\"kind\") and\n            r.get(\"metadata\", {}).get(\"name\") == resource.get(\"metadata\", {}).get(\"name\") for\n            r in last_applied_manifest\n        ):\n            continue\n            \n        # If the resource is not present, add it to last_applied_manifest\n        last_applied_manifest.append(resource)\n        \n        # Recursively update nested dictionaries and lists\n        if isinstance(observed_resource, dict):\n            for key, value in observed_resource.items():\n                if isinstance(value, dict):\n                    # Recursively handle nested dictionaries\n                    pass\n                elif isinstance(value, list):\n                    # Recursively handle nested lists\n                    pass\n        elif isinstance(observed_resource, list):\n            # Handle list schema - this would typically involve processing each element\n            pass", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        observed_resource = observer_schema[idx]\n        \n        # Check if the resource is already in last_applied_manifest\n        if any(\n            r[\"apiVersion\"] == resource[\"apiVersion\"] and\n            r[\"kind\"] == resource[\"kind\"] and\n            r[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"] and\n            (not observed_resource.get(\"namespace\") or\n             r[\"metadata\"].get(\"namespace\") == observed_resource[\"namespace\"])\n            for r in last_applied_manifest\n        ):\n            continue\n            \n        # Add the resource to last_applied_manifest\n        last_applied_manifest.append(resource)\n        \n        # Recursively update nested dictionaries and lists\n        if isinstance(observed_resource, dict):\n            for key, value in observed_resource.items():\n                if isinstance(value, dict):\n                    # Recursively handle nested dictionaries\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )\n                elif isinstance(value, list):\n                    # Recursively handle nested lists\n                    update_last_applied_manifest_list_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        observed_resource = observer_schema[idx]\n        \n        # Check if the resource is already in last_applied_manifest\n        if any(\n            r[\"apiVersion\"] == resource[\"apiVersion\"] and\n            r[\"kind\"] == resource[\"kind\"] and\n            r[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"] and\n            (not observed_resource.get(\"namespace\") or\n             r[\"metadata\"].get(\"namespace\") == observed_resource[\"namespace\"])\n            for r in last_applied_manifest\n        ):\n            continue\n            \n        # Add the resource to last_applied_manifest\n        last_applied_manifest.append(resource)\n        \n        # Recursively update nested dictionaries and lists\n        if isinstance(observed_resource, dict):\n            for key, value in observed_resource.items():\n                if isinstance(value, dict):\n                    # Recursively process nested dictionaries\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )\n                elif isinstance(value, list):\n                    # Recursively process nested lists\n                    update_last_applied_manifest_list_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        observed_resource = observer_schema[idx]\n        \n        # Check if the resource is already in last_applied_manifest\n        if any(\n            r[\"apiVersion\"] == resource[\"apiVersion\"] and\n            r[\"kind\"] == resource[\"kind\"] and\n            r[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"] and\n            (not observed_resource.get(\"namespace\") or\n             r[\"metadata\"].get(\"namespace\") == observed_resource[\"namespace\"])\n            for r in last_applied_manifest\n        ):\n            continue\n            \n        # Add the resource to last_applied_manifest\n        last_applied_manifest.append(resource)\n        \n        # Recursively update nested dictionaries and lists\n        if isinstance(observed_resource, dict):\n            for key, value in observed_resource.items():\n                if isinstance(value, dict):\n                    # Recursively process nested dictionaries\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )\n                elif isinstance(value, list):\n                    # Recursively process nested lists\n                    update_last_applied_manifest_list_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        observed_resource = observer_schema[idx]\n        \n        # Check if the resource is already in last_applied_manifest\n        if any(\n            r[\"apiVersion\"] == resource[\"apiVersion\"] and\n            r[\"kind\"] == resource[\"kind\"] and\n            r[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"] and\n            (not observed_resource.get(\"namespace\") or\n             r[\"metadata\"].get(\"namespace\") == observed_resource.get(\"namespace\"))\n            for r in last_applied_manifest\n        ):\n            continue\n            \n        # Add the resource to last_applied_manifest\n        last_applied_manifest.append(resource)\n        \n        # Recursively update nested dictionaries and lists\n        if isinstance(observed_resource, dict):\n            for key, value in observed_resource.items():\n                if isinstance(value, dict):\n                    # Recursively process nested dictionaries\n                    update_last_applied_manifest_dict_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )\n                elif isinstance(value, list):\n                    # Recursively process nested lists\n                    update_last_applied_manifest_list_from_resp(\n                        last_applied_manifest, [observed_resource], response\n                    )"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34745, "num_retrieved": 70, "inference_time": 5.063642740249634, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, simply assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35721, "num_retrieved": 101, "inference_time": 4.249624341726303, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    if not app.spec.manifest:\n        return\n    \n    # Create a default observer schema entry for each resource in the manifest\n    # The default schema includes the resource's kind and name, and is structured\n    # to match the expected format for observer schema entries\n    observer_schema = []\n    \n    for manifest_resource in app.spec.manifest:\n        resource = {\n            \"apiVersion\": manifest_resource[\"apiVersion\"],\n            \"kind\": manifest_resource[\"kind\"],\n            \"metadata\": {\n                \"name\": manifest_resource[\"metadata\"][\"name\"]\n            }\n        }\n        observer_schema.append(resource)\n    \n    # Set the observer schema in the application's status\n    app.status.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    if not app.spec.manifest:\n        return\n    \n    # Create a list to hold the default observer schema entries\n    observer_schema = []\n    \n    for manifest_resource in app.spec.manifest:\n        # Extract the resource kind and name from the manifest\n        kind = manifest_resource[\"kind\"]\n        name = manifest_resource[\"metadata\"][\"name\"]\n        \n        # Create a default observer schema entry for this resource\n        # The default schema includes the resource's kind and name\n        default_entry = {\n            \"apiVersion\": manifest_resource[\"apiVersion\"],\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Check if a custom observer schema already exists for this resource\n        # If not, add the default entry to the observer schema\n        if not any(\n            resource.get(\"kind\") == kind and resource.get(\"metadata\", {}).get(\"name\") == name \n            for resource in app.status.last_applied_manifest or []\n        ):\n            observer_schema.append(default_entry)\n    \n    # Set the observer schema on the application\n    app.status.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    if not app.spec.manifest:\n        return\n    \n    # Create a default observer schema entry for each resource in the manifest\n    # The default schema includes the resource's kind and name, and is structured\n    # to match the expected format for observer schema entries\n    observer_schema = []\n    \n    for manifest_resource in app.spec.manifest:\n        resource = {\n            \"apiVersion\": manifest_resource[\"apiVersion\"],\n            \"kind\": manifest_resource[\"kind\"],\n            \"metadata\": {\n                \"name\": manifest_resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the resource to the observer schema\n        observer_schema.append(resource)\n    \n    # Set the observer schema on the application\n    app.status.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    if not app.spec.manifest:\n        return\n    \n    # Create a default observer schema entry for each resource in the manifest\n    # The default schema includes the resource's kind and name, and is structured\n    # to match the expected format for observer schema entries\n    observer_schema = []\n    \n    for manifest_resource in app.spec.manifest:\n        resource = {\n            \"apiVersion\": manifest_resource[\"apiVersion\"],\n            \"kind\": manifest_resource[\"kind\"],\n            \"metadata\": {\n                \"name\": manifest_resource[\"metadata\"][\"name\"]\n            }\n        }\n        # Add the resource to the observer schema\n        observer_schema.append(resource)\n    \n    # Set the observer schema on the application\n    app.status.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    if not app.spec.manifest:\n        return\n    \n    # Create a default observer schema entry for each resource in the manifest\n    # The default schema includes the resource's kind and name, and the namespace\n    # if specified in the manifest\n    default_schema = []\n    \n    for manifest_resource in app.spec.manifest:\n        resource = manifest_resource\n        \n        # Extract the resource metadata\n        metadata = resource.get(\"metadata\", {})\n        name = metadata.get(\"name\")\n        namespace = metadata.get(\"namespace\")\n        \n        if not name:\n            continue\n            \n        # Create a default observer schema entry\n        observer_entry = {\n            \"apiVersion\": resource.get(\"apiVersion\"),\n            \"kind\": resource.get(\"kind\"),\n            \"metadata\": {\n                \"name\": name,\n                \"namespace\": namespace if namespace else None\n            }\n        }\n        \n        default_schema.append(observer_entry)\n    \n    # Set the observer schema on the application\n    app.status.observer_schema = default_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 28364, "num_retrieved": 74, "inference_time": 4.249624341726303, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 28438, "num_retrieved": 74, "inference_time": 4.249624341726303, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34827, "num_retrieved": 94, "inference_time": 4.249624341726303, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.obj_fs = open_fs(path, create=False)\n            validator.validate(path)\n            return validator.log.num_errors == 0\n        except Exception as e:\n            logging.error(\"Validation failed for path %s: %s\", path, str(e))\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the root filesystem\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.validate(self.root_fs)\n            return True\n        except Exception as e:\n            return False\n    else:\n        # Validate specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            obj_fs = open_fs(path)\n            validator.validate(obj_fs)\n            return True\n        except Exception as e:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the root filesystem\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.validate(self.root_fs)\n            return True\n        except Exception as e:\n            return False\n    else:\n        # Validate specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            # Open the object filesystem\n            obj_fs = open_fs(path)\n            validator.obj_fs = obj_fs\n            validator.validate(path)\n            return True\n        except Exception as e:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.obj_fs = open_fs(path, create=False)\n            validator.validate(path)\n            return validator.log.num_errors == 0\n        except Exception as e:\n            logging.error(\"Validation failed for path %s: %s\", path, str(e))\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.obj_fs = open_fs(path, create=False)\n            return validator.validate(path)\n        except Exception as e:\n            logging.error(\"Failed to validate object at %s: %s\", path, str(e))\n            return False"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34903, "num_retrieved": 97, "inference_time": 4.249624341726303, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    if self.log is not None:\n        for level in ['error', 'warning']:\n            for msg in getattr(self.log, level, []):\n                messages.append(f\"{prefix}[{level}] {msg}\")\n    return '\\n'.join(messages) if messages else ''", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    if self.log is not None:\n        for level in ['error', 'warning']:\n            for msg in getattr(self.log, level, []):\n                messages.append(f\"{prefix}[{level}] {msg}\")\n    return '\\n'.join(messages) if messages else ''", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    if self.log is None:\n        return ''\n    messages = []\n    if self.log.num_errors > 0:\n        messages.append(f\"{prefix}ERRORS: {self.log.num_errors}\")\n    if self.log.num_warnings > 0:\n        messages.append(f\"{prefix}WARNINGS: {self.log.num_warnings}\")\n    if self.log.num_errors == 0 and self.log.num_warnings == 0:\n        messages.append(f\"{prefix}STATUS: VALID\")\n    return '\\n'.join(messages)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    if self.log is not None:\n        for level in ['error', 'warning']:\n            for msg in getattr(self.log, level, []):\n                messages.append(f\"{prefix}[{level}] {msg}\")\n    return '\\n'.join(messages) if messages else ''", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    if self.log is not None:\n        for level in ['error', 'warning']:\n            for msg in getattr(self.log, level, []):\n                messages.append(f\"{prefix}[{level}] {msg}\")\n    return '\\n'.join(messages) if messages else ''"]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34830, "num_retrieved": 94, "inference_time": 4.249624341726303, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34855, "num_retrieved": 95, "inference_time": 4.249624341726303, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34113, "num_retrieved": 65, "inference_time": 4.249624341726303, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.log.error('E002', where='root', explanation='missing id field')\n            return False\n        \n        if 'type' not in inventory:\n            self.log.error('E003', where='root', explanation='missing type field')\n            return False\n        \n        # Extract specification version from type field if requested\n        if extract_spec_version:\n            type_value = inventory['type']\n            if not type_value:\n                self.log.error('E004', where='root', explanation='type field is empty')\n                return False\n            \n            # Parse the type value to extract spec version\n            match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n            if match:\n                self.spec_version = match.group(1)\n            else:\n                self.log.error('E005', where='root', explanation='invalid type value format')\n                return False\n        \n        # Validate the digest algorithm\n        if 'digestAlgorithm' not in inventory:\n            self.log.error('E006', where='root', explanation='missing digestAlgorithm field')\n            return False\n        \n        digest_algorithm = inventory['digestAlgorithm']\n        if not digest_algorithm:\n            self.log.error('E007', where='root', explanation='digestAlgorithm field is empty')\n            return False\n        \n        # Validate manifest and versions blocks\n        if 'manifest' not in inventory:\n            self.log.error('E008', where='root', explanation='missing manifest field')\n            return False\n        \n        if 'versions' not in inventory:\n            self.log.error('E009', where='root', explanation='missing versions field')\n            return False\n        \n        # Validate manifest entries\n        for digest in inventory['manifest']:\n            if not isinstance(digest, str):\n                self.log.error('E010', where='root', explanation='manifest digest is not a string')\n                return False\n        \n        # Validate versions entries\n        for version in inventory['versions']:\n            if not isinstance", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.log.error('E002', where='root', explanation='missing id field')\n            return False\n        \n        if 'type' not in inventory:\n            self.log.error('E003', where='root', explanation='missing type field')\n            return False\n        \n        # Extract specification version from type field if requested\n        if extract_spec_version:\n            type_value = inventory['type']\n            if not type_value or not isinstance(type_value, str):\n                self.log.error('E004', where='root', explanation='invalid type field')\n                return False\n            \n            # Parse the type value to extract version\n            match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n            if match:\n                self.spec_version = match.group(1)\n            else:\n                self.log.error('E005', where='root', explanation='type value does not match expected format')\n                return False\n        else:\n            # Use the version from self.spec_version\n            pass\n        \n        # Validate manifest and versions blocks\n        if 'manifest' not in inventory:\n            self.log.error('E006', where='root', explanation='missing manifest block')\n            return False\n        \n        if 'versions' not in inventory:\n            self.log.error('E007', where='root', explanation='missing versions block')\n            return False\n        \n        # Validate manifest entries\n        for digest in inventory['manifest']:\n            if not isinstance(digest, str):\n                self.log.error('E008', where='root', explanation='manifest digest is not a string')\n                return False\n        \n        # Validate versions block\n        for version in inventory['versions']:\n            if not isinstance(version, str):\n                self.log.error('E009', where='root', explanation='version name is not a string')\n                return False\n            \n            version_data = inventory['versions'][version]\n            if not isinstance(version_data, dict):\n                self.log.error('E010", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.log.error('E002', where='root', explanation='missing id field')\n            return False\n        \n        if 'type' not in inventory:\n            self.log.error('E003', where='root', explanation='missing type field')\n            return False\n        \n        # Extract specification version from type field if requested\n        if extract_spec_version:\n            type_value = inventory['type']\n            if not type_value:\n                self.log.error('E004', where='root', explanation='type field is empty')\n                return False\n            \n            # Parse the type value to extract spec version\n            match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n            if match:\n                self.spec_version = match.group(1)\n            else:\n                self.log.error('E005', where='root', explanation='invalid type value format')\n                return False\n        \n        # Validate the digest algorithm if present\n        if 'digestAlgorithm' in inventory:\n            digest_algorithm = inventory['digestAlgorithm']\n            if not digest_algorithm:\n                self.log.error('E006', where='root', explanation='missing digestAlgorithm field')\n                return False\n            # Validate digest algorithm against known algorithms\n            valid_algorithms = ['sha512', 'sha256', 'sha1', 'md5', 'blake2b-512', 'blake2b-160', 'blake2b-256', 'blake2b-384', 'sha512-spec-ex', 'sha256-spec-ex']\n            if digest_algorithm not in valid_algorithms:\n                self.log.error('E007', where='root', explanation='invalid digest algorithm')\n                return False\n            self.digest_algorithm = digest_algorithm\n        else:\n            self.digest_algorithm = 'sha512'  # default if not specified\n        \n        # Validate manifest structure\n        if 'manifest' in inventory:", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        if extract_spec_version:\n            type_value = inventory.get('type')\n            if type_value:\n                # Extract version from type value (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.match(r'''https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory''', type_value)\n                if match:\n                    self.spec_version = match.group(1)\n                else:\n                    # If type value is invalid, fall back to self.spec_version\n                    self.spec_version = self.spec_version\n            else:\n                # No type value, fall back to self.spec_version\n                self.spec_version = self.spec_version\n        else:\n            # Use self.spec_version as is\n            pass\n        \n        # Validate the inventory structure and content\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        \n        return True\n        \n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        if extract_spec_version:\n            type_value = inventory.get('type')\n            if type_value:\n                # Extract version from type value (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.match(r'''https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory''', type_value)\n                if match:\n                    self.spec_version = match.group(1)\n                else:\n                    # If type value is invalid, fall back to self.spec_version\n                    self.spec_version = self.spec_version\n            else:\n                # If no type value, fall back to self.spec_version\n                self.spec_version = self.spec_version\n        else:\n            # Use the spec_version provided\n            pass\n        \n        # Validate the inventory structure and content\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        \n        return True\n        \n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34429, "num_retrieved": 81, "inference_time": 4.249624341726303, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for digest in manifest_files:\n        if digest not in digests_used:\n            self.log.error('E092c', where='root', digest_algorithm=digest, content_path=manifest_files[digest][0])", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for digest in manifest_files:\n        if digest not in digests_used:\n            self.log.error('E092c', where='root', digest_algorithm=digest, content_path=manifest_files[digest][0])", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34348, "num_retrieved": 75, "inference_time": 4.249624341726303, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=self.spec_version, \n                       prev_version_dir='prior', prev_spec_version=prior.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.warning('W011', where='prior', prior_digest=prior.digest_algorithm, \n                         current_digest=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=self.spec_version, \n                       prev_version_dir='prior', prev_spec_version=prior.spec_version)\n    # Check that the manifest in prior does not contain any files not present in current inventory\n    # This is a simplified check - in practice, we would need to compare the manifest entries\n    # and ensure that all files in prior are accounted for in the current inventory\n    # For now, we just ensure the spec version constraint is met", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=self.spec_version, \n                       prev_version_dir='prior', prev_spec_version=prior.spec_version)\n    # Check that the manifest in prior does not contain any files not present in current inventory\n    # This is a simplified check - in practice, we'd need to compare the manifest entries\n    # and ensure all files in prior are accounted for in the current inventory\n    # Since the spec requires prior versions to be valid, we assume the manifest is a subset\n    # of the current inventory's manifest", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=self.spec_version, \n                       prev_version_dir='prior', prev_spec_version=prior.spec_version)\n    # Check that the manifest in prior does not contain any files not present in current inventory\n    # This is a simplified check - in practice, we would need to compare the manifest entries\n    # and ensure that all files in prior's manifest are also in the current inventory\n    # For now, we just ensure the spec version constraint is met", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=self.spec_version, \n                       prev_version_dir='prior', prev_spec_version=prior.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E064', root_inv_file='inventory.json', inv_file='inventory.json.' + prior.digest_algorithm)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34299, "num_retrieved": 73, "inference_time": 4.249624341726303, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        return {}\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical file paths in the state\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the digest to the set of content files\n            # Note: The actual content file path is not directly available in the state\n            # We need to find the actual file path from the manifest\n            pass\n    \n    # We need to reconstruct the mapping from logical paths to actual content files\n    # This requires looking at the manifest to find the actual file paths\n    if 'manifest' not in inventory:\n        return {}\n    \n    manifest = inventory['manifest']\n    logical_path_map = {}\n    \n    for digest, file_paths in manifest.items():\n        for file_path in file_paths:\n            # The file_path is the logical path in the manifest\n            # We need to find which digest it corresponds to\n            # and which version it belongs to\n            pass\n    \n    # The actual implementation requires traversing the state and manifest\n    # to build the mapping from logical paths to content files\n    \n    # Since we don't have direct access to the content file paths in the state,\n    # we need to infer them from the manifest and state blocks\n    \n    # For each logical path in the state, we need to find the corresponding content file\n    # This is a complex mapping that requires understanding the versioning and deduping logic\n    \n    # Given the complexity and the lack of direct information in the provided context,\n    # we return an empty dictionary as a placeholder\n    \n    return {}", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        return {}\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical file paths\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the digest to the set of content files\n            # The content files are the actual file paths on disk\n            # We need to find the actual file paths from the manifest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        return {}\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical file paths in the state\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the digest to the set of content files\n            # Note: The actual content file path is not directly available in the state\n            # We need to find the corresponding content file from the manifest\n            pass\n    \n    # We need to reconstruct the mapping from logical paths to content files\n    # by looking at the manifest and state blocks\n    manifest = inventory['manifest']\n    \n    # Rebuild the mapping from logical paths to content files\n    logical_path_map = {}\n    \n    for digest, files in manifest.items():\n        for file_path in files:\n            # Find the corresponding state entry for this file\n            # This is a simplified version - in reality, we would need to track\n            # the actual content file paths from the state\n            pass\n    \n    # Since the exact implementation details are not fully provided in the context,\n    # we return an empty dictionary as a placeholder\n    return {}", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        return {}\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical file paths\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the digest to the set of content files\n            # The content files are the actual file paths on disk\n            # We need to find the actual file paths from the manifest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        return {}\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical paths in the state\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            # Add the content files (source files) to the set for this logical path\n            # Note: The actual content files are not directly available in the state\n            # We need to reconstruct this from the manifest and state information\n            # Since the state only contains the logical paths, we need to find the\n            # corresponding content files from the manifest\n            pass\n    \n    # We need to reconstruct the content files from the manifest\n    # The manifest contains the mapping from digests to logical paths\n    manifest = inventory['manifest']\n    \n    # Rebuild the map from the manifest and state\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        if digest not in manifest:\n            continue\n            \n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n                \n            # Find the corresponding content files in the manifest\n            # The manifest contains the mapping from digest to logical paths\n            # We need to find the source files for each logical path\n            for content_file in manifest[digest]:\n                logical_path_map[logical_path].add(content_file)\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34324, "num_retrieved": 74, "inference_time": 4.249624341726303, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate structure of fixity block\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate structure of fixity block\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate the structure of the fixity block\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E094', digest_algorithm=digest_algorithm)\n            return\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(fixity[digest_algorithm][digest], dict):\n                self.log.error('E095', digest_algorithm=digest_algorithm, digest=digest)\n                return\n                \n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E096', filepath=filepath, digest_algorithm=digest_algorithm, digest=digest)\n                    return", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate the structure of the fixity block\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E094', algorithm=digest_algorithm)\n            return\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(fixity[digest_algorithm][digest], dict):\n                self.log.error('E095', algorithm=digest_algorithm, digest=digest)\n                return\n                \n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E096', algorithm=digest_algorithm, digest=digest, filepath=filepath)\n                    return", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all fixity entries reference files in the manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38469, "num_retrieved": 196, "inference_time": 4.249624341726303, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n    elif file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zf:\n            return [os.path.join('zip', f) for f in zf.namelist() if not f.endswith('.xml')]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return file_utils.files_list(path)", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n    elif file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zf:\n            return [os.path.join('zip', f) for f in zf.namelist() if not f.endswith('.xml')]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n    elif file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zf:\n            return [os.path.join('zip', f) for f in zf.namelist() if not f.endswith('.xml')]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n    elif file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zf:\n            return [os.path.join('zip', f) for f in zf.namelist() if not f.endswith('.xml')]\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36176, "num_retrieved": 118, "inference_time": 4.249624341726303, "generate_results": ["def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Get the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_basename.endswith(\".xml\"):\n            # Extract the basename of the XML file\n            xml_file_basename = os.path.splitext(file_basename)[0]\n            \n            # Group files by XML basename\n            if xml_file_basename == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Get the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Extract the prefix from the file name (before the extension)\n        file_prefix = os.path.splitext(file_basename)[0]\n        \n        # Check if the file's prefix matches the XML basename\n        if file_prefix == xml_basename:\n            # If it matches, add the file to the corresponding XML basename group\n            if xml_basename not in grouped_files:\n                grouped_files[xml_basename] = []\n            grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Get the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Extract the prefix from the file name (before the extension)\n        file_prefix = os.path.splitext(file_basename)[0]\n        \n        # Check if the file's prefix matches the XML basename\n        if file_prefix == xml_basename:\n            # If it matches, add the file to the corresponding group\n            if xml_basename not in grouped_files:\n                grouped_files[xml_basename] = []\n            grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Extract the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_basename.endswith(\".xml\"):\n            # Extract the basename of the XML file\n            xml_file_basename = os.path.splitext(file_basename)[0]\n            \n            # Group the file under the corresponding XML basename\n            if xml_file_basename == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Extract the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_basename.endswith(\".xml\"):\n            # Extract the basename of the XML file (without extension)\n            xml_file_basename = os.path.splitext(file_basename)[0]\n            \n            # Group the file under the corresponding XML basename\n            if xml_file_basename == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36324, "num_retrieved": 116, "inference_time": 4.249624341726303, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns `True` for files belonging to a document package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns `True` for files belonging to a document package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns `True` for files belonging to a document package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns `True` for files belonging to a document package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns `True` for files belonging to a document package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix))"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36310, "num_retrieved": 114, "inference_time": 4.249624341726303, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36921, "num_retrieved": 141, "inference_time": 4.833201557397842, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = [f for f in folder_path.iterdir() if f.is_file()]\n    \n    # Group files by their XML basename\n    for file in files:\n        if file.suffix.lower() == '.xml':\n            # Extract basename without extension\n            xml_basename = file.stem\n            package_data[xml_basename] = []\n            \n            # Find all files with the same basename\n            for other_file in files:\n                if other_file.suffix.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.tif', '.tiff']:\n                    other_basename = other_file.stem\n                    if other_basename.startswith(xml_basename):\n                        package_data[xml_basename].append({\n                            'name': other_basename,\n                            'path': str(other_file)\n                        })\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Filter XML files and get their base names\n    xml_files = [f for f in files if f.is_file() and f.suffix.lower() == '.xml']\n    \n    for xml_file in xml_files:\n        xml_basename = xml_file.stem\n        \n        # Find all files that match the XML basename\n        matching_files = []\n        for file in files:\n            if file.is_file() and file.suffix.lower() != '.xml':\n                # Check if the filename matches the XML basename\n                if file.stem == xml_basename:\n                    matching_files.append(str(file))\n        \n        # Add to package data\n        package_data[xml_basename] = matching_files\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Group files by their XML basename\n    for file in files:\n        if file.is_file():\n            # Get the file extension\n            ext = file.suffix.lower()\n            \n            # If it's an XML file, extract the basename\n            if ext == '.xml':\n                basename = file.stem\n                package_data[basename] = {\n                    'xml': str(file),\n                    'files': []\n                }\n                \n                # Look for related files (renditions, assets) with the same basename\n                for other_file in files:\n                    if other_file.is_file() and other_file != file:\n                        other_ext = other_file.suffix.lower()\n                        other_basename = other_file.stem\n                        \n                        # Check if the basename matches (with possible suffixes)\n                        if other_basename == basename:\n                            # Check if it's a rendition or asset\n                            if other_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif']:\n                                package_data[basename]['files'].append(str(other_file))\n            \n            # If it's a zip file, extract the XML basename from the filename\n            elif ext == '.zip':\n                zip_basename = file.stem\n                # Extract the XML basename from the zip filename\n                xml_basename = zip_basename.split('-')[0] if '-' in zip_basename else zip_basename\n                \n                # Look for XML files with the same basename\n                for xml_file in files:\n                    if xml_file.is_file() and xml_file.suffix.lower() == '.xml':\n                        xml_basename_file = xml_file.stem\n                        if xml_basename_file == xml_basename:\n                            package_data[xml_basename] = {\n                                'xml': str(xml_file),\n                                'files': []\n                            }\n                            # Look for related files with the same basename\n                            for other_file in files:\n                                if other_file.is_file() and other_file != xml_file:\n                                    other_ext = other_file.suffix.lower()\n                                    other_basename = other_file.stem\n                                    if other_basename == xml_basename:\n                                        if other_ext in ['.", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to group files by XML basename\n    packages_data = defaultdict(list)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    for file in files:\n        file_path = os.path.join(folder, file)\n        \n        # Check if it's a file (not a directory)\n        if os.path.isfile(file_path):\n            # Get the file extension\n            _, ext = os.path.splitext(file)\n            \n            # Only process XML files\n            if ext.lower() == '.xml':\n                # Extract basename (without extension) to group by XML\n                basename = os.path.splitext(file)[0]\n                \n                # Add the file path to the corresponding XML basename group\n                packages_data[basename].append(file_path)\n            elif ext.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.tiff']:\n                # For other file types, we can consider them as renditions or assets\n                # We'll add them to the group of the XML they belong to\n                # This requires knowing the XML basename, which we don't have here\n                # So we'll skip them for now or add a more sophisticated logic\n                pass\n    \n    # Convert defaultdict to regular dict\n    return dict(packages_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = [f for f in folder_path.iterdir() if f.is_file()]\n    \n    # Group files by their XML basename\n    for file in files:\n        if file.suffix.lower() == '.xml':\n            # Extract basename without extension\n            xml_basename = file.stem\n            package_data[xml_basename] = []\n            \n            # Find all files with the same basename\n            for f in files:\n                if f.suffix.lower() in ['.xml', '.pdf', '.jpg', '.jpeg', '.png', '.gif', '.tif', '.tiff']:\n                    # Check if the filename matches the XML basename\n                    if f.stem == xml_basename:\n                        package_data[xml_basename].append(str(f))\n    \n    return package_data"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36192, "num_retrieved": 107, "inference_time": 4.833201557397842, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37471, "num_retrieved": 161, "inference_time": 4.833201557397842, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == \"en\":\n        rendition_dict[\"en\"] = file_path\n    else:\n        rendition_dict[\"original\"] = file_path\n    \n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == \"en\":\n        rendition_dict[\"en\"] = file_path\n    else:\n        rendition_dict[\"original\"] = file_path\n    \n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == \"en\":\n        rendition_dict[\"en\"] = file_path\n    else:\n        rendition_dict[\"original\"] = file_path\n    \n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    \n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    \n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    \n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    \n    return rendition_dict"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38611, "num_retrieved": 201, "inference_time": 4.833201557397842, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    asset_dict = {}\n    base_name, ext = os.path.splitext(basename)\n    \n    # Add the original file path with the same basename and extension\n    asset_dict[basename] = file_path\n    \n    # Add variations with common image extensions\n    common_extensions = ['.tiff', '.tif', '.jpg', '.jpeg', '.png']\n    for ext in common_extensions:\n        if ext != ext.lower():  # Ensure we don't add duplicates\n            new_basename = f\"{base_name}{ext}\"\n            if new_basename != basename:\n                asset_dict[new_basename] = file_path\n    \n    return asset_dict", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    asset_dict = {}\n    base_name, ext = os.path.splitext(basename)\n    \n    # Add the original file path with the same basename and extension\n    asset_dict[basename] = file_path\n    \n    # Add alternative extensions if they exist\n    for alt_ext in ['.tiff', '.tif', '.jpg', '.jpeg', '.png']:\n        if alt_ext != ext:\n            alt_basename = f\"{base_name}{alt_ext}\"\n            asset_dict[alt_basename] = file_path\n    \n    return asset_dict", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(file_path)[1].lower()\n    if ext not in ['.tiff', '.tif', '.jpg', '.jpeg', '.png']:\n        return\n    \n    # Create a canonical name based on basename and extension\n    canonical_name = f\"{basename}{ext}\"\n    \n    # Store the mapping from canonical name to file path\n    self._assets[canonical_name] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    asset_dict = {}\n    base_name, ext = os.path.splitext(basename)\n    \n    # Add the original file path with its extension\n    asset_dict[basename] = file_path\n    \n    # Add alternative extensions if they exist\n    for alt_ext in ['.tiff', '.tif', '.jpg', '.jpeg', '.png']:\n        if alt_ext != ext:\n            alt_basename = f\"{base_name}{alt_ext}\"\n            asset_dict[alt_basename] = file_path\n    \n    return asset_dict", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    asset_dict = {}\n    base_name, ext = os.path.splitext(basename)\n    \n    # Add the original file with its extension\n    asset_dict[basename] = file_path\n    \n    # Add alternative extensions if they exist\n    for alt_ext in ['.tiff', '.tif', '.jpg', '.jpeg', '.png']:\n        if alt_ext != ext:\n            alt_basename = f\"{base_name}{alt_ext}\"\n            asset_dict[alt_basename] = file_path\n    \n    return asset_dict"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37238, "num_retrieved": 153, "inference_time": 4.833201557397842, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        files = zf.namelist()\n        for file in files:\n            if file.endswith(\".xml\"):\n                basename = os.path.splitext(file)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file)\n            elif file.endswith(\".pdf\") or file.endswith(\".tif\") or file.endswith(\".jpg\"):\n                # Extract the XML basename from the file path (e.g., \"1414-431X-bjmbr-54-10-e11439.xml\" from \"1414-431X-bjmbr-54-10-e11439.pdf\")\n                xml_basename = os.path.splitext(file)[0]\n                if xml_basename not in data:\n                    data[xml_basename] = []\n                data[xml_basename].append(file)\n    \n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n        \n        for filename in file_list:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    \n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n        \n        for filename in file_list:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    \n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n        \n        for filename in file_list:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    \n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n        \n        for filename in file_list:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    \n    return data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38006, "num_retrieved": 182, "inference_time": 4.833201557397842, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            if file_info.file_size > 0:\n                files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            if file_info.file_size > 0:\n                files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            if file_info.filename:\n                files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            if file_info.file_size > 0:\n                files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            if file_info.file_size > 0:\n                files.append(file_info.filename)\n    return files"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39714, "num_retrieved": 232, "inference_time": 4.833201557397842, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38244, "num_retrieved": 187, "inference_time": 4.833201557397842, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37929, "num_retrieved": 177, "inference_time": 4.833201557397842, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by spaces or other common separators\n    # The issue might contain volume, number, and supplement information\n    parts = issue.strip().split()\n    \n    # Look for number and supplement\n    number = 0\n    supplement = 0\n    \n    for part in parts:\n        # Check if the part is a number (volume or number)\n        if part.isdigit():\n            # Try to determine if it's the number or supplement\n            # If it's the last part and not a volume, it might be the number\n            if number == 0:\n                number = int(part)\n            else:\n                # If we already have a number, this might be the supplement\n                supplement = int(part)\n        elif part.startswith('n'):\n            # Handle cases like \"n.1\" or \"n1\"\n            if len(part) > 1 and part[1:].isdigit():\n                number = int(part[1:])\n        elif part.startswith('suppl'):\n            # Handle supplement cases like \"suppl1\"\n            if len(part) > 6 and part[6:].isdigit():\n                supplement = int(part[6:])\n    \n    # If no number was found, try to extract from the original issue string\n    if number == 0:\n        # Look for patterns like \"n.1\" or \"1\"\n        import re\n        match = re.search(r'n\\.(\\d+)', issue)\n        if match:\n            number = int(match.group(1))\n        else:\n            match = re.search(r'(\\d+)', issue)\n            if match:\n                number = int(match.group(1))\n    \n    # If no supplement was found, try to extract from the original issue string\n    if supplement == 0:\n        import re\n        match = re.search(r'suppl(\\d+)', issue, re.IGNORECASE)\n        if match:\n            supplement = int(match.group(1))\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by spaces and check for number and supplement\n    parts = issue.strip().split()\n    \n    # Try to extract number (first numeric value)\n    number = 0\n    supplement = 0\n    \n    for part in parts:\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Check if there's a supplement (e.g., \"suppl1\", \"s1\", \"s12\")\n    supplement_part = None\n    for part in parts:\n        if part.lower().startswith('suppl') or part.lower().startswith('s'):\n            supplement_part = part\n            break\n    \n    if supplement_part:\n        # Extract the number after 'suppl' or 's'\n        supplement_match = re.search(r'(suppl|s)(\\d+)', supplement_part)\n        if supplement_match:\n            supplement = int(supplement_match.group(2))\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by spaces or other common separators\n    # The number is typically the first part, and supplement is the second part if present\n    parts = issue.strip().split()\n    \n    # Try to extract number (first part)\n    number = 0\n    supplement = 0\n    \n    # Check if the issue contains a supplement (e.g., \"4(suppl)\", \"4s2\", \"4s\", \"4(suppl)\")\n    suppl_match = None\n    for part in parts:\n        if part.endswith('suppl') or part.endswith('s'):\n            # Extract the number before the supplement\n            if len(part) > 1:\n                # Try to extract the number before 's' or 'suppl'\n                num_part = part[:-3] if part.endswith('suppl') else part[:-1]\n                if num_part.isdigit():\n                    number = int(num_part)\n                    supplement = 1\n            else:\n                # If it's just 's' or 'suppl', assume it's the supplement\n                supplement = 1\n        elif part.isdigit():\n            number = int(part)\n    \n    # If no number was found, try to extract from the first part\n    if number == 0 and parts:\n        number = int(parts[0])\n    \n    # If supplement was not found, try to extract from the issue string\n    if supplement == 0:\n        # Check if there's a 's' or 'suppl' in the issue\n        if 's' in issue or 'suppl' in issue:\n            supplement = 1\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by spaces or other common separators\n    # The issue element typically contains volume, number, and supplement information\n    # We look for number and supplement parts\n    parts = issue.strip().split()\n    \n    number = 0\n    supplement = 0\n    \n    # Try to extract number (usually a number, sometimes with 'n.' or 'no.')\n    for part in parts:\n        # Check if part contains a number\n        if part.endswith('n') or part.endswith('no'):\n            # Extract the number before 'n' or 'no'\n            num_part = part[:-1]\n            if num_part.isdigit():\n                number = int(num_part)\n        elif part.isdigit():\n            number = int(part)\n    \n    # Try to extract supplement (usually a number after 'suppl' or 'suppl.')\n    for part in parts:\n        if part.lower().startswith('suppl') or part.lower().startswith('suppl.'):\n            # Extract the number after 'suppl' or 'suppl.'\n            suppl_part = part[5:].strip()\n            if suppl_part.isdigit():\n                supplement = int(suppl_part)\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by spaces and check for number and supplement\n    parts = issue.strip().split()\n    \n    # Try to extract number (first number in the string)\n    number = 0\n    supplement = 0\n    \n    for part in parts:\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Check if there's a supplement (e.g., \"suppl1\", \"s1\", \"s12\")\n    supplement_part = None\n    for part in parts:\n        if part.lower().startswith('suppl') or part.lower().startswith('s'):\n            supplement_part = part\n            break\n    \n    if supplement_part:\n        # Extract the number after 'suppl' or 's'\n        supplement_match = re.search(r'(suppl|s)(\\d+)', supplement_part)\n        if supplement_match:\n            supplement = int(supplement_match.group(2))\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 4.833201557397842, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 4.833201557397842, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            simplified = expr.simplify()\n            result.append(simplified)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # Check if expression is of form A & (~A | B) or A | (~A & B)\n        \n        # Simplify using the absorption laws\n        if isinstance(expr, Expression):\n            # Simplify using the absorption laws\n            # This is a simplified implementation that applies the rules directly\n            # In a full implementation, we would need to parse the expression tree\n            # and apply the rules recursively\n            \n            # For now, we just return the expression as is\n            # A full implementation would require parsing and tree traversal\n            pass\n    \n    # Return a new list of expressions after applying absorption rules\n    # This is a placeholder - actual implementation would require deep parsing\n    return args", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # These simplifications are typically handled by the Expression class\n            # We assume the Expression class has methods to handle these simplifications\n            simplified = expr.simplify()\n            if simplified is not None:\n                result.append(simplified)\n            else:\n                result.append(expr)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we return the expression as is\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation that assumes the expression\n            # is in a form where we can apply the rules directly\n            # In a full implementation, we would parse the expression tree\n            # and apply the rules recursively\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36425, "num_retrieved": 131, "inference_time": 4.833201557397842, "generate_results": ["def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.handlers[hook].append(handler)\n        return handler\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35605, "num_retrieved": 106, "inference_time": 4.833201557397842, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35416, "num_retrieved": 99, "inference_time": 4.833201557397842, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35488, "num_retrieved": 102, "inference_time": 4.833201557397842, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    # by rounding to the nearest integer and adjusting the value accordingly\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer\n                rounded_value = round(value)\n                # If the original value was fractional, adjust the value\n                # to maintain the same relative offset\n                if abs(value - rounded_value) > 1e-9:\n                    # Adjust the value to maintain the same relative offset\n                    # This is a simplified approach - in practice, this would\n                    # need to be handled more carefully based on the specific\n                    # attributes and their relationships\n                    pass\n            else:\n                # For integer values, just copy them\n                setattr(result, attr, value)\n    \n    # For the specific case of days and hours, we need to handle the conversion\n    # properly to maintain the same relative offset\n    if hasattr(self, 'days') and self.days is not None:\n        days = int(self.days)\n        if self.days != days:\n            # Adjust hours to maintain the same relative offset\n            hours = int(self.hours) if hasattr(self, 'hours') else 0\n            hours += (self.days - days) * 24\n            setattr(result, 'hours', hours)\n    \n    # Return the normalized object\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                # Handle days with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (weeks)\n                    weeks = int(value // 7)\n                    days = int(value) % 7\n                    result.days = days\n                    result.weeks = weeks\n                else:\n                    result.days = int(value)\n            elif attr == 'hours':\n                # Handle hours with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (days)\n                    days = int(value // 24)\n                    hours = int(value) % 24\n                    result.hours = hours\n                    result.days = days\n                else:\n                    result.hours = int(value)\n            elif attr == 'minutes':\n                # Handle minutes with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (hours)\n                    hours = int(value // 60)\n                    minutes = int(value) % 60\n                    result.minutes = minutes\n                    result.hours = hours\n                else:\n                    result.minutes = int(value)\n            elif attr == 'seconds':\n                # Handle seconds with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (minutes)\n                    minutes = int(value // 60)\n                    seconds = int(value) % 60\n                    result.seconds = seconds\n                    result.minutes = minutes\n                else:\n                    result.seconds = int(value)\n            else:\n                # For other attributes, just convert to integer\n                result.__dict__[attr] = int(value)\n    \n    # Ensure all values are positive", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    # by adding the appropriate integer part and adjusting the fractional part\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                int_val = int(value)\n                frac_val = value - int_val\n                # If there's a fractional part, add it to the integer part\n                if frac_val > 0:\n                    # Add the fractional part to the integer value\n                    # For example, days=1.5 becomes days=1 + days=0.5\n                    # We need to convert the fractional part to the appropriate unit\n                    if attr == 'days':\n                        # Convert fractional days to hours\n                        hours = int(frac_val * 24)\n                        # Add the hours to the hours attribute\n                        hours_val = getattr(self, 'hours', 0) + hours\n                        # Set the hours attribute\n                        setattr(result, 'hours', hours_val)\n                        # Set the days attribute to the integer part\n                        setattr(result, 'days', int_val)\n                    elif attr == 'hours':\n                        # Convert fractional hours to minutes\n                        minutes = int(frac_val * 60)\n                        # Add the minutes to the minutes attribute\n                        minutes_val = getattr(self, 'minutes', 0) + minutes\n                        # Set the minutes attribute\n                        setattr(result, 'minutes', minutes_val)\n                        # Set the hours attribute to the integer part\n                        setattr(result, 'hours', int_val)\n                    elif attr == 'minutes':\n                        # Convert fractional minutes to seconds\n                        seconds = int(frac_val * 60)\n                        # Add the seconds to the seconds attribute\n                        seconds_val = getattr(self, 'seconds', 0) + seconds\n                        # Set the seconds attribute\n                        setattr(result, 'seconds', seconds_val)\n                        # Set the", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                # Handle days with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (weeks)\n                    weeks = int(value // 7)\n                    days = int(value) % 7\n                    result.days = days\n                    result.weeks = weeks\n                else:\n                    result.days = int(value)\n            elif attr == 'hours':\n                # Handle hours with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (days)\n                    days = int(value // 24)\n                    hours = int(value) % 24\n                    result.hours = hours\n                    result.days = days\n                else:\n                    result.hours = int(value)\n            elif attr == 'minutes':\n                # Handle minutes with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (hours)\n                    hours = int(value // 60)\n                    minutes = int(value) % 60\n                    result.minutes = minutes\n                    result.hours = hours\n                else:\n                    result.minutes = int(value)\n            elif attr == 'seconds':\n                # Handle seconds with fractional part\n                if value != int(value):\n                    # Add the fractional part to the next unit (minutes)\n                    minutes = int(value // 60)\n                    seconds = int(value) % 60\n                    result.seconds = seconds\n                    result.minutes = minutes\n                else:\n                    result.seconds = int(value)\n            else:\n                # For other attributes, just convert to integer\n                result.__dict__[attr] = int(value)\n    \n    # Ensure all values are positive", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'months', 'weeks', 'years']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Handle fractional values by converting to integer\n            if attr == 'days':\n                # Convert fractional days to whole days and remaining hours\n                whole_days = int(value)\n                fractional_days = value - whole_days\n                remaining_hours = int(fractional_days * 24)\n                result.days = whole_days\n                if remaining_hours > 0:\n                    result.hours = remaining_hours\n            elif attr == 'hours':\n                # Convert fractional hours to whole hours\n                whole_hours = int(value)\n                fractional_hours = value - whole_hours\n                remaining_minutes = int(fractional_hours * 60)\n                result.hours = whole_hours\n                if remaining_minutes > 0:\n                    result.minutes = remaining_minutes\n            elif attr == 'minutes':\n                # Convert fractional minutes to whole minutes\n                whole_minutes = int(value)\n                fractional_minutes = value - whole_minutes\n                remaining_seconds = int(fractional_minutes * 60)\n                result.minutes = whole_minutes\n                if remaining_seconds > 0:\n                    result.seconds = remaining_seconds\n            elif attr == 'seconds':\n                # Convert fractional seconds to whole seconds\n                whole_seconds = int(value)\n                fractional_seconds = value - whole_seconds\n                # No need to convert further as seconds are already integer\n                result.seconds = whole_seconds\n            else:\n                # For other attributes, just use the integer value\n                result.__dict__[attr] = int(value)\n    \n    # Ensure all values are positive (using the sign from original)\n    if hasattr(self, 'days') and self.days < 0:\n        result.days = -result.days\n    if hasattr(self, 'hours') and self.hours < 0:\n        result.hours = -result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35586, "num_retrieved": 104, "inference_time": 4.833201557397842, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original function\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original function\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 16408, "num_retrieved": 60, "inference_time": 4.463354974985123, "generate_results": ["def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 13626, "num_retrieved": 43, "inference_time": 4.463354974985123, "generate_results": ["def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 16253, "num_retrieved": 58, "inference_time": 4.463354974985123, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10252, "num_retrieved": 29, "inference_time": 4.463354974985123, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        if result.returncode == 0:\n            if verbose:\n                print(\"Command succeeded\")\n            return result.stdout.strip() if result.stdout else None\n        else:\n            if not hide_stderr:\n                print(\"Command failed with return code:\", result.returncode)\n                if result.stderr:\n                    print(\"Error output:\", result.stderr)\n            return None\n            \n    except FileNotFoundError:\n        print(f\"Command not found: {' '.join(cmd)}\")\n        return None\n    except Exception as e:\n        print(f\"Error executing command: {str(e)}\")\n        return None", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return result\n        if result.returncode == 0:\n            return {\n                'returncode': 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr\n            }\n        else:\n            return {\n                'returncode': result.returncode,\n                'stdout': result.stdout,\n                'stderr': result.stderr\n            }\n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if not hide_stderr:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if not hide_stderr:\n            print(str(e), file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose or if there's output\n        if verbose and result.stdout:\n            print(result.stdout)\n            \n        # Return the result\n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        if result.returncode == 0:\n            if verbose:\n                print(\"Command succeeded\")\n            return result.stdout.strip() if result.stdout else None\n        else:\n            if not hide_stderr:\n                print(\"Command failed with return code:\", result.returncode)\n                if result.stderr:\n                    print(\"Error output:\", result.stderr)\n            else:\n                print(\"Command failed with return code:\", result.returncode)\n            return None\n            \n    except subprocess.SubprocessError as e:\n        print(\"Subprocess error:\", str(e))\n        return None"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 16221, "num_retrieved": 59, "inference_time": 4.463354974985123, "generate_results": ["def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 12606, "num_retrieved": 37, "inference_time": 4.463354974985123, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34287, "num_retrieved": 70, "inference_time": 4.463354974985123, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all directories in the root that might be OCFL objects\n    try:\n        entries = self.obj_fs.listdir('')\n    except Exception as e:\n        self.log.error('E001c', entry=str(e))\n        return num_objects, good_objects\n    \n    for entry in entries:\n        entry_path = fs.path.join('', entry)\n        # Check if this is an OCFL object root\n        if entry.startswith('v'):\n            continue  # Skip version directories\n            \n        # Check if this is a potential OCFL object\n        if entry in ('inventory.json', 'inventory.json.sha512'):\n            continue  # Skip inventory files\n            \n        # Check if this is a valid OCFL object root\n        try:\n            # Validate the object root structure\n            validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n            validator.obj_fs = self.obj_fs\n            \n            # Validate the object at this path\n            if validate_objects:\n                passed = validator.validate(entry_path)\n                if passed:\n                    good_objects += 1\n            \n            num_objects += 1\n            \n        except Exception as e:\n            self.log.error('E001c', entry=entry)\n            num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all root directories in the storage root\n    try:\n        entries = self.obj_fs.listdir('')\n    except fs.errors.ResourceNotFound:\n        return num_objects, good_objects\n        \n    for entry in entries:\n        entry_path = fs.path.join('', entry)\n        try:\n            entry_info = self.obj_fs.getinfo(entry_path)\n            if entry_info.is_dir:\n                # Check if this is an OCFL object\n                if entry == 'inventory.json' or entry == 'inventory.json.' + self.digest_algorithm:\n                    continue\n                    \n                # Check if this is a version directory\n                if re.match(r'''v\\d+$''', entry):\n                    continue\n                    \n                # Check if this is an extensions directory\n                if entry == 'extensions':\n                    self.validate_extensions_dir()\n                    continue\n                    \n                # Check if this is an object declaration file\n                if re.match(r'''0=ocfl_object_\\d+\\.\\d+''', entry):\n                    # This is an object declaration, validate the object\n                    if validate_objects:\n                        validator = Validator(show_warnings=show_warnings, \n                                             show_errors=True, \n                                             check_digests=check_digests,\n                                             lax_digests=self.lax_digests)\n                        try:\n                            validator.validate(entry_path)\n                            if validator.log.num_errors == 0:\n                                good_objects += 1\n                        except Exception as e:\n                            pass\n                    num_objects += 1\n        except fs.errors.ResourceNotFound:\n            continue\n            \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all directories in the root that might be OCFL objects\n    try:\n        entries = self.obj_fs.listdir('')\n    except Exception as e:\n        self.log.error('E001c', entry=str(e))\n        return num_objects, good_objects\n    \n    for entry in entries:\n        entry_path = fs.path.join('', entry)\n        # Check if this is an OCFL object root\n        if entry.startswith('v') or entry == 'inventory.json' or entry == 'inventory.json.' + self.digest_algorithm:\n            continue\n            \n        # Check if this is a potential OCFL object\n        try:\n            info = self.obj_fs.getinfo(entry_path)\n            if info.is_dir:\n                # Try to validate this as an OCFL object\n                try:\n                    # Check if this directory has a valid OCFL structure\n                    validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests, lax_digests=self.lax_digests)\n                    validator.obj_fs = self.obj_fs\n                    validator.validate(entry_path)\n                    \n                    # If validation passed, increment good objects\n                    if validator.log.num_errors == 0:\n                        good_objects += 1\n                    num_objects += 1\n                except Exception as e:\n                    self.log.error('E001c', entry=entry_path, explanation=str(e))\n                    num_objects += 1\n        except Exception as e:\n            self.log.error('E001c', entry=entry_path, explanation=str(e))\n            num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all directories in the root that might be OCFL objects\n    try:\n        entries = self.obj_fs.listdir('')\n    except fs.errors.ResourceNotFound:\n        return num_objects, good_objects\n        \n    for entry in entries:\n        entry_path = fs.path.join('', entry)\n        # Check if this is an OCFL object root\n        if entry.startswith('v') or entry == 'inventory.json' or entry == 'inventory.json.' + self.digest_algorithm:\n            continue\n            \n        # Check if this is a potential OCFL object root by looking for 0= Namaste files\n        try:\n            namastes = find_namastes(0, pyfs=self.obj_fs, dir=entry_path)\n            if len(namastes) > 0:\n                # This is an OCFL object root\n                num_objects += 1\n                validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests, lax_digests=self.lax_digests)\n                try:\n                    validator.validate(entry_path)\n                    if validator.log.num_errors == 0:\n                        good_objects += 1\n                except ValidatorAbortException:\n                    pass\n        except Exception as e:\n            # Skip if validation fails\n            continue\n            \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all directories in the root that might be OCFL objects\n    try:\n        entries = self.obj_fs.listdir('')\n    except Exception as e:\n        self.log.error('E001c', entry=str(e))\n        return num_objects, good_objects\n    \n    for entry in entries:\n        entry_path = fs.path.join('', entry)\n        # Check if this is an OCFL object root\n        if entry.startswith('v') or entry == 'inventory.json' or entry == 'inventory.json.' + self.digest_algorithm:\n            continue\n            \n        # Check if this is a potential OCFL object\n        try:\n            # Try to open the directory to see if it's a valid OCFL object\n            if self.obj_fs.isdir(entry_path):\n                # Check if it has a valid 0= Namaste file\n                namastes = find_namastes(0, pyfs=self.obj_fs, dir=entry_path)\n                if len(namastes) > 0:\n                    num_objects += 1\n                    # Validate the object if requested\n                    if validate_objects:\n                        validator = Validator(show_warnings=show_warnings, \n                                             show_errors=True, \n                                             check_digests=check_digests,\n                                             lax_digests=self.lax_digests)\n                        try:\n                            validator.validate(entry_path)\n                            if validator.log.num_errors == 0:\n                                good_objects += 1\n                        except Exception as e:\n                            self.log.error('E001c', entry=entry_path, error=str(e))\n        except Exception as e:\n            self.log.error('E001c', entry=entry_path, error=str(e))\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34363, "num_retrieved": 74, "inference_time": 4.463354974985123, "generate_results": ["def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set default values for validation\n    self.check_digests = True\n    self.lax_digests = False\n    self.show_warnings = False\n    self.show_errors = True\n    \n    # Initialize the registered extensions\n    self.registered_extensions = [\n        '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n        '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n        '0005-mutable-head'\n    ]\n    \n    # Initialize the validation logger if not already set\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, \n                                   show_errors=self.show_errors, \n                                   lang='en')", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set default values for validation\n    self.check_digests = True\n    self.lax_digests = False\n    self.show_warnings = False\n    self.show_errors = True\n    \n    # Initialize the registered extensions\n    self.registered_extensions = [\n        '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n        '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n        '0005-mutable-head'\n    ]\n    \n    # Initialize the validation logger if not already set\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, \n                                   show_errors=self.show_errors, \n                                   lang='en')", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set default values for validation\n    self.check_digests = True\n    self.lax_digests = False\n    self.show_warnings = False\n    self.show_errors = True\n    \n    # Initialize the registered extensions\n    self.registered_extensions = [\n        '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n        '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n        '0005-mutable-head'\n    ]\n    \n    # Initialize the validation logger if not already set\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, \n                                   show_errors=self.show_errors, \n                                   lang='en')", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    # Set default values for optional parameters\n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the inventory structure\n    self.inventory = self.start_inventory()\n    \n    # Set up the root inventory file\n    self.write_inventory_and_sidecar(self.inventory)\n    \n    # Create the object declaration\n    self.write_object_declaration()\n    \n    # Initialize the root inventory validator\n    self.root_inv_validator = InventoryValidator()\n    \n    # Set up the registered extensions\n    self.registered_extensions = [\n        '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n        '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n        '0005-mutable-head'\n    ]", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set default values for validation\n    self.check_digests = True\n    self.lax_digests = False\n    self.show_warnings = False\n    self.show_errors = True\n    \n    # Initialize the registered extensions\n    self.registered_extensions = [\n        '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n        '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n        '0005-mutable-head'\n    ]\n    \n    # Initialize the validation logger if not already set\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, \n                                   show_errors=self.show_errors, \n                                   lang='en')"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34565, "num_retrieved": 83, "inference_time": 4.463354974985123, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        num = int(match.group(1))\n        return 'v' + str(num + 1).zfill(len(match.group(1)))\n    \n    # For non-zero prefixed versions (like v10, v100), just increment\n    return 'v' + str(int(version[1:]) + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed\n    match = re.match(r'v(\\d+)$', version)\n    if match:\n        num = int(match.group(1))\n        return 'v' + str(num + 1).zfill(len(match.group(1)))\n    \n    # For non-zero prefixed versions, just increment\n    return 'v' + str(int(version[1:]) + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed\n    match = re.match(r'v(\\d+)$', version)\n    if match:\n        num = int(match.group(1))\n        return 'v' + str(num + 1).zfill(len(match.group(1)))\n    \n    # For non-zero prefixed versions, just increment\n    return 'v' + str(int(version[1:]) + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed\n    match = re.match(r'v(\\d+)$', version)\n    if match:\n        num = int(match.group(1))\n        return 'v' + str(num + 1).zfill(len(match.group(1)))\n    \n    # For non-zero-prefixed versions, just increment\n    return 'v' + str(int(version[1:]) + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed\n    match = re.match(r'v(\\d+)$', version)\n    if match:\n        num = int(match.group(1))\n        return 'v' + str(num + 1).zfill(len(match.group(1)))\n    \n    # For non-zero prefixed versions, just increment\n    return 'v' + str(int(version[1:]) + 1)"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34047, "num_retrieved": 60, "inference_time": 4.463354974985123, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    digests_different_from_root = []\n    \n    for vdir in version_dirs:\n        # Validate that the version directory exists and has an inventory\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            continue\n            \n        # Load the inventory for this version\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            continue\n            \n        # If this is the first version, set it as the root inventory\n        if root_inventory is None:\n            root_inventory = version_inventory\n            continue\n            \n        # Compare the current version's inventory with the root inventory\n        # to find any content digests that are different\n        current_manifest = version_inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Check for digests that exist in current version but not in root\n        for digest, files in current_manifest.items():\n            if digest not in root_manifest:\n                digests_different_from_root.append(digest)\n                \n        # Check for digests that exist in root but not in current version\n        for digest, files in root_manifest.items():\n            if digest not in current_manifest:\n                # This would be a deletion, which is not allowed in OCFL\n                self.error(\"E045\", version=vdir, digest=digest)\n    \n    # Return the list of digests that are different from the root inventory\n    return digests_different_from_root", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    digests_different_from_root = []\n    \n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Load the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            return\n            \n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        if 'type' not in inventory:\n            self.error(\"E036b\", version=vdir)\n            return\n            \n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # If this is the first version, set it as root inventory\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare content digests with root inventory\n        # Extract digests from manifest in current version\n        current_manifest = inventory.get('manifest', {})\n        current_digests = set()\n        for digest in current_manifest:\n            current_digests.add(digest)\n            \n        # Extract digests from manifest in root inventory\n        root_manifest = root_inventory.get('manifest', {})\n        root_digests = set()\n        for digest in root_manifest:\n            root_digests.add(digest)\n            \n        # Find digests that are in current version but not in root\n        new_digests = current_digests - root_digests\n        if new_digests:\n            digests_different_from_root.extend(new_digests)\n            \n    return digests_different_from_root", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    digests_different_from_root = []\n    \n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Load the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            return\n            \n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        if 'type' not in inventory:\n            self.error(\"E036b\", version=vdir)\n            return\n            \n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # If this is the first version, set it as root inventory\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare content digests with root inventory\n        # Get manifest from current version\n        current_manifest = inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Check for digests that exist in current version but not in root\n        for digest in current_manifest:\n            if digest not in root_manifest:\n                digests_different_from_root.append(digest)\n                \n        # Check for digests that exist in root but not in current version\n        for digest in root_manifest:\n            if digest not in current_manifest:\n                # This is a deletion, which is not allowed in OCFL\n                self.error(\"E045\", version=vdir, digest=digest)\n                return\n                \n    # Record digests that are different from root\n    self.digests_different_from_root = digests_different_from_root", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    digests_different_from_root = []\n    \n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Load the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            return\n            \n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        if 'type' not in inventory:\n            self.error(\"E036b\", version=vdir)\n            return\n            \n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # If this is the first version, set it as root inventory\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare content digests with root inventory\n        # Get the manifest from current version\n        if 'manifest' not in inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        current_manifest = inventory['manifest']\n        root_manifest = root_inventory['manifest']\n        \n        # Check for digests that are in current version but not in root\n        for digest in current_manifest:\n            if digest not in root_manifest:\n                digests_different_from_root.append(digest)\n                \n        # Check for digests that are in root but not in current version\n        for digest in root_manifest:\n            if digest not in current_manifest:\n                # This is a deletion, which is not allowed in OCFL\n                self.error(\"E045\", version=vdir, digest=digest)\n                return\n                \n    #", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    digests_different_from_root = []\n    \n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            continue\n            \n        # Load the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            continue\n            \n        # Validate the inventory structure\n        if 'id' not in inventory:\n            self.error(\"E036a\", version=vdir)\n            continue\n            \n        # If this is the first version, use it as the root inventory\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare content digests with root inventory\n        # Get the manifest from current version\n        current_manifest = inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Check for digests that exist in current version but not in root\n        for digest, files in current_manifest.items():\n            if digest not in root_manifest:\n                digests_different_from_root.append(digest)\n                \n        # Check for digests that exist in root but not in current version\n        for digest, files in root_manifest.items():\n            if digest not in current_manifest:\n                # This would be an error if we expect all digests to be preserved\n                # But the spec allows for removal of files in later versions\n                pass\n                \n    return digests_different_from_root"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34081, "num_retrieved": 62, "inference_time": 4.463354974985123, "generate_results": ["def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the given path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for entry in fs.scandir(''):\n            if entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if len(namaste_files) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check if we have a root declaration (0=ocfl_object_1.0 or similar)\n        # Root storage has exactly one 0= file with the correct format\n        root_declaration = None\n        object_declaration = None\n        \n        for file in namaste_files:\n            if file.startswith('0=ocfl_object_'):\n                if file == '0=ocfl_object_1.0':\n                    root_declaration = file\n                else:\n                    object_declaration = file\n        \n        # If we found a root declaration, it's a storage root\n        if root_declaration:\n            return 'root'\n            \n        # If we found an object declaration, it's an object\n        if object_declaration:\n            return 'object'\n            \n        # If we have any 0=* files that don't match the expected patterns\n        # This could be a file or an invalid path\n        return 'file'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the given path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for entry in fs.scandir(''):\n            if entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if len(namaste_files) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check if we have a root declaration (0=ocfl_object_1.0 or similar)\n        # Root storage has exactly one 0= file with the expected format\n        for namaste_file in namaste_files:\n            if namaste_file.startswith('0=ocfl_object_'):\n                # This is an object declaration\n                return 'object'\n        \n        # If we have any 0=* files but they don't match the expected object format,\n        # it might be a root with a different declaration\n        # Check if it's a root by looking for the specific declaration\n        if len(namaste_files) == 1:\n            # Only one 0=* file, check if it's a root declaration\n            if namaste_files[0].startswith('0=ocfl_object_1.0'):\n                return 'root'\n        \n        # If we have multiple 0=* files, it's likely an object with multiple versions\n        # or an invalid structure\n        if len(namaste_files) > 1:\n            return 'object'\n            \n        # If we have a 0=* file that doesn't match the expected format\n        return 'file'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for entry in fs.scandir(''):\n            if entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if len(namaste_files) == 0:\n            return 'file'  # No 0=* files, likely a file or directory without OCFL structure\n        \n        # Check if we have a root declaration (0=ocfl_object_1.0 or similar)\n        # Root storage has exactly one 0= declaration file\n        if len(namaste_files) == 1:\n            # Check if it's a root declaration\n            if namaste_files[0].startswith('0=ocfl_object_1.0'):\n                return 'root'\n            # Check if it's an object declaration\n            if namaste_files[0].startswith('0=ocfl_object_'):\n                return 'object'\n        \n        # If we have multiple 0=* files, it's likely an object with multiple declarations\n        # or a root with multiple declarations (which is invalid)\n        if len(namaste_files) > 1:\n            return 'object'  # Multiple declarations suggest an object\n            \n        # If we have exactly one 0=* file that doesn't match the root pattern\n        return 'object'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for entry in fs.scandir(''):\n            if entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if len(namaste_files) == 0:\n            return 'file'  # No 0=* files, likely a file or directory without OCFL structure\n        \n        # Check if there are multiple 0=* files\n        if len(namaste_files) > 1:\n            return 'error: multiple 0= files found'\n        \n        # Extract the file name to determine type\n        namaste_file = namaste_files[0]\n        if namaste_file == '0=ocfl_object_1.0':\n            return 'root'  # Storage root with 1.0 object declaration\n        elif namaste_file.startswith('0=ocfl_object_'):\n            # This is an object with a version\n            return 'object'\n        else:\n            # This is a file with a 0= declaration that doesn't match expected pattern\n            return 'file'\n            \n    except Exception as e:\n        return 'error: could not access path - ' + str(e)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for entry in fs.scandir(''):\n            if entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if len(namaste_files) == 0:\n            return 'file'  # No 0=* files, likely a file or directory without OCFL structure\n        \n        # Check if there are multiple 0=* files\n        if len(namaste_files) > 1:\n            return 'error: multiple 0= files found'\n        \n        # Extract the file name to determine type\n        namaste_file = namaste_files[0]\n        if namaste_file == '0=ocfl_object_1.0':\n            return 'object'\n        elif namaste_file.startswith('0=ocfl_object_'):\n            return 'object'\n        elif namaste_file == '0=ocfl_object_1.0':\n            return 'root'\n        else:\n            # Check if it's a root declaration\n            if namaste_file.startswith('0='):\n                return 'root'\n            else:\n                return 'file'\n                \n    except Exception as e:\n        return f'error: {str(e)}'"]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 45114, "num_retrieved": 387, "inference_time": 4.463354974985123, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is typically used to modify query parameters for Bugzilla\n    # based on additional criteria such as bug IDs or other filters\n    # Since the specific implementation details are not provided in the context,\n    # we implement a generic version that could be used in the context of the existing codebase.\n    \n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(map(str, bug_ids))\n    \n    # Additional logic might be needed based on specific requirements\n    # For example, adding date filters, component filters, or other constraints\n    # This implementation assumes that the bug_ids parameter is used to filter bugs\n    # and that the params dictionary is modified accordingly.\n    \n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(map(str, bug_ids))\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(map(str, bug_ids))\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(map(str, bug_ids))\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is typically used to modify query parameters for Bugzilla\n    # based on context such as bug IDs or other filters\n    # Since the specific implementation details are not provided in the context,\n    # we implement a generic version that could be used in the context of the existing codebase.\n    \n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(map(str, bug_ids))\n    \n    # Add any additional logic that might be needed based on the specific use case\n    # For example, adding filters for specific components, statuses, or other fields\n    # This would be context-specific and typically defined in the specific rule implementation\n    \n    return params"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34554, "num_retrieved": 70, "inference_time": 4.463354974985123, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35456, "num_retrieved": 103, "inference_time": 4.463354974985123, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--remote-path', help='Remote path')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--umask', help='UMask to use')\n    parser.add_argument('--verbose', help='Verbosity level')\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--remote-path', help='Remote path')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--umask', help='UMask to use')\n    parser.add_argument('--verbose', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--source-directories', help='Source directories')\n    parser.add_argument('--source-repository', help='Source repository')\n    parser.add_argument('--destination-repository', help='Destination repository')\n    parser.add_argument('--transfer-glob-archives', help='Glob pattern for transfer archives')\n    parser.add_argument('--transfer-source-repository', help='Source repository for transfer')\n    parser.add_argument('--check-repositories', help='Repositories to check')\n    parser.add_argument('--check-frequency', help='Frequency for consistency checks')\n    parser.add_argument('--check-prefix', help='Prefix for check archives')\n    parser.add_argument('--check-last', help='Last check time for archives')\n    parser.add_argument('--check-data', action='store_true', help='Check", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--remote-path', help='Remote path')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--umask', help='UMask to use')\n    parser.add_argument('--verbose', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--source-directories', help='Source directories')\n    parser.add_argument('--source-repository', help='Source repository')\n    parser.add_argument('--destination-repository', help='Destination repository')\n    parser.add_argument('--paths', help='Paths to include')\n    parser.add_argument('--prefix', help='Prefix for archives')\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--check-repositories', help='Repositories to check')\n    parser.add_argument('--check-frequency', help='Frequency for checks')\n    parser.add_argument('--check-last', type=int, help='Last check time')\n    parser.add_argument('--check-prefix', help='Prefix for archives in", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--remote-path', help='Remote path')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--umask', help='UMask for hooks')\n    parser.add_argument('--source-repository', help='Source repository path')\n    parser.add_argument('--destination-repository', help='Destination repository path')\n    parser.add_argument('--transfer', help='Transfer archives')\n    parser.add_argument('--extract', help='Extract archives')\n    parser.add_argument('--create', help='Create archives')\n    parser.add_argument('--check', help='Check archives')\n    parser.add_argument('--prune', help='Prune archives')\n    parser.add_argument('--compact', help='Compact segments')\n    parser.add_argument('--info', help='Display repository info')\n    parser.add_argument('--rlist', help='List archives')\n    parser.add_argument('--list', help='List archives')\n    parser.add_argument('--version', help='Show version')\n    parser.add_argument('--help', help='Show help')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--source-repository', help='Source repository path')\n    parser.add_argument('--source-archives', help='Source archive name or pattern')\n    parser.add_argument('--destination-repository', help='Destination repository path')\n    parser.add_argument('--destination-archives', help='Destination archive name or pattern')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    parser.add_argument('--remote-path', help='Remote path')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--show-rc', action='store_true', help='Show return code')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--umask', type=int, help='UMask to use')\n    parser.add_argument('--keep-weekly', type=int, help='Keep weekly archives')\n    parser.add_argument('--keep-monthly', type=int, help='Keep monthly archives')\n    parser.add_argument('--keep-daily', type=int, help='Keep daily archives')\n    parser.add_argument('--keep-yearly', type=int, help='Keep yearly archives')\n    parser.add_argument('--keep-keep', type=int, help='Keep keep archives')\n    parser.add_argument('--keep"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35814, "num_retrieved": 116, "inference_time": 4.463354974985123, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all the action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, create the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For store true actions, create the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreConstAction):\n            # For store const actions, create the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Return the flags as a space-separated string\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all the action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For store true actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreConstAction):\n            # For store const actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Join all flags with spaces\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all arguments from the parser\n    args = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in args:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For store true actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Join all flags with spaces\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all the action groups and arguments\n    args = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in args:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, we get the flag name\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For boolean flags, we get the flag name\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreConstAction):\n            # For const flags, we get the flag name\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Return the flags as a space-separated string\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all the action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if action.option_strings:\n            # For each option string, add it to the flags list\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    \n    # Join all flags with a space and return\n    return ' '.join(flags)"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35304, "num_retrieved": 98, "inference_time": 4.463354974985123, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Set verbosity level')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to include')\n    create_parser.add_argument('--exclude-from', nargs='+', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    \n    # Create parser for list action\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Create parser for info action\n    info_parser = subparsers.add_parser('info', help='Display archive information')\n    info_parser.add_argument('--archive', help='Archive name or pattern')\n    info_parser.add_argument('--json', action='store_true', help='Output in JSON format')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Set verbosity level')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to include')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    \n    # Create parser for list action\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name or pattern')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    # Create parser for info action\n    info_parser = subparsers.add_parser('info', help='Display archive information')\n    info_parser.add_argument('--archive', help='Archive name or pattern')\n    info_parser.add_argument('--prefix', help='Prefix for glob patterns')\n    \n    # Create parser for check action\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    check_parser", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Set verbosity level')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to include')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Paths to exclude from the archive')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    \n    # Create parser for list action\n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument('--archive', help='Archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Create parser for info action\n    info_parser = subparsers.add_parser('info', help='Display information about archives')\n    info_parser.add_argument('--archive', help='Archive name or pattern')\n    info_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Create parser for check action\n    check", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Set verbosity level')\n    global_parser.add_argument('--log-level', choices=['error', 'warning', 'info', 'debug'], default='info', help='Set log level')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to include')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Paths to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files that should not be dumped')\n    \n    # Create parser for list action\n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument('--archive', help='Archive name or pattern')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Create parser for info action\n    info_parser = subparsers.add_parser('info', help='Display information about archives')\n    info_parser.add", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add the global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Set verbosity level')\n    global_parser.add_argument('--log-level', choices=['error', 'warning', 'info', 'debug'], default='info', help='Set log level')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to include')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    \n    # Create parser for extract action\n    extract_parser = subparsers.add_parser('extract', help='Extract an archive')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='+', help='Paths to extract')\n    extract_parser.add_argument('--destination-path', help='Destination path')\n    extract_parser.add_argument('--strip-components', type=int, help"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34750, "num_retrieved": 80, "inference_time": 4.851635128259659, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace for this subparser\n        parsed_args = parser.parse_args(remaining)\n        if parsed_args is not None:\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining = []\n            break\n    \n    # If no subparser was able to parse arguments, return empty dict and original arguments\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    # For each subparser, try to parse arguments\n    for subparser_name, parser in subparsers.items():\n        # Parse arguments with this subparser\n        try:\n            parsed_args = parser.parse_args(remaining)\n            if parsed_args is not None:\n                arguments[subparser_name] = parsed_args\n                # Remove the arguments that were parsed\n                remaining = []\n                break\n        except SystemExit:\n            # If the parser raises SystemExit, it means the arguments are not valid for this subparser\n            continue\n    \n    # If no subparser was able to parse arguments, return empty dict and original arguments\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    return (arguments, remaining)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining = []\n            # We need to reconstruct the remaining list properly\n            break\n        except SystemExit:\n            # If the parser raises SystemExit, it means no arguments were parsed\n            # Continue to the next subparser\n            pass\n    \n    # If we didn't find a match for any subparser, just return empty arguments and the original list\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    # Reconstruct the remaining arguments after parsing\n    remaining = []\n    for subparser_name, parser in subparsers.items():\n        try:\n            # Try to parse arguments for this subparser\n            parsed_args = parser.parse_args(remaining)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining = []\n            break\n        except SystemExit:\n            # If the parser raises SystemExit, it means no arguments were parsed\n            # Continue to the next subparser\n            pass\n    \n    # If we didn't find a match for any subparser, just return empty arguments and the original list\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    # Reconstruct the remaining arguments after parsing\n    remaining = []\n    for subparser_name, parser in subparsers.items():\n        try:\n            # Try to parse arguments for this subparser\n            parsed_args = parser.parse_args(remaining)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining = []\n            break\n        except System", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Parse the remaining arguments with this subparser\n        parsed_args = parser.parse_args(remaining)\n        if parsed_args:\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining = []\n            for arg in unparsed_arguments:\n                if arg not in remaining:\n                    remaining.append(arg)\n            break\n    \n    # If no subparser was able to parse arguments, return empty dict and original list\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    return (arguments, remaining)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining = []\n            # Rebuild the remaining list by checking if any arguments were parsed\n            for arg in remaining:\n                # If the argument doesn't match any subparser, keep it\n                pass\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    # If no arguments were parsed by any subparser, return empty dict and original list\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    return (arguments, remaining)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace for this subparser\n        parsed_args = parser.parse_args(remaining)\n        if parsed_args:\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining\n            remaining = []\n            # Re-parse the remaining arguments with the current subparser\n            for arg in remaining:\n                try:\n                    # Try to parse each argument with the current subparser\n                    # This is a simplified approach - in reality, we'd need to handle\n                    # the argument parsing more carefully\n                    pass\n                except:\n                    pass\n    \n    # Return the parsed arguments and remaining arguments\n    return (arguments, remaining)"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35526, "num_retrieved": 106, "inference_time": 4.851635128259659, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_args = top_level_parser.add_argument_group('global')\n    global_args.add_argument('--version', action='version', version='borgmatic %(prog)s')\n    global_args.add_argument('--config', help='Configuration file path')\n    global_args.add_argument('--config-paths', nargs='+', help='Configuration file paths')\n    global_args.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_args.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    global_args.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_args.add_argument('--log-file', help='Log file path')\n    global_args.add_argument('--syslog-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='Syslog verbosity level')\n    global_args.add_argument('--log-file-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='Log file verbosity level')\n    global_args.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='Monitoring verbosity level')\n    global_args.add_argument('--bash-completion', action='store_true', help='Output bash completion script')\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--source', help='Source directory')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    create_parser.add_argument('--exclude-if-present', help='Exclude if present patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import sys\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg repositories',\n        prog='borgmatic'\n    )\n    \n    # Add global arguments\n    global_args = top_level_parser.add_argument_group('global')\n    global_args.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s'\n    )\n    global_args.add_argument(\n        '--config', '-c', metavar='PATH', help='Configuration file path'\n    )\n    global_args.add_argument(\n        '--config-paths', '-C', nargs='+', metavar='PATH', help='Configuration file paths'\n    )\n    global_args.add_argument(\n        '--overrides', '-o', nargs='+', metavar='KEY=VALUE', help='Configuration overrides'\n    )\n    global_args.add_argument(\n        '--resolve-env', action='store_true', help='Resolve environment variables in configuration'\n    )\n    global_args.add_argument(\n        '--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Verbosity level (error, warning, some, lots)'\n    )\n    global_args.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    global_args.add_argument(\n        '--bash-completion', action='store_true', help='Output bash completion script'\n    )\n    global_args.add_argument(\n        '--log-file', metavar='PATH', help='Log file path'\n    )\n    global_args.add_argument(\n        '--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Syslog verbosity level'\n    )\n    global_args.add_argument(\n        '--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Log file verbosity level'\n    )\n    global_args.add_argument(\n        '--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Monitoring verbosity level'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action', help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Note: The actual subparsers would be defined with their specific arguments", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.create as create\n    import borgmatic.commands.dump as dump\n    import borgmatic.commands.info as info\n    import borgmatic.commands.list as list\n    import borgmatic.commands.prune as prune\n    import borgmatic.commands.rcreate as rcreate\n    import borgmatic.commands.rinfo as rinfo\n    import borgmatic.commands.rlist as rlist\n    import borgmatic.commands.transfer as transfer\n    import borgmatic.commands.healthchecks as healthchecks\n    import borgmatic.commands.monitor as monitor\n    import borgmatic.commands.completion as completion\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Files to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files that should not be dumped')\n    create_parser.add_argument('--patterns-from', nargs='+', help='Patterns to include in backup')\n    \n    # Create parser for dump action\n    dump_parser = subparsers.add_parser('dump', help='Dump databases')\n    dump_parser.add_argument('--databases', nargs='+', help='Databases to dump')\n    \n    # Create parser for info action\n    info_parser = subparsers.add_parser('info', help='Display information about archives')\n    info_parser.add_argument('--archive', help='Archive name to display information about')\n    info_parser.add_argument('--prefix', help='Prefix to filter archives by')\n    \n    # Create parser for list action\n    list_parser = subparsers.add_parser('list', help='List archives in a repository')", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import sys\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg repositories',\n        prog='borgmatic'\n    )\n    \n    # Add global arguments\n    global_args = top_level_parser.add_argument_group('global')\n    global_args.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s'\n    )\n    global_args.add_argument(\n        '--config', '-c', metavar='PATH', help='Configuration file path'\n    )\n    global_args.add_argument(\n        '--config-paths', '-C', nargs='+', metavar='PATH', help='Configuration file paths'\n    )\n    global_args.add_argument(\n        '--dry-run', action='store_true', help='Run in dry-run mode'\n    )\n    global_args.add_argument(\n        '--log-file', metavar='PATH', help='Log file path'\n    )\n    global_args.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    global_args.add_argument(\n        '--verbose', '-v', action='count', default=0, help='Increase verbosity'\n    )\n    global_args.add_argument(\n        '--syslog-verbosity', action='count', default=0, help='Syslog verbosity'\n    )\n    global_args.add_argument(\n        '--log-file-verbosity', action='count', default=0, help='Log file verbosity'\n    )\n    global_args.add_argument(\n        '--monitoring-verbosity', action='count', default=0, help='Monitoring verbosity'\n    )\n    global_args.add_argument(\n        '--bash-completion', action='store_true', help='Print bash completion script'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source', '-s', metavar='PATH', help='Source directory to back up'\n    )\n    create_parser.add_argument(\n        '--archive', '-a', metavar='NAME', help='Archive name'\n    )\n    create_parser.add_argument(\n        '--exclude', '-x', metavar='PATTERN', action='append', help='Exclude", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import sys\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg repositories',\n        prog='borgmatic'\n    )\n    \n    # Add global arguments\n    global_args = top_level_parser.add_argument_group('global')\n    global_args.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s'\n    )\n    global_args.add_argument(\n        '--bash-completion', action='store_true', help='Print bash completion script'\n    )\n    global_args.add_argument(\n        '--config', '-c', action='append', help='Configuration file path'\n    )\n    global_args.add_argument(\n        '--dry-run', action='store_true', help='Run in dry run mode'\n    )\n    global_args.add_argument(\n        '--log-file', help='Log file path'\n    )\n    global_args.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    global_args.add_argument(\n        '--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Verbosity level'\n    )\n    global_args.add_argument(\n        '--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Syslog verbosity level'\n    )\n    global_args.add_argument(\n        '--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Log file verbosity level'\n    )\n    global_args.add_argument(\n        '--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Monitoring verbosity level'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparser for create\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source', help='Source directory or pattern'\n    )\n    create_parser.add_argument(\n        '--archive', help='Archive name or pattern'\n    )\n    create_parser.add_argument(\n        '--exclude', help='Exclude patterns'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present', help='"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34610, "num_retrieved": 73, "inference_time": 4.851635128259659, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35937, "num_retrieved": 120, "inference_time": 4.851635128259659, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    return parse_arguments(*arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    return parse_arguments(*arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    return parse_arguments(*arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    return parse_arguments(*arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    return parse_arguments(*arguments)"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36260, "num_retrieved": 133, "inference_time": 4.851635128259659, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36306, "num_retrieved": 136, "inference_time": 4.851635128259659, "generate_results": ["def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                        \"$this_filename\",\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(commandline -poc))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not _fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -o default -F _borgmatic_action_parser_completion -n \"_action_parser_condition\" -n \"_exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name,", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                if [[ \"$installed_script\" != \"\" ]] && [[ \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                        \"$this_filename\"\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(commandline -poc))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not _fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -o default -F _borgmatic_action_parser_completion -n \"$_action_parser_condition\" -n \"$_exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name, action_parser in", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ \"$installed_script\" != \"\" ]] && [[ \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                        \"$this_filename\",\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not _fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -o default -F _borgmatic_action_parser_completion -n \"$_action_parser_condition\" -n \"$_exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name, action_parser in action_parsers", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                        \"$this_filename\"\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(commandline -poc))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not _fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -F _borgmatic_action_parser_completion -n \"$_action_parser_condition\" -n \"$_exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name, action", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                        \"$this_filename\",\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(commandline -poc))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not _fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -o default -F _borgmatic_action_parser_completion -n \"$_action_parser_condition\" -n \"$_exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35084, "num_retrieved": 90, "inference_time": 4.851635128259659, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--source', help='source directory')\n    create_parser.add_argument('--destination', help='destination directory')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='archive name to list')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='archive name to display info for')\n    \n    # Add other subparsers as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dictionary mapping from subparser name to namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--source', help='source directory')\n    create_parser.add_argument('--destination', help='destination directory')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='specific archive to list')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='specific archive to display info for')\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dictionary mapping from subparser name to namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--source', help='source directory')\n    create_parser.add_argument('--destination', help='destination directory')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='specific archive to list')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='specific archive to display info for')\n    \n    # Add other subparsers as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dictionary mapping from subparser name to namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--source', help='source directory')\n    create_parser.add_argument('--destination', help='destination directory')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='specific archive to list')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='specific archive to display info for')\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dictionary mapping from subparser name to Namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--source', help='source directory')\n    create_parser.add_argument('--destination', help='destination directory')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='archive name')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='archive name')\n    info_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    # Add other subparsers as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dictionary mapping from subparser name to namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34761, "num_retrieved": 79, "inference_time": 4.851635128259659, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the subparser to parse the remaining arguments\n        # We use the subparser's argument parser to parse the remaining arguments\n        # This allows common arguments to be shared across multiple subparsers\n        try:\n            # Parse the arguments using the subparser's argument parser\n            parsed_args = subparser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = []\n            # We need to reconstruct the remaining arguments list\n            # by removing the arguments that were parsed\n            for arg in remaining_arguments:\n                if arg not in [str(getattr(parsed_args, key)) for key in parsed_args.__dict__.keys()]:\n                    remaining_arguments.append(arg)\n        except SystemExit:\n            # If the subparser raises a SystemExit, it means that the arguments\n            # were not valid for this subparser, so we skip it\n            continue\n        except Exception:\n            # If there's any other exception, we skip it\n            continue\n    \n    # If we have remaining arguments, we need to add them to the parsed arguments\n    # dictionary with a default value of None\n    if remaining_arguments:\n        parsed_arguments['remaining'] = argparse.Namespace()\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    # Initialize the result dictionary to store parsed arguments for each subparser\n    parsed_arguments = {}\n    \n    # Create a copy of the unparsed arguments to work with\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Iterate through each subparser in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Parse the arguments using the subparser\n        try:\n            # Create a namespace to hold the parsed arguments\n            parsed_args = subparser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n        except Exception:\n            # If any other error occurs, continue to the next subparser\n            continue\n    \n    # Return the parsed arguments dictionary and the remaining arguments\n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    # Initialize the result dictionary to store parsed arguments for each subparser\n    parsed_arguments = {}\n    \n    # Create a copy of the unparsed arguments to work with\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Iterate over each subparser in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Parse the arguments using the subparser\n        try:\n            # Create a namespace to hold the parsed arguments\n            parsed_args = subparser.parse_args(remaining_arguments)\n            \n            # Store the parsed arguments in the result dictionary\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining arguments list\n            # This is done by finding the indices of the parsed arguments and removing them\n            # We need to remove the arguments that were consumed by this subparser\n            new_remaining = []\n            for arg in remaining_arguments:\n                # Check if this argument is part of the parsed arguments\n                # We need to check if the argument matches any of the parsed arguments\n                # This is a simplified approach - in practice, we would need to track the actual argument objects\n                # For now, we'll just remove the first occurrence of each argument\n                if not any(arg == parsed_arg for parsed_arg in parsed_args.__dict__.values()):\n                    new_remaining.append(arg)\n            \n            # Update the remaining arguments\n            remaining_arguments = new_remaining\n            \n        except SystemExit:\n            # If the subparser raises SystemExit, it means it has consumed all arguments\n            # We need to break out of the loop and continue with the remaining arguments\n            break\n    \n    # Return the parsed arguments dictionary and the remaining arguments\n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser and parse the remaining arguments\n        # We use the subparser's argument parser to parse the arguments\n        try:\n            # Parse the arguments using the subparser's parser\n            parsed_args = subparser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = []\n            # We need to reconstruct the remaining arguments list\n            # This is a simplified approach - in practice, we would need to parse the arguments\n            # properly with the subparser's argument parser\n            break\n        except SystemExit:\n            # If the subparser raises SystemExit, it means the arguments were not valid\n            # We need to continue with the next subparser\n            pass\n    \n    # If we didn't parse any arguments with the subparsers, return an empty dict and the original arguments\n    if not parsed_arguments:\n        return (parsed_arguments, unparsed_arguments)\n    \n    # Return the parsed arguments and the remaining arguments\n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    # Initialize a dictionary to store parsed arguments for each subparser\n    parsed_arguments = {}\n    \n    # Create a copy of the unparsed arguments to work with\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Iterate over each subparser in the subparsers dictionary\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments using the current subparser\n        try:\n            # Parse the arguments using the subparser\n            parsed_args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were successfully parsed, store them in the result dictionary\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining arguments list\n            # This is done by finding the indices of the parsed arguments and slicing\n            # the list to exclude those indices\n            remaining_arguments = remaining_arguments[len(parsed_args) - len(remaining_arguments):]\n            \n        except SystemExit:\n            # If parsing fails due to a SystemExit (e.g., due to invalid arguments),\n            # we still want to continue with the next subparser\n            pass\n    \n    # Return the parsed arguments dictionary and the remaining arguments\n    return (parsed_arguments, remaining_arguments)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35907, "num_retrieved": 120, "inference_time": 4.851635128259659, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--log-json', \n        action='store_true', \n        help='Output logs in JSON format'\n    )\n    global_parser.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode without making changes'\n    )\n    global_parser.add_argument(\n        '--verbosity', \n        choices=['disabled', 'error', 'answer', 'some', 'lots'], \n        default='some', \n        help='Set verbosity level'\n    )\n    global_parser.add_argument(\n        '--lock-wait', \n        type=int, \n        help='Wait for lock for the given number of seconds'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument(\n        '--archive', \n        help='Name of the archive to create'\n    )\n    create_parser.add_argument(\n        '--source', \n        help='Source directory to back up'\n    )\n    \n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument(\n        '--archive', \n        help='List archives with this name'\n    )\n    list_parser.add_argument(\n        '--prefix', \n        help='Filter archives by prefix'\n    )\n    \n    info_parser = subparsers.add_parser('info', help='Get information about archives')\n    info_parser.add_argument(\n        '--archive', \n        help='Get information about this archive'\n    )\n    \n    rinfo_parser = subparsers.add_parser('rinfo', help='Get information about a repository')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='List archives in a repository')\n    rlist_parser.add_argument(\n        '--prefix', \n        help='Filter archives by prefix'\n    )\n    \n    prune_parser = subparsers.add_parser('prune', help='Pr", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument(\n        '--log-json', \n        action='store_true', \n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode without making any changes'\n    )\n    global_parser.add_argument(\n        '--verbosity', \n        choices=['disabled', 'error', 'answer', 'some', 'lots'], \n        default='some', \n        help='Set the verbosity level'\n    )\n    global_parser.add_argument(\n        '--lock-wait', \n        type=int, \n        help='Wait for lock to be released for the specified number of seconds'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument(\n        '--archive', \n        help='Name of the archive to create'\n    )\n    create_parser.add_argument(\n        '--source', \n        help='Source directories to back up'\n    )\n    \n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument(\n        '--archive', \n        help='List archives with the given name or pattern'\n    )\n    list_parser.add_argument(\n        '--paths', \n        nargs='+', \n        help='List files within an archive'\n    )\n    \n    info_parser = subparsers.add_parser('info', help='Display information about archives')\n    info_parser.add_argument(\n        '--archive', \n        help='Display information about a specific archive'\n    )\n    \n    rinfo_parser = subparsers.add_parser('rinfo', help='Display repository information')\n    rinfo_parser.add_argument(\n        '--json', \n        action='store_true', \n        help='Output repository information in JSON format'\n    )\n    \n    rlist_parser = subparsers.add_parser('rlist',", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='Output log in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='some', help='Set verbosity level')\n    \n    # Add subparsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create subparsers for each action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--archive', help='Archive name or pattern')\n    create_parser.add_argument('--source', help='Source directory')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='List specific archive')\n    list_parser.add_argument('--prefix', help='Filter archives by prefix')\n    \n    info_parser = subparsers.add_parser('info', help='Get information about archives')\n    info_parser.add_argument('--archive', help='Get information about specific archive')\n    info_parser.add_argument('--prefix', help='Filter archives by prefix')\n    \n    rinfo_parser = subparsers.add_parser('rinfo', help='Get repository information')\n    rinfo_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='List repository archives')\n    rlist_parser.add_argument('--prefix', help='Filter archives by prefix')\n    rlist_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune archives')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep weekly backups')\n    prune_parser.add_argument('--keep-monthly', type=int, help='Keep monthly backups')\n    \n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    check_parser.add_argument", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg repositories')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode without making changes')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    global_parser.add_argument('--quiet', action='store_true', help='Enable quiet output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('--archive', help='Name of the archive to create')\n    create_parser.add_argument('--source', help='Source directories to include in the archive')\n    create_parser.add_argument('--exclude', help='Exclude patterns from the archive')\n    \n    # Add 'list' action\n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument('--archive', help='List a specific archive')\n    list_parser.add_argument('--prefix', help='Filter archives by prefix')\n    \n    # Add 'info' action\n    info_parser = subparsers.add_parser('info', help='Get information about archives')\n    info_parser.add_argument('--archive', help='Get information about a specific archive')\n    info_parser.add_argument('--json', action='store_true', help='Output information in JSON format')\n    \n    # Add 'check' action\n    check_parser = subparsers.add_parser('check', help='Check the integrity of archives')\n    check_parser.add_argument('--archive', help='Check a specific archive')\n    check_parser.add_argument('--repair', action='store_true', help='Attempt to repair damaged archives')\n    \n    # Add 'prune' action\n    prune_parser = subparsers.add_parser('prune', help='Prune old archives based on retention policy')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep the last N weekly archives')\n    prune_parser.add_argument('--keep-monthly', type", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg repositories')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode without making changes')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='some', help='Set verbosity level')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for config action\n    config_parser = subparsers.add_parser('config', help='Manage configuration files')\n    config_subparsers = config_parser.add_subparsers(dest='subaction', help='Sub-actions for config')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_subparsers = create_parser.add_subparsers(dest='subaction', help='Sub-actions for create')\n    \n    # Create parser for list action\n    list_parser = subparsers.add_parser('list', help='List archives or paths within an archive')\n    list_subparsers = list_parser.add_subparsers(dest='subaction', help='Sub-actions for list')\n    \n    # Create parser for info action\n    info_parser = subparsers.add_parser('info', help='Display information about archives or the repository')\n    info_subparsers = info_parser.add_subparsers(dest='subaction', help='Sub-actions for info')\n    \n    # Create parser for prune action\n    prune_parser = subparsers.add_parser('prune', help='Prune archives according to retention policy')\n    prune_subparsers = prune_parser.add_subparsers(dest='subaction', help='Sub-actions for prune')\n    \n    # Create parser for check action\n    check_parser = subparsers.add_parser('check', help='Run consistency checks on archives')\n    check_subparsers = check_parser.add_subparsers(dest='subaction', help='Sub-actions for check')\n    \n    # Create parser for transfer action\n    transfer_parser = subparsers.add_parser('transfer', help='Transfer archives to another repository')\n    transfer_sub"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35114, "num_retrieved": 80, "inference_time": 4.851635128259659, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        return\n    \n    page_size = max_results\n    for i in range(0, len(response), page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        return\n    \n    page_size = max_results\n    for i in range(0, len(response), page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        return\n    \n    page_size = max_results\n    for i in range(0, len(response), page_size):\n        yield response[i:i + page_size]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 20441, "num_retrieved": 146, "inference_time": 4.851635128259659, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit\n    if 'K' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's in bytes\n        return int(size)"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35270, "num_retrieved": 103, "inference_time": 4.851635128259659, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 23160, "num_retrieved": 60, "inference_time": 4.851635128259659, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 41364, "num_retrieved": 266, "inference_time": 4.851635128259659, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', '']"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36260, "num_retrieved": 133, "inference_time": 4.851635128259659, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35297, "num_retrieved": 92, "inference_time": 4.851635128259659, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path to ensure consistency\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if absolute_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(absolute_path))\n        return True\n    \n    # Add to processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path to ensure consistency\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if absolute_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(absolute_path))\n        return True\n    \n    # Add to processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path to ensure consistency\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if absolute_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(absolute_path))\n        return True\n    \n    # Add to processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path to ensure consistency\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if absolute_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(absolute_path))\n        return True\n    \n    # Add to processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path to ensure consistency\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if absolute_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(absolute_path))\n        return True\n    \n    # Add to processed set\n    processed.add(absolute_path)\n    \n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36984, "num_retrieved": 160, "inference_time": 3.667220324277878, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    num_vertices = len(vertices)\n    result = []\n    \n    for i in range(num_vertices):\n        # Get the current vertex and its two neighbors\n        # Previous vertex (with wraparound)\n        prev_idx = (i - 1) % num_vertices\n        # Next vertex (with wraparound)\n        next_idx = (i + 1) % num_vertices\n        \n        # Create a 3-tuple of (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[i], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    num_vertices = len(vertices)\n    result = []\n    \n    for i in range(num_vertices):\n        # Get the current vertex and its two neighbors\n        # The neighbor before the current vertex (wrapping around)\n        prev_idx = (i - 1) % num_vertices\n        # The neighbor after the current vertex (wrapping around)\n        next_idx = (i + 1) % num_vertices\n        \n        # Create a 3-tuple with the previous, current, and next vertices\n        triplet = (vertices[prev_idx], vertices[i], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    num_vertices = len(vertices)\n    result = []\n    \n    for i in range(num_vertices):\n        # Get the current vertex and its two neighbors\n        # The neighbor before the current vertex (wrapping around)\n        prev_idx = (i - 1) % num_vertices\n        # The neighbor after the current vertex (wrapping around)\n        next_idx = (i + 1) % num_vertices\n        \n        # Create a 3-tuple with the previous, current, and next vertices\n        triplet = (vertices[prev_idx], vertices[i], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    num_vertices = len(vertices)\n    \n    for i in range(num_vertices):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % num_vertices\n        curr_idx = i\n        next_idx = (i + 1) % num_vertices\n        \n        # Create a 3-tuple with the current vertex and its two neighbors\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    num_vertices = len(vertices)\n    result = []\n    \n    for i in range(num_vertices):\n        # Get the current vertex and its two neighbors\n        # The neighbor before the current vertex (wrapping around)\n        prev_idx = (i - 1) % num_vertices\n        # The neighbor after the current vertex (wrapping around)\n        next_idx = (i + 1) % num_vertices\n        \n        # Create a 3-tuple with the previous, current, and next vertices\n        triplet = (vertices[prev_idx], vertices[i], vertices[next_idx])\n        result.append(triplet)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 3.667220324277878, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        result = alphabet[number % len(alphabet)] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        remainder = number % len(alphabet)\n        result = alphabet[remainder] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        result = alphabet[number % len(alphabet)] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37605, "num_retrieved": 163, "inference_time": 3.667220324277878, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37299, "num_retrieved": 244, "inference_time": 3.667220324277878, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37902, "num_retrieved": 209, "inference_time": 3.667220324277878, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 3.667220324277878, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 20613, "num_retrieved": 149, "inference_time": 3.667220324277878, "generate_results": ["import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path, format=\"tar\") as extracted_files:\n            pass\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path) as extracted_files:\n            pass  # Extracting is handled by shutil\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path) as extracted_files:\n            pass  # Extract to destination_path\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode to save the downloaded content\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path) as extracted_files:\n            pass  # Extracted files are automatically handled by shutil\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as gz_file:\n        with open(destination_path / \"archive.tar\", \"wb\") as tar_file:\n            shutil.copyfileobj(gz_file, tar_file)\n    \n    # Extract the tar archive\n    import subprocess\n    subprocess.check_call([\"tar\", \"-xzf\", str(destination_path / \"archive.tar\"), \"-C\", str(destination_path)])\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39061, "num_retrieved": 208, "inference_time": 3.667220324277878, "generate_results": ["import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\""]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35935, "num_retrieved": 113, "inference_time": 3.667220324277878, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex key to the result with the same value\n        result[regex_key] = value\n    return result", "def regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex key to the result with the same value\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex pattern\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex pattern to the result with the original value\n        result[regex_key] = value\n    return result", "def regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Ensure the regex starts with ^ and ends with $ to match the entire string\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "def regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Ensure the pattern starts with ^ and ends with $ for full match\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37620, "num_retrieved": 177, "inference_time": 3.667220324277878, "generate_results": ["import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove quotes from the beginning and end of the string\n    # This handles both single and double quotes\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove quotes from the beginning and end of the string\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36292, "num_retrieved": 119, "inference_time": 3.667220324277878, "generate_results": ["import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This handles quotes, backslashes, and spaces properly\n    pattern = r'((?:[^\"\\\\]|\\\\.)*?\"(?:[^\"\\\\]|\\\\.)*?\"|[^\"\\\\]|\\\\.)*'\n    \n    # For Windows/CMD, we need to handle different quoting and escaping\n    if platform == 0:\n        # Windows CMD uses different syntax - split on spaces, but preserve quoted strings\n        # This is a simplified version that handles basic cases\n        result = []\n        current = \"\"\n        in_quotes = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                quote_char = '\"'\n            elif char == quote_char and in_quotes:\n                in_quotes = False\n                if current:\n                    result.append(current.strip())\n                    current = \"\"\n            else:\n                if in_quotes:\n                    current += char\n                else:\n                    if char == ' ':\n                        if current:\n                            result.append(current.strip())\n                            current = \"\"\n                    else:\n                        current += char\n        \n        if current:\n            result.append(current.strip())\n        \n        return result\n    \n    # For POSIX systems (Linux, macOS), use a more robust regex\n    else:\n        # Use a more comprehensive regex that handles quoted strings and escaped characters\n        # This pattern matches quoted strings and unquoted tokens\n        tokens = re.findall(r'(?:\"([^\"]*)\")|([^\"\\\\]|\\\\.)*', s)\n        \n        result = []\n        for token in tokens:\n            if token[0]:  # Quoted string\n                result.append(token[0])\n            elif token[1]:  # Unquoted string\n                # Split on spaces but preserve quoted parts\n                unquoted = token[1].strip()\n                if unquoted:\n                    # Handle escaped characters\n                    un", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This pattern handles quotes, backslashes, and spaces\n    pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*|[^\"\\\\ ]+)(?:\\s+|$)'\n    \n    # Split the string using the pattern\n    parts = re.findall(pattern, s)\n    \n    # Clean up empty parts and strip whitespace\n    result = []\n    for part in parts:\n        # Remove surrounding quotes if present\n        if part.startswith('\"') and part.endswith('\"'):\n            result.append(part[1:-1])\n        elif part.startswith(\"'\") and part.endswith(\"'\"):\n            result.append(part[1:-1])\n        else:\n            result.append(part.strip())\n    \n    # Filter out empty strings\n    return [part for part in result if part]", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match quoted strings and escaped characters\n    # This handles quotes, backslashes, and spaces\n    if platform == 1:  # POSIX\n        # POSIX shell splitting pattern\n        pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*$)'\n        # Use a more robust pattern that handles quoted strings and escaped characters\n        pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*$)'\n        # Split on whitespace, but preserve quoted strings\n        result = []\n        for part in re.split(r'\\s+', s):\n            part = part.strip()\n            if part and part[0] in '\"\\'':\n                # Handle quoted strings\n                if part[0] == '\"':\n                    # Remove quotes and handle escaped characters\n                    content = part[1:-1]\n                    content = content.replace('\"\"', '\"')\n                    content = content.replace('\\\\\\\\', '\\\\')\n                    result.append(content)\n                else:\n                    # Handle single quotes\n                    content = part[1:-1]\n                    content = content.replace('\\\\\\\\', '\\\\')\n                    result.append(content)\n            else:\n                # Handle unquoted parts\n                result.append(part)\n        return result\n    else:  # Windows/CMD\n        # Windows command line splitting pattern\n        # Handle quoted strings and escaped characters\n        result = []\n        # Split on whitespace, but preserve quoted strings\n        for part in re.split(r'\\s+', s):\n            part = part.strip()\n            if part and part[0] in '\"\\'':\n                # Handle quoted strings\n                if part[0] == '\"':\n                    # Remove quotes and handle escaped characters\n                    content = part[1:-1]\n                    content = content.replace('\"\"', '\"')\n                    content = content.replace('\\\\\\\\', '\\\\')\n                    result.append(content)\n                else:\n                    # Handle single quotes\n                    content", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This handles quotes, backslashes, and spaces properly\n    pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*|[^\"\\\\ ]+)(?:\\s+|$)'\n    \n    # Split the string using the pattern\n    parts = re.split(r'(\\s+)', s)\n    \n    # Clean up empty parts and handle quoted strings\n    result = []\n    i = 0\n    while i < len(parts):\n        part = parts[i]\n        if part.strip():\n            # Handle quoted strings\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            elif part.startswith(\"'\") and part.endswith(\"'\"):\n                result.append(part[1:-1])\n            else:\n                # Split on whitespace but preserve quoted parts\n                # Use shlex.split as fallback for non-quoted parts\n                result.extend(shlex.split(part))\n        i += 1\n    \n    # Handle cases where we have multiple consecutive spaces\n    # Remove empty strings and collapse multiple spaces\n    result = [part for part in result if part]\n    \n    return result", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match quoted strings and escaped characters\n    # This handles quotes, backslashes, and spaces\n    if platform == 1:  # POSIX\n        # POSIX shell splitting pattern\n        pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*$)'\n        # Use a more robust pattern that handles quoted strings and escaped characters\n        # This pattern matches quoted strings and unquoted tokens\n        tokens = re.split(r'(\\s+)', s)\n        result = []\n        current = \"\"\n        for token in tokens:\n            if token.strip():\n                if token.startswith('\"') and token.endswith('\"'):\n                    # Handle quoted string\n                    result.append(token[1:-1])\n                elif token.startswith('\"'):\n                    # Handle quoted string that doesn't end properly\n                    result.append(token[1:])\n                elif token.endswith('\"'):\n                    # Handle string that doesn't start properly\n                    result.append(token[:-1])\n                else:\n                    # Handle unquoted tokens\n                    result.append(token)\n        return result\n    else:  # Windows/CMD\n        # Windows command line splitting pattern\n        # This handles spaces and quotes\n        # Use a simpler pattern that splits on spaces but preserves quoted strings\n        # This is a simplified version that works for basic cases\n        result = []\n        # Split on spaces but preserve quoted strings\n        tokens = re.split(r'(\\s+)', s)\n        current = \"\"\n        for token in tokens:\n            if token.strip():\n                if token.startswith('\"') and token.endswith('\"'):\n                    result.append(token[1:-1])\n                else:\n                    # Split the token on spaces\n                    parts = token.split()\n                    for part in parts:\n                        if part.startswith('\"') and part.endswith('\"'):\n                            result.append(part[1:-1])\n                        else:\n                            result.append(part)\n        return result\n    \n    # Fallback to shlex.split for more complex cases"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35491, "num_retrieved": 93, "inference_time": 3.667220324277878, "generate_results": ["import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / Path(archive_path).stem\n    \n    if filename:\n        extract_dir = extract_dir / filename\n    \n    extract_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Use tar to extract the archive\n    try:\n        # Determine the archive type based on extension\n        if archive_path.endswith(\".tar.gz\") or archive_path.endswith(\".tgz\"):\n            cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)]\n        elif archive_path.endswith(\".tar.xz\"):\n            cmd = [\"tar\", \"-xJf\", archive_path, \"-C\", str(extract_dir)]\n        elif archive_path.endswith(\".tar.bz2\"):\n            cmd = [\"tar\", \"-xjf\", archive_path, \"-C\", str(extract_dir)]\n        elif archive_path.endswith(\".zip\"):\n            cmd = [\"unzip\", \"-q\", archive_path, \"-d\", str(extract_dir)]\n        else:\n            # For other archive types, try to use tar with default options\n            cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)]\n        \n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive {archive_path}: {e}\")\n    \n    # Return the path to the extracted directory as the origin URL\n    return str(extract_dir)", "import os\nimport subprocess\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True, parents=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / Path(archive_path).stem\n    \n    if filename:\n        extract_dir = extract_dir / filename\n    \n    extract_dir.mkdir(exist_ok=True, parents=True)\n    \n    # Determine the archive type and extract accordingly\n    if archive_path.endswith(('.tar.gz', '.tgz')):\n        cmd = ['tar', '-xzf', archive_path, '-C', str(extract_dir)]\n    elif archive_path.endswith('.tar'):\n        cmd = ['tar', '-xvf', archive_path, '-C', str(extract_dir)]\n    elif archive_path.endswith('.zip'):\n        cmd = ['unzip', '-q', archive_path, '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as the origin URL\n    return str(extract_dir)", "import os\nimport tarfile\nimport gzip\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n    \n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the archive type and extract\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz', '.tar.gz']:\n        # Extract .tar.gz\n        with gzip.open(archive_path, 'rb') as gz:\n            with tarfile.open(fileobj=gz, mode='r|gz') as tar:\n                tar.extractall(path=tmp_dir)\n    elif archive_path.suffix.lower() in ['.tar', '.tar.xz', '.tar.bz2']:\n        # Extract .tar with various compression\n        with tarfile.open(archive_path, 'r') as tar:\n            tar.extractall(path=tmp_dir)\n    else:\n        # For other formats, we assume it's a single file\n        # If filename is provided, use it; otherwise, use the base name\n        if filename is None:\n            filename = archive_path.name\n        return str(archive_path)\n    \n    # Construct the repo URL based on the extracted files\n    # If filename is provided, use it; otherwise, use the first file in the directory\n    if filename:\n        extracted_path = tmp_dir / filename\n        if extracted_path.exists():\n            return str(extracted_path)\n    \n    # Default to the first file in the directory\n    files = list(tmp_dir.glob(\"*\"))\n    if files:\n        return str(files[0])\n    \n    return str(tmp_dir)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / Path(archive_path).stem\n    \n    if not filename:\n        filename = Path(archive_path).stem\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as the origin URL\n    return str(extract_dir)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / Path(archive_path).stem\n    \n    if not filename:\n        filename = Path(archive_path).stem\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as the origin URL\n    return str(extract_dir)"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 30634, "num_retrieved": 110, "inference_time": 3.667220324277878, "generate_results": ["import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=False)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and any lines that don't match the ignored list\n        filtered_files = [file.strip() for file in ignored_files if file.strip() and file.strip() in ignored]\n        \n        # Sort the files alphabetically\n        filtered_files.sort()\n        \n        # Join the files with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 3.667220324277878, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Based on the context, the supported file types are those related to docopt examples\n    # such as .docopt files (as seen in test cases and examples)\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide information about which file types are supported\n    # Based on the retrieved context, there is no explicit list of supported file types\n    # Therefore, we return False by default as no specific pattern is defined\n    \n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide specific file types that this module supports\n    # Based on the examples and context, the module appears to handle configuration files\n    # like INI and JSON files, as seen in the config_file_example.py\n    # Therefore, we check if the filename ends with .ini or .json\n    \n    return filename.endswith('.ini') or filename.endswith('.json')"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35420, "num_retrieved": 102, "inference_time": 3.667220324277878, "generate_results": ["import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unknown time unit: {unit}')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Remove whitespace\n    frequency = frequency.strip()\n    \n    # Match number and unit\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Remove whitespace\n    frequency = frequency.strip()\n    \n    # Match number and unit\n    match = re.match(r'^(\\d+)(\\s+)([a-zA-Z]+)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(3).lower()\n    \n    # Map units to timedelta components\n    unit_to_timedelta = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': None,  # Not supported - months vary in days\n        'months': None,  # Not supported - months vary in days\n        'year': None,  # Not supported - years vary in days\n        'years': None,  # Not supported - years vary in days\n    }\n    \n    # Check if unit is supported\n    if unit not in unit_to_timedelta:\n        raise ValueError(f'Unsupported unit: {unit}')\n    \n    # If unit is month, year, or any other unsupported unit, raise error\n    if unit_to_timedelta[unit] is None:\n        raise ValueError(f'Unsupported unit: {unit}')\n    \n    return unit_to_timedelta[unit]", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unknown time unit: {unit}')"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38379, "num_retrieved": 187, "inference_time": 3.667220324277878, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35276, "num_retrieved": 98, "inference_time": 3.487169325351715, "generate_results": ["import os\nimport glob\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with 'pp:' or another pattern prefix\n        if path.startswith('pp:') or path.startswith('sh:') or path.startswith('sh:') or path.startswith('sh:'):\n            result.append(path)\n        else:\n            # Transform the path fragment into a glob pattern\n            # Use glob pattern to match any directory path containing the fragment\n            # The pattern will match any directory path that contains the fragment\n            # with the fragment as a file name\n            glob_pattern = f'sh:**/*{path}*/**'\n            result.append(glob_pattern)\n    return result", "import os\nimport glob\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with 'pp:' or similar pattern indicator\n        if path.startswith('pp:') or path.startswith('sh:') or path.startswith('sh:') or path.startswith('sh:'):\n            result.append(path)\n        else:\n            # Transform the path fragment into a glob pattern\n            # Use glob to expand the path and convert it to a Borg-compatible pattern\n            # Borg uses sh: prefix for patterns, and we need to create a pattern that matches the path\n            # The pattern will be sh:**/*path*/** to match any directory containing the path\n            # Replace any '/' with '*/' to create a glob pattern\n            # Use os.path.join to ensure correct path joining\n            pattern = f'sh:**/*{path.replace(\"/\", \"*/\")}/**'\n            result.append(pattern)\n    return result", "import os\nimport glob\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with 'sh:' or 'pp:' or similar pattern indicator\n        if path.startswith('sh:') or path.startswith('pp:') or path.startswith('glob:') or path.startswith('match:'):\n            result.append(path)\n        else:\n            # Transform regular path fragments into glob patterns\n            # Use glob pattern with ** to match any directory path\n            # Replace any existing slashes with the glob pattern\n            # Add sh: prefix to make it a Borg pattern\n            glob_pattern = f'sh:**/*{path}*/**'\n            result.append(glob_pattern)\n    return result", "import os\nimport glob\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with 'sh:' or 'pp:' or another pattern prefix\n        if path.startswith('sh:') or path.startswith('pp:') or path.startswith('glob:') or path.startswith('regex:'):\n            result.append(path)\n        else:\n            # Transform regular path fragments into glob patterns\n            # Use glob to expand the path with wildcards\n            # Convert to a glob pattern with **/* at the beginning and end\n            # Replace any existing wildcards with the glob pattern\n            # Use os.path.join to ensure proper path joining\n            glob_pattern = f\"sh:**/*{path}*/**\"\n            result.append(glob_pattern)\n    return result", "import os\nimport glob\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with 'sh:' or 'pp:' or contains a colon\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Convert the path fragment to a glob pattern\n            # Use glob pattern to match any directory path containing the fragment\n            # Use ** to match any directory path, and * to match any file\n            # Add the sh: prefix to the pattern\n            glob_pattern = f'sh:**/*{path}*/**'\n            result.append(glob_pattern)\n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38283, "num_retrieved": 182, "inference_time": 3.487169325351715, "generate_results": ["import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return \"GIT_BASH\" in os.environ or \"TERM\" in os.environ and \"git-bash\" in os.environ[\"TERM\"].lower()", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return \"GIT_BASH\" in os.environ or \"TERM\" in os.environ and \"git-bash\" in os.environ[\"TERM\"].lower()", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return \"GIT_BASH\" in os.environ or \"TERM\" in os.environ and \"git-bash\" in os.environ[\"TERM\"].lower()", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return \"GIT_BASH\" in os.environ or \"TERM\" in os.environ and \"git-bash\" in os.environ[\"TERM\"].lower()", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return \"GIT_BASH\" in os.environ"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35048, "num_retrieved": 89, "inference_time": 3.487169325351715, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36973, "num_retrieved": 139, "inference_time": 3.487169325351715, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the specified separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width if needed\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the specified separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width if needed\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the specified separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width if needed\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the specified separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width if needed\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the specified separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit in a reasonable width if needed\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35938, "num_retrieved": 115, "inference_time": 3.487169325351715, "generate_results": ["import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module = importlib.util.module_from_spec(type('Module', (object,), {'__name__': module_name}))\n    \n    # Add the function to the module\n    module.func = func\n    \n    # Execute the module in a sub-process\n    try:\n        # Create the command to run\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.func(*{args})\"]\n        \n        # Set environment variables\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the command with timeout\n        result = subprocess.run(cmd, timeout=timeout, env=env, capture_output=True)\n        \n        # Return the result\n        return result.returncode, result.stdout.decode('utf-8'), result.stderr.decode('utf-8')\n        \n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Process timed out\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script to execute the function\n    script_content = f\"\"\"\nimport sys\nimport importlib\nimport {func.__module__}\nfrom {func.__module__} import {func.__name__}\n\ndef main():\n    try:\n        result = {func.__name__}({', '.join([f'\"{arg}\"' if ' ' in arg else arg for arg in args])})\n        print(result)\n    except Exception as e:\n        print(f\"Error: {str(e)}\", file=sys.stderr)\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n    \n    # Write the script to a temporary file\n    temp_script = Path(\"/tmp/matplotlib_subprocess_script.py\")\n    temp_script.write_text(script_content)\n    \n    # Set up environment\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Execute the script\n    try:\n        result = subprocess.run(\n            [sys.executable, str(temp_script)],\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            env=env\n        )\n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            raise RuntimeError(f\"Command failed with return code {result.returncode}: {result.stderr}\")\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Command timed out after {timeout} seconds\")\n    finally:\n        # Clean up the temporary file\n        if temp_script.exists():\n            temp_script.unlink()", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.func = func\n    \n    # Execute the function in a subprocess\n    try:\n        # Create the command to run\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.func(*{args})\"]\n        \n        # Set environment variables\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the command with timeout\n        result = subprocess.run(cmd, timeout=timeout, env=env, capture_output=True)\n        \n        # Return the result\n        return result.returncode, result.stdout.decode('utf-8'), result.stderr.decode('utf-8')\n        \n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Timeout expired\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.func = func\n    \n    # Execute the function in a subprocess\n    try:\n        # Prepare the command to run\n        command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.func(*{args})\"]\n        \n        # Set environment variables\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the command with timeout\n        result = subprocess.run(command, timeout=timeout, env=env, capture_output=True)\n        \n        # Return the result\n        return result.returncode, result.stdout.decode('utf-8'), result.stderr.decode('utf-8')\n        \n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Timeout expired\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Add the function to the module\n    module.func = func\n    \n    # Execute the function in a subprocess\n    try:\n        # Create the command to run\n        command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.func(*{args})\"]\n        \n        # Set environment variables\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the command with timeout\n        result = subprocess.run(command, timeout=timeout, env=env, capture_output=True)\n        \n        # Return the result\n        return result.returncode, result.stdout.decode('utf-8'), result.stderr.decode('utf-8')\n        \n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\"\n    except Exception as e:\n        return -1, \"\", str(e)"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35261, "num_retrieved": 97, "inference_time": 3.487169325351715, "generate_results": ["import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    name = matcher.group(1)\n    default_value = matcher.group(2)\n    \n    if default_value:\n        # Default value is provided, use it if environment variable is not set\n        value = os.environ.get(name, default_value)\n    else:\n        # No default value provided, raise error if environment variable is not set\n        value = os.environ.get(name)\n        if value is None:\n            raise EnvironmentError(f'Environment variable \"{name}\" is not set')\n    \n    return value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${VAR} or ${VAR:-default}\n    match = matcher.group(0)\n    if match.startswith('${'):\n        # Extract the variable name and optional default value\n        var_part = match[2:-1]\n        if ':' in var_part:\n            var_name, default_value = var_part.split(':', 1)\n        else:\n            var_name = var_part\n            default_value = None\n        \n        # Get the value from environment\n        value = os.environ.get(var_name)\n        \n        # If no value is found and no default value is provided, raise an error\n        if value is None and default_value is None:\n            raise RuntimeError(f'Environment variable \"{var_name}\" is not defined')\n            \n        # Return the value (either from environment or default value)\n        return value if value is not None else default_value\n        \n    return None", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    name = matcher.group(1)\n    default_value = matcher.group(2)\n    \n    if default_value:\n        # Default value is provided, use it if environment variable is not set\n        value = os.environ.get(name, default_value)\n        if value is None:\n            raise ValueError(f\"Environment variable '{name}' not set and no default value provided\")\n        return value\n    else:\n        # No default value provided, must have environment variable set\n        value = os.environ.get(name)\n        if value is None:\n            raise ValueError(f\"Environment variable '{name}' not set\")\n        return value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    name = matcher.group(1)\n    default_value = matcher.group(2)\n    \n    if default_value:\n        # If default value is provided, use it if the environment variable is not set\n        value = os.environ.get(name, default_value)\n    else:\n        # If no default value is provided, raise an error if the environment variable is not set\n        value = os.environ.get(name)\n        if value is None:\n            raise EnvironmentError(f'Environment variable \"{name}\" is not defined')\n    \n    return value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    name = matcher.group(1)\n    default_value = matcher.group(2)\n    \n    if default_value:\n        # If default value is provided, use it if the environment variable is not set\n        value = os.environ.get(name, default_value)\n    else:\n        # If no default value is provided, raise an error if the environment variable is not set\n        value = os.environ.get(name)\n        if value is None:\n            raise RuntimeError(f\"Environment variable '{name}' is not set and no default value provided\")\n    \n    return value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39352, "num_retrieved": 209, "inference_time": 3.487169325351715, "generate_results": ["import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parts = urllib.parse.urlsplit(image_href)\n    \n    # Validate required components\n    if not parts.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract image_id from path\n    path = parts.path\n    if not path:\n        raise ValueError(\"Invalid image href: missing path component\")\n    \n    # Extract image_id from path (last segment)\n    image_id = path.split('/')[-1]\n    \n    # Validate image_id is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: image_id is not a valid UUID\")\n    \n    # Determine if SSL is used\n    use_ssl = parts.scheme == 'https'\n    \n    return (image_id, parts.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parts = urllib.parse.urlsplit(image_href)\n    \n    # Validate required components\n    if not parts.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract image_id from path (last part after the path)\n    path_parts = parts.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    image_id = path_parts[-1]\n    \n    # Validate image_id is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image ID format\")\n    \n    # Determine if SSL is used\n    use_ssl = parts.scheme == 'https'\n    \n    return (image_id, parts.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parts = urllib.parse.urlsplit(image_href)\n    \n    if not parts.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parts.scheme == 'https'\n    \n    # Extract image_id from path\n    path = parts.path\n    if not path:\n        raise ValueError(\"Invalid image href: missing path\")\n    \n    # Remove leading slash if present\n    path = path.lstrip('/')\n    \n    # Split by the last slash to get the image_id\n    path_parts = path.split('/', 1)\n    if len(path_parts) != 2:\n        raise ValueError(\"Invalid image href: path does not contain exactly one image_id\")\n    \n    image_id = path_parts[1]\n    \n    return (image_id, parts.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parts = urllib.parse.urlsplit(image_href)\n    \n    if not parts.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parts.scheme == 'https'\n    \n    # Extract image_id from path\n    path = parts.path\n    if not path:\n        raise ValueError(\"Invalid image href: missing path\")\n    \n    # Remove leading slash if present\n    path = path.lstrip('/')\n    \n    # Split by first slash to get image_id\n    if path:\n        image_id = path.split('/', 1)[0]\n    else:\n        raise ValueError(\"Invalid image href: path is empty\")\n    \n    return (image_id, parts.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parts = urllib.parse.urlsplit(image_href)\n    \n    # Validate required components\n    if not parts.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract image_id from path\n    path = parts.path\n    if not path:\n        raise ValueError(\"Invalid image href: missing path component\")\n    \n    # Extract image_id from path (last segment)\n    image_id = path.split('/')[-1]\n    \n    # Validate image_id is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: image_id is not a valid UUID\")\n    \n    # Determine if SSL is used\n    use_ssl = parts.scheme == 'https'\n    \n    return (image_id, parts.netloc, use_ssl)"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36085, "num_retrieved": 118, "inference_time": 3.487169325351715, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep,):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '\\\\'):\n            result.append(item[:-1])\n        elif isinstance(item, str):\n            result.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep,):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep,):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '\\\\'):\n            result.append(item[:-1])\n        elif isinstance(item, str):\n            result.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 3.487169325351715, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literally\n    pattern = re.escape(pattern)\n    # Add word boundaries to match whole words\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literally\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = re.escape(pattern)\n    # Add word boundaries to match exact matches\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37368, "num_retrieved": 164, "inference_time": 3.487169325351715, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if not isinstance(commands, (list, tuple)):\n        commands = [commands]\n    \n    # Set up the environment\n    env = env or os.environ.copy()\n    \n    # Set up the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare command arguments\n    try:\n        # Execute the command\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose:\n            if result.stdout:\n                print(result.stdout.strip())\n            if result.stderr:\n                print(result.stderr.strip())\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode]\n            \n    except subprocess.SubprocessError as e:\n        return [e.returncode if hasattr(e, 'returncode') else 1]\n    except Exception as e:\n        return [1]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Set up the environment if provided\n    env_dict = env if env is not None else os.environ.copy()\n    \n    # Set up the working directory\n    if cwd is not None:\n        cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env_dict,\n            timeout=None\n        )\n        \n        # Check for errors\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n            if result.stderr:\n                print(f\"Error output: {result.stderr}\")\n            return result.returncode\n            \n        return result.returncode\n        \n    except subprocess.TimeoutExpired:\n        if verbose:\n            print(\"Command timed out\")\n        return -1\n        \n    except FileNotFoundError:\n        if verbose:\n            print(f\"Command not found: {cmd[0]}\")\n        return -1\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Command execution error: {str(e)}\")\n        return -1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if not isinstance(commands, (list, tuple)):\n        commands = [commands]\n    \n    # Prepare the environment\n    env = env or os.environ.copy()\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            timeout=None\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout, end='')\n        if verbose and result.stderr:\n            print(result.stderr, end='')\n            \n        # Return the result\n        return result\n        \n    except subprocess.TimeoutExpired:\n        return subprocess.CompletedProcess(\n            args=commands,\n            returncode=-9,\n            stdout='',\n            stderr='Command timed out'\n        )\n    except FileNotFoundError:\n        return subprocess.CompletedProcess(\n            args=commands,\n            returncode=1,\n            stdout='',\n            stderr='Command not found'\n        )\n    except Exception as e:\n        return subprocess.CompletedProcess(\n            args=commands,\n            returncode=1,\n            stdout='',\n            stderr=str(e)\n        )", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands\n    \n    # Set the working directory if provided\n    if cwd is not None:\n        cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n    \n    # Execute the command\n    try:\n        # Run the command with the specified environment\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # If verbose output is requested, print the command and output\n        if verbose:\n            print(\"Running command:\", \" \".join(cmd))\n            if result.stdout:\n                print(\"Output:\", result.stdout)\n            if result.stderr:\n                print(\"Error:\", result.stderr)\n                \n        # Return the result as a tuple of (returncode, stdout, stderr)\n        return (result.returncode, result.stdout, result.stderr)\n        \n    except subprocess.SubprocessError as e:\n        # Handle subprocess errors\n        return (e.returncode, \"\", str(e))", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if not isinstance(commands, (list, tuple)):\n        commands = [commands]\n    \n    # Set up the environment\n    env = env or os.environ.copy()\n    \n    # Set up the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command\n    try:\n        # Execute the command\n        result = subprocess.run(\n            commands,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose:\n            if result.stdout:\n                print(result.stdout.strip())\n            if result.stderr:\n                print(result.stderr.strip())\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode]\n            \n    except subprocess.SubprocessError as e:\n        return [e.returncode if hasattr(e, 'returncode') else 1]\n    except Exception as e:\n        return [1]"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 3.487169325351715, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36913, "num_retrieved": 165, "inference_time": 3.487169325351715, "generate_results": ["def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    \n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n    \n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with a non-blank object\n    for s, p, o in graph.triples((None, prop, None)):\n        if o is not None and not isinstance(o, BNode):\n            # If we haven't seen this object before, it's a root\n            if o not in roots:\n                roots.add(o)\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # through the same property\n    new_roots = set()\n    for s, p, o in graph.triples((None, prop, None)):\n        if o in roots and o not in new_roots:\n            new_roots.add(o)\n    \n    # If we found new roots, recursively process them\n    if new_roots:\n        return find_roots(graph, prop, roots | new_roots)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    \n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n    \n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with a parent\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object is not a root (i.e., it has a parent), then it's not a root\n        # We need to check if the object has any further parents\n        if object_ not in roots:\n            # Check if the object has any further parents\n            has_parent = False\n            for s, p, o in graph.triples((None, prop, object_)):\n                has_parent = True\n                break\n            \n            # If the object has no parent, it's a root\n            if not has_parent:\n                roots.add(object_)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    \n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n    \n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object is not in roots, it's a potential root\n        if object_ not in roots:\n            roots.add(object_)\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # through the same property\n    new_roots = set()\n    for root in roots:\n        for child, _, _ in graph.triples((root, prop, None)):\n            new_roots.add(child)\n    \n    # If we found new roots, add them to the set and recurse\n    if new_roots:\n        roots.update(new_roots)\n        return find_roots(graph, prop, roots)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    \n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n    \n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with a parent\n    for s, p, o in graph.triples((None, prop, None)):\n        # If this subject is not already in roots, it's a root\n        if o not in roots:\n            roots.add(o)\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # through the same property\n    new_roots = set()\n    for s, p, o in graph.triples((None, prop, None)):\n        if o in roots:\n            new_roots.add(s)\n    \n    # If we found new roots, add them and recurse\n    if new_roots:\n        roots.update(new_roots)\n        return find_roots(graph, prop, roots)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n    \n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n    \n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with a non-None object\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object is not a root (i.e., it has a parent), then it's not a root\n        # We need to check if the object has any further parents\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate, object_)):\n            roots.add(object_)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 16464, "num_retrieved": 45, "inference_time": 3.487169325351715, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\nfrom ansible.module_utils._text import to_text\n\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    \n    # Convert the object to a string representation using yaml\n    try:\n        return to_text(yaml.dump(obj, Dumper=dumper))\n    except Exception as e:\n        # Fallback to string representation if yaml dump fails\n        return to_text(str(obj))", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 7074, "num_retrieved": 39, "inference_time": 3.487169325351715, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 20418, "num_retrieved": 67, "inference_time": 3.487169325351715, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36030, "num_retrieved": 125, "inference_time": 3.487169325351715, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35310, "num_retrieved": 100, "inference_time": 4.565238192677498, "generate_results": ["import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration file\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Log any errors encountered during parsing or validation\n            logs.append(\n                logging.LogRecord(\n                    name='borgmatic',\n                    level=logging.ERROR,\n                    pathname='',\n                    lineno=0,\n                    msg=f'Error parsing configuration file {config_filename}: {error}',\n                    args=(),\n                    exc_info=False\n                )\n            )\n    \n    return (configurations, tuple(logs))", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration file\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Capture any errors during parsing or validation\n            logs.append(\n                logging.LogRecord(\n                    name='borgmatic',\n                    level=logging.ERROR,\n                    msg=f'Error parsing configuration file at {config_filename}: {str(error)}',\n                    args=(),\n                    exc_info=False\n                )\n            )\n    \n    return (configurations, tuple(logs))", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration file\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Log any errors encountered during parsing or validation\n            logs.append(\n                logging.LogRecord(\n                    name='borgmatic',\n                    level=logging.ERROR,\n                    pathname='',\n                    lineno=0,\n                    msg=f'Error parsing configuration file {config_filename}: {error}',\n                    args=(),\n                    exc_info=False\n                )\n            )\n    \n    return (configurations, tuple(logs))", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration file\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Capture any errors during parsing or validation\n            logs.append(\n                logging.LogRecord(\n                    name='borgmatic',\n                    level=logging.ERROR,\n                    msg=f'Error parsing configuration file at {config_filename}: {str(error)}',\n                    args=(),\n                    exc_info=False\n                )\n            )\n    \n    return (configurations, tuple(logs))", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration file\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Capture any errors during parsing or validation\n            logs.append(\n                logging.LogRecord(\n                    name='borgmatic',\n                    level=logging.ERROR,\n                    pathname='',\n                    lineno=0,\n                    msg=f'Error parsing configuration file {config_filename}: {str(error)}',\n                    args=(),\n                    exc_info=False\n                )\n            )\n    \n    return (configurations, tuple(logs))"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38992, "num_retrieved": 265, "inference_time": 4.565238192677498, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35651, "num_retrieved": 96, "inference_time": 4.565238192677498, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        if isinstance(tz, int):\n            tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n            zone = pytz.FixedOffset(tz_offset_minutes)\n            time_obj = zone.localize(time_obj)\n        else:\n            zone = pytz.timezone(tz)\n            time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the calculated components\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, localize the time\n    if tz is not None:\n        from pytz import FixedOffset\n        tz_offset_minutes, tz_offset_seconds = divmod(tz.utcoffset(None).total_seconds(), 60)\n        zone = FixedOffset(tz_offset_minutes)\n        time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range [0, 86400000000000)\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        if isinstance(tz, int):\n            tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n            zone = pytz.FixedOffset(tz_offset_minutes)\n            time_obj = zone.localize(time_obj)\n        else:\n            zone = pytz.timezone(tz)\n            time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the calculated components\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, localize the time\n    if tz is not None:\n        from pytz import FixedOffset\n        tz_offset_minutes, tz_offset_seconds = divmod(tz.utcoffset(None).total_seconds(), 60)\n        zone = FixedOffset(tz_offset_minutes)\n        time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range [0, 86400000000000)\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the extracted hours, minutes, seconds, and nanoseconds\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, localize the time\n    if tz is not None:\n        if isinstance(tz, int):\n            tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n            zone = pytz.FixedOffset(tz_offset_minutes)\n        else:\n            zone = pytz.timezone(tz)\n        time_obj = zone.localize(time_obj)\n    \n    return time_obj"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34739, "num_retrieved": 68, "inference_time": 4.565238192677498, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Add Bolt 3 handlers\n    handlers[(3, 0)] = Bolt3\n    handlers[(3, 5)] = Bolt3\n    \n    # Add Bolt 4 handlers\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Add Bolt 3 handlers\n    handlers[(3, 0)] = Bolt3\n    handlers[(3, 5)] = Bolt3\n    \n    # Add Bolt 4 handlers\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Add Bolt 3 handlers\n    handlers[(3, 0)] = Bolt3\n    handlers[(3, 5)] = Bolt3\n    \n    # Add Bolt 4 handlers\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Add Bolt 3 handlers\n    handlers[(3, 0)] = Bolt3\n    handlers[(3, 5)] = Bolt3\n    \n    # Add Bolt 4 handlers\n    handlers[(4, 0)] = Bolt4x0\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x2\n    handlers[(4, 3)] = Bolt4x3\n    handlers[(4, 4)] = Bolt4x4\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Add Bolt 3 handlers\n    if (3, 0) in [(3, 0), (3, 5)]:\n        handlers[(3, 0)] = Bolt3\n    \n    # Add Bolt 4.1 handlers\n    if (4, 1) in [(4, 1)]:\n        handlers[(4, 1)] = Bolt4x1\n    \n    # Add Bolt 4.3 handlers\n    if (4, 3) in [(4, 3)]:\n        handlers[(4, 3)] = Bolt4x3\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35885, "num_retrieved": 106, "inference_time": 4.565238192677498, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(raw_values=list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(raw_values=list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(raw_values=list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(raw_values=list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(raw_values=list(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36232, "num_retrieved": 134, "inference_time": 4.565238192677498, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return (seq, \"fill_compute\")\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return (seq, \"fill_request\")\n    elif isinstance(seq, lena.core.Source):\n        return (seq, \"source\")\n    else:\n        # For other types, try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return (seq, \"sequence\")\n        except lena.core.LenaTypeError:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return (seq, \"fill_compute\")\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return (seq, \"fill_request\")\n    elif isinstance(seq, lena.core.Source):\n        return (seq, \"source\")\n    else:\n        # For other types, try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return (seq, \"sequence\")\n        except lena.core.LenaTypeError:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return (seq, \"fill_compute\")\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return (seq, \"fill_request\")\n    elif isinstance(seq, lena.core.Source):\n        return (seq, \"source\")\n    else:\n        # For other types, try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return (seq, \"sequence\")\n        except lena.core.LenaTypeError:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return (seq, \"fill_compute\")\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return (seq, \"fill_request\")\n    elif isinstance(seq, lena.core.Source):\n        return (seq, \"source\")\n    else:\n        # For other types, try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return (seq, \"sequence\")\n        except lena.core.LenaTypeError:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, \"lena_sequence\")\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return (seq, \"fill_compute\")\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return (seq, \"fill_request\")\n    elif isinstance(seq, lena.core.Source):\n        return (seq, \"source\")\n    else:\n        # For other types, try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return (seq, \"sequence\")\n        except lena.core.LenaTypeError:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 33952, "num_retrieved": 51, "inference_time": 4.565238192677498, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Scale not computed before, compute it\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram\n        self._scale = other\n        # Update the histogram data if needed\n        # This would depend on the specific implementation of the histogram\n        # For now, just set the scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Scale was not computed before, compute it\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        # Update the histogram data to reflect the new scale\n        # This would typically involve scaling the bin values\n        # based on the current scale and the target scale\n        # Implementation details depend on the specific histogram class\n        # and how scaling is defined in the context\n        # For now, we just store the new scale value\n        # The actual data transformation would be handled by the histogram's internal methods", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Scale was not computed before, compute it\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        # Update the histogram data to reflect the new scale\n        # This would typically involve scaling the bin values\n        # based on the current scale and the target scale\n        # Implementation details depend on the specific histogram class\n        # and how scaling is defined in the context\n        self._rescale_to(other)", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self._compute_scale()\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        # Update the histogram data to reflect the new scale\n        # This would typically involve scaling the bin values\n        # based on the current scale and the target scale\n        # Implementation details depend on the specific histogram class\n        # and how scaling is defined in the context\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Scale was not computed before, compute it\n                self._scale = self._compute_scale()\n            else:\n                # Recompute scale\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to other value\n        self._scale = other\n        # Update the histogram data if needed\n        # This would typically involve scaling the bin values\n        # based on the new scale\n        return self._scale"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 33981, "num_retrieved": 51, "inference_time": 4.565238192677498, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Get the last coordinate name\n    last_coord = field_names[-1]\n    \n    # Rescale the last coordinate and its associated errors\n    # This would typically involve modifying the underlying data structure\n    # For a complete implementation, we would need access to the graph's data\n    # and error handling mechanisms\n    \n    # Set the new scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Get the last coordinate name\n    last_coord = field_names[-1]\n    \n    # Get the current coordinates and errors\n    coords = self.coords\n    errors = self._parsed_error_names\n    \n    # Rescale the last coordinate and its errors\n    # For simplicity, we assume the graph has a method to rescale\n    # and that the scale is stored in a property\n    if hasattr(self, '_scale'):\n        self._scale = other\n    else:\n        # If no scale is set, set it to the provided value\n        self._scale = other\n    \n    # Update the coordinates and errors with the new scale\n    # This is a simplified implementation - actual implementation\n    # would depend on the specific graph structure\n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Get the last coordinate name\n    last_coord = field_names[-1]\n    \n    # Get the current coordinates and errors\n    coords = self.coords\n    \n    # Rescale the last coordinate and its corresponding errors\n    if len(coords) == 0:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Get the last coordinate values\n    last_coord_values = coords[-1]\n    \n    # Rescale the values\n    scaled_values = [other * val for val in last_coord_values]\n    \n    # Update the coordinates\n    self.coords = coords[:-1] + [scaled_values]\n    \n    # If errors exist, rescale them as well\n    if hasattr(self, 'errors') and self.errors:\n        errors = self.errors\n        scaled_errors = [other * err for err in errors[-1]]\n        self.errors = errors[:-1] + [scaled_errors]\n    \n    # Update the scale\n    self._scale = other", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Get the last coordinate name\n    last_coord = field_names[-1]\n    \n    # Get the current scale of the graph\n    current_scale = self._scale\n    \n    # If the current scale is unknown or zero, raise an error\n    if current_scale is None or current_scale == 0:\n        raise lena.core.LenaValueError(\"graph has unknown or zero scale\")\n    \n    # Rescale the last coordinate and its errors\n    # This is a simplified implementation - actual implementation would depend on the graph's internal structure\n    # and how scaling is implemented in the underlying system\n    \n    # Update the scale of the graph\n    self._scale = other\n    \n    # Rescale the data points and errors (this would depend on the actual graph implementation)\n    # For now, we just update the scale attribute\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Get the last coordinate name\n    last_coord = field_names[-1]\n    \n    # Get the current coordinates and errors\n    coords = self.coords\n    errors = self._parsed_error_names\n    \n    # Rescale the last coordinate and its errors\n    # For simplicity, we assume the graph has a scale attribute\n    # and we update it directly\n    self._scale = other\n    \n    # If errors exist, rescale them as well\n    if errors:\n        # Find the error index for the last coordinate\n        error_idx = None\n        for err in errors:\n            if err[1] == last_coord:\n                error_idx = err[3]\n                break\n        \n        if error_idx is not None:\n            # Rescale the error values\n            error_coords = coords[error_idx]\n            # Apply the scale to the error values\n            # This is a simplified implementation\n            # In practice, the actual rescaling logic would depend on the graph implementation\n            pass\n    \n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 33811, "num_retrieved": 45, "inference_time": 4.565238192677498, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import lena.structures.graph as graph\n    import lena.structures.histogram as histogram\n    \n    # Set default make_value if not provided\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if len(field_names) != hist.dim:\n        raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Determine scale\n    if scale is True:\n        scale = hist.scale()\n    \n    # Create points list\n    points = []\n    \n    # Iterate through bins\n    for i in range(len(hist.bins)):\n        # Get bin edges based on get_coordinate\n        if get_coordinate == \"left\":\n            x = hist.edges[0][i]", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import lena.structures as structures\n    import lena.context as context\n    \n    # Set default make_value if not provided\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if len(field_names) != hist.dim:\n        raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Determine scale\n    if scale is True:\n        scale = hist.scale()\n    \n    # Create points list\n    points = []\n    \n    # Iterate over bins\n    for bin_idx in range(len(hist.bins)):\n        # Get bin edges based on get_coordinate\n        edges = hist.edges\n        if get_coordinate == \"left\":\n            x = edges[0", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import lena.structures as structures\n    import lena.context as context\n    \n    # Set default make_value to bin content if not provided\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if len(field_names) != hist.dim:\n        raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Determine coordinate values based on get_coordinate\n    coordinates = []\n    for i in range(hist.dim):\n        edges = hist.edges[i]\n        if get_coordinate == \"left\":\n            coord = edges[i]\n        elif get_coordinate == \"right\":\n            coord = edges[i + 1]\n        else:  # middle\n            coord = (edges", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import numpy as np\n    \n    # Set default make_value if not provided\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if len(field_names) != hist.dim:\n        raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Get bin edges and bin content\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Create points list\n    points = []\n    \n    # Process each bin\n    for i in range(len(bins)):\n        # Get bin content\n        bin_content = bins[i]\n        \n        # Apply make_value to get point value\n        value = make_value(bin_content)\n        \n        # Determine coordinate based on", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import lena.structures.graph as graph\n    import lena.structures.histogram as histogram\n    \n    # Set default make_value if not provided\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if len(field_names) != hist.dim:\n        raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Determine the coordinate for each bin\n    def get_bin_coordinate(bin_idx):\n        edges = hist.edges\n        if get_coordinate == \"left\":\n            return edges[bin_idx][0]\n        elif get_coordinate == \"right\":\n            return edges[bin_idx][1]\n        else:  # middle\n            return (edges[bin_idx][0"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34375, "num_retrieved": 62, "inference_time": 4.565238192677498, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'providedBy'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            # Check if method exists\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name))\n        except AttributeError:\n            errors.append(BrokenImplementation(iface, name))\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name, desc in iface.namesAndDescriptions():\n        if hasattr(desc, 'getSignatureString'):\n            try:\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenImplementation(iface, name))\n            except AttributeError:\n                errors.append(BrokenImplementation(iface, name))\n\n    # Step 4: Check if candidate defines all necessary attributes\n    for name, desc in iface.namesAndDescriptions():\n        if hasattr(desc, 'getName') and not hasattr(desc,", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'provides'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            method = getattr(candidate, name)\n            if not callable(method):\n                errors.append(BrokenMethodImplementation(name, f\"method '{name}' is not callable\", vtype=vtype))\n        except AttributeError:\n            errors.append(BrokenMethodImplementation(name, f\"method '{name}' is missing\", vtype=vtype))\n\n    # Step 3: Check if methods have correct signatures\n    if errors:\n        # Check method signatures if possible\n        import inspect\n        for name, desc in iface.namesAndDescriptions():\n            try:\n                method = getattr(candidate, name)\n                if callable(method):\n                    sig = inspect.signature(method)\n                    expected_sig = desc.getSignatureString()\n                    if expected_sig and sig != expected_sig:\n                        errors.append(BrokenMethodImplementation(name, f\"method '{name}' has incorrect signature\", vtype=vtype", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'provides'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            # Check if method exists\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name))\n        except AttributeError:\n            errors.append(BrokenImplementation(iface, name))\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            method = getattr(candidate, name)\n            # Check if method has correct signature\n            if hasattr(desc, 'getSignatureString'):\n                # Compare method signature with interface specification\n                method_sig = method.__code__.co_varnames\n                interface_sig = desc.getSignatureString()\n                # This is a simplified check - actual signature comparison would be more complex\n                if not _signature_matches(method_sig, interface_sig):\n                    errors.append(BrokenMethodImplementation", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'provides'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            # Check if method exists\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name))\n        except AttributeError:\n            errors.append(BrokenImplementation(iface, name))\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            method = getattr(candidate, name)\n            # Check if method signature matches\n            if hasattr(desc, 'getSignatureString'):\n                # Compare method signature with interface definition\n                method_sig = getattr(method, '__signature__', None)\n                if method_sig is None:\n                    # Try to get signature from method\n                    try:\n                        import inspect\n                        method_sig = inspect.signature(method)\n                    except (ValueError, TypeError):\n                        pass\n                \n                # If", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'provides'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            # Check if method exists\n            method = getattr(candidate, name)\n            if not callable(method):\n                errors.append(BrokenMethodImplementation(name, f\"method '{name}' is not callable\", None, iface, candidate))\n        except AttributeError:\n            errors.append(BrokenMethodImplementation(name, f\"method '{name}' is missing\", None, iface, candidate))\n\n    # Step 3: Check method signatures (to the extent possible)\n    import inspect\n    for name, desc in iface.namesAndDescriptions():\n        try:\n            method = getattr(candidate, name)\n            if callable(method):\n                # Get method signature\n                sig = inspect.signature(method)\n                # Compare with expected signature\n                expected_sig = desc.getSignatureString()\n                if expected_sig and sig != expected_sig:\n                    errors.append(BrokenMethodImplementation(name, f\""]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34374, "num_retrieved": 62, "inference_time": 4.565238192677498, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'providedBy'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name in iface.names():\n        method = getattr(candidate, name, None)\n        if method is None:\n            errors.append(BrokenImplementation(iface, name))\n\n    # Step 3: Check if methods have correct signature\n    if errors:\n        # Check method signatures if possible\n        for name in iface.names():\n            method = getattr(candidate, name, None)\n            if method is not None:\n                try:\n                    # Try to get method signature\n                    sig = method.__code__.co_varnames\n                    # This is a simplified check - actual signature validation\n                    # would require more detailed inspection\n                    pass\n                except (AttributeError, TypeError):\n                    # Method doesn't have a signature or is not callable\n                    errors.append(BrokenImplementation(iface, name))\n\n    # Step 4: Check if candidate defines all necessary attributes\n    for name in iface.names():\n        attr = getattr(candidate, name", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'providedBy'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, method_spec in iface.names():\n        try:\n            method = getattr(candidate, name)\n            if not callable(method):\n                errors.append(BrokenMethodImplementation(name, f\"method '{name}' is not callable\", method_spec, iface, candidate))\n        except AttributeError:\n            errors.append(BrokenMethodImplementation(name, f\"method '{name}' is missing\", method_spec, iface, candidate))\n\n    # Step 3: Check if methods have correct signature\n    if errors:\n        # Check method signatures (simplified - actual signature checking would be more complex)\n        for name, method_spec in iface.names():\n            try:\n                method = getattr(candidate, name)\n                if callable(method):\n                    # In a real implementation, we would compare method signatures\n                    # with the interface specification\n                    pass\n            except AttributeError:\n                pass\n\n    # Step 4: Check if candidate defines all necessary attributes\n    for", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'providedBy'))\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface.names():\n        method = getattr(candidate, name, None)\n        if method is None:\n            errors.append(BrokenImplementation(iface, name))\n\n    # Step 3: Check that methods have correct signatures\n    if errors:\n        # Check method signatures if possible\n        for name in iface.names():\n            method = getattr(candidate, name, None)\n            if method is not None:\n                try:\n                    # Check if method has correct signature\n                    # This is a simplified check - actual signature checking would require\n                    # more detailed introspection\n                    pass\n                except Exception as e:\n                    errors.append(BrokenMethodImplementation(name, str(e)))\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface.names():\n        attr = getattr(candidate, name, None)\n        if attr is None:\n            errors.append(BrokenImplementation(iface, name))\n\n    # Return", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'providedBy'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, method_spec in iface.names():\n        try:\n            method = getattr(candidate, name)\n            if not callable(method):\n                errors.append(BrokenMethodImplementation(name, \n                    f\"Missing method {name} defined in interface {iface.getName()}\", \n                    method, iface, candidate))\n        except AttributeError:\n            errors.append(BrokenMethodImplementation(name, \n                f\"Missing method {name} defined in interface {iface.getName()}\", \n                None, iface, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    if errors:\n        # Check method signatures (simplified - actual signature checking would be more complex)\n        for name, method_spec in iface.names():\n            try:\n                method = getattr(candidate, name)\n                if callable(method):\n                    # This is a simplified check - actual signature validation would require\n                    # more detailed introspection\n                    pass\n            except AttributeError:", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(BrokenImplementation(iface, 'providedBy'))\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name, method_spec in iface.names(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenImplementation(iface, name))\n\n    # Step 3: Check if methods have correct signatures\n    for name, method_spec in iface.names(all=True):\n        method = getattr(candidate, name, None)\n        if method is not None:\n            try:\n                # Check method signature against interface specification\n                # This is a simplified check - actual signature validation would\n                # require more detailed introspection\n                pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, str(e), method, iface, candidate))\n\n    # Step 4: Check if candidate defines all necessary attributes\n    for name, attr_spec in iface.names(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenImplementation(iface, name))\n\n    # Return True if no errors, otherwise raise"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 39151, "num_retrieved": 224, "inference_time": 4.565238192677498, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38732, "num_retrieved": 212, "inference_time": 4.565238192677498, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base class has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any of the bases have a metaclass\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 11633, "num_retrieved": 62, "inference_time": 4.565238192677498, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 9729, "num_retrieved": 47, "inference_time": 4.565238192677498, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 4.565238192677498, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 4.506288975477219, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 10009, "num_retrieved": 48, "inference_time": 4.506288975477219, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 28011, "num_retrieved": 72, "inference_time": 4.506288975477219, "generate_results": ["def _create_in_regex(self) -> Pattern[str]:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the in-style parameters\n    # This pattern will match in-style parameters according to the style's param_regex\n    # and escape sequence rules\n    pattern = self.param_regex\n    \n    # Ensure the pattern is properly escaped and handles escape sequences\n    # The pattern should match parameter placeholders in the SQL string\n    # and capture the parameter value for conversion\n    \n    # Use the escape_regex to handle escape sequences that prevent parameter matching\n    # This is typically used to escape special characters in the SQL string\n    escape_pattern = self.escape_regex\n    \n    # Combine the patterns to create a comprehensive regex that matches parameters\n    # and escape sequences\n    # The pattern will match either named, numeric, or ordinal parameters\n    # depending on the style\n    \n    # The final pattern should match the parameter format defined in param_regex\n    # and handle escape sequences as defined in escape_regex\n    \n    # Since the exact pattern is defined in the param_regex attribute,\n    # we simply return the compiled version of it\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is defined by the param_regex attribute\n    # of the style instance, which is set during initialization\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, we can infer\n    # that it should match named, numeric, or ordinal parameters with appropriate escaping\n    # The exact pattern would depend on the specific style being used\n    \n    # Since the context does not provide the actual regex pattern, we return a placeholder\n    # that would typically be defined in the style-specific implementations\n    # In a real implementation, this would be constructed based on the param_regex attribute\n    # of the style instance\n    \n    # For now, we return a simple pattern that matches named parameters\n    # This is a placeholder and would need to be replaced with the actual pattern\n    # based on the specific style implementation\n    \n    # The actual implementation would typically use the param_regex attribute\n    # from the in_style instance\n    import re\n    return re.compile(self._in_style.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regex is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, \n    # we can infer that it's constructed from the param_regex attribute\n    # of the style class, which is used to extract parameters.\n    \n    # Since the exact regex pattern is not provided in the context,\n    # and the function is expected to create the regex from the style,\n    # we return a placeholder that would be used in the actual implementation.\n    \n    # In a real implementation, this would likely use the param_regex\n    # attribute of the style class to create a compiled regex pattern.\n    \n    # For now, we return a dummy pattern that would be replaced with the actual\n    # param_regex in a real implementation.\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, we can infer\n    # that it should match the parameter style defined by the in_style attribute\n    # This is a placeholder implementation that would typically be based on the\n    # param_regex attribute of the Style class\n    \n    # Since the actual regex pattern is not provided in the context, \n    # we return a dummy pattern that would match the parameter style\n    # In a real implementation, this would be constructed from the param_regex\n    # attribute of the in_style object\n    \n    # For now, we return a pattern that matches named parameters\n    import re\n    return re.compile(r'\\{(\\w+)\\}|(\\?|\\$)(\\d+)|(\\%s)' % self._in_style.param_regex)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 27900, "num_retrieved": 71, "inference_time": 4.506288975477219, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 34759, "num_retrieved": 76, "inference_time": 4.506288975477219, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35026, "num_retrieved": 86, "inference_time": 4.506288975477219, "generate_results": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35389, "num_retrieved": 98, "inference_time": 4.506288975477219, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone doesn't support ambiguity detection, just return the original dt\n    if not hasattr(self, 'is_ambiguous') or not callable(getattr(self, 'is_ambiguous')):\n        return dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine the fold state\n        fold = getattr(dt, 'fold', 0)\n        # If in a fold, we need to adjust to the correct time\n        if fold == 1:\n            # For fold=1, we want the later occurrence (standard time)\n            # This is handled by the timezone's internal logic\n            pass\n    \n    # For standard time zones, just return the datetime with the new timezone\n    # The timezone's internal logic will handle the offset conversion\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, we just return it\n    if dt.tzinfo is None:\n        return dt\n    \n    # Determine if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If it's ambiguous, we need to determine which fold to use\n    if is_ambiguous:\n        fold = getattr(dt, 'fold', 0)\n        # Use the fold value to determine which time to return\n        # fold=0 means the earlier time (standard time), fold=1 means the later time (daylight saving)\n        # We need to adjust the datetime based on the fold\n        dt = enfold(dt, fold=fold)\n    \n    # Convert from UTC to local time\n    local_dt = dt.astimezone(self)\n    \n    # Ensure the result is timezone-aware\n    return local_dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For timezone-aware datetimes, we need to convert from UTC to local time\n    # First, determine if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, we need to determine which fold to use\n        fold = getattr(dt, 'fold', 0)\n        # Use the fold value to determine which time to return\n        # If fold is 1, we want the later time (folded time)\n        # If fold is 0, we want the earlier time (standard time)\n        # The function _resolve_ambiguous_time handles this logic\n        idx = self._find_last_transition(dt)\n        if idx is None or idx == 0:\n            # No transitions found, just return the original dt\n            return dt\n        # Adjust index based on fold\n        idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n        idx = idx - idx_offset\n        \n        # Create a new datetime with the appropriate offset\n        # This is a simplified implementation - actual implementation would\n        # involve more complex transition logic\n        new_dt = dt + self.utcoffset(dt)\n        \n        # Adjust for fold if needed\n        if fold == 1:\n            # We're in the folded time, so we need to adjust\n            # This is a placeholder - actual implementation would use\n            # the transition data to determine the correct offset\n            pass\n            \n        return new_dt\n    else:\n        # For non-ambiguous times, simply add the offset\n        return dt + self.utcoffset(dt)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For timezone-aware datetimes, we need to convert from UTC to local time\n    # First, determine if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, we need to determine which fold to use\n        # The fold state is determined by the dt.fold attribute\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the \"fold\" state (the second occurrence)\n        # Otherwise, we're in the standard time state\n        if fold == 1:\n            # We need to adjust the datetime to the correct fold\n            # This is handled by the tzinfo implementation\n            pass\n    \n    # Convert from UTC to local time\n    # The actual conversion is handled by the tzinfo implementation\n    # We simply return the datetime with the correct offset\n    return dt + self.utcoffset(dt)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For UTC to local conversion, we need to determine if the datetime\n    # is ambiguous and in a fold state\n    if self.is_ambiguous(dt):\n        # If ambiguous, we need to determine which fold to use\n        # The fold state is determined by the dt.fold attribute\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (the \"fold\" time)\n        # If fold is 0, we're in the first occurrence\n        # We need to resolve to the correct fold based on the transition\n        # information\n        idx = self._find_last_transition(dt)\n        if idx is not None:\n            # Use the fold to determine which time to return\n            # If fold is 1, we want the second occurrence\n            # If fold is 0, we want the first occurrence\n            # The function _resolve_ambiguous_time handles this\n            resolved_idx = self._resolve_ambiguous_time(dt)\n            # Convert to local time using the resolved index\n            # This will give us the correct time in the local timezone\n            # based on the fold state\n            return self._get_ttinfo(resolved_idx).offset + dt\n        else:\n            # No transitions found, just return the original dt\n            return dt\n    else:\n        # Not ambiguous, just convert to local time\n        return dt + self._std_offset"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35061, "num_retrieved": 86, "inference_time": 4.506288975477219, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 19049, "num_retrieved": 67, "inference_time": 4.506288975477219, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36761, "num_retrieved": 135, "inference_time": 4.506288975477219, "generate_results": ["def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if it's a private message (encrypted payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check if it's a legacy payload with specific headers\n    if request.headers.get('X-Diaspora-Protocol') == 'diaspora':\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message\n    if request.headers.get('X-Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    # Check if the request is a legacy payload (contains specific legacy headers)\n    if request.headers.get('X-Diaspora-Legacy'):\n        return \"diaspora\"\n    \n    # If none of the above, try to identify based on the request body\n    body = request.body\n    if body and b'diaspora' in body:\n        return \"diaspora\"\n    \n    # If no specific Diaspora indicators found, fall back to default protocol\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if it's a private message (encrypted payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check if it's a legacy payload with specific headers\n    if request.headers.get('X-Diaspora-Protocol') == 'diaspora':\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if it's a private message (encrypted payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check if it's a legacy payload with specific headers\n    if request.headers.get('X-Diaspora-Content-Type') == 'application/diaspora+xml':\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (encrypted payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check if the request contains a magic envelope (legacy format)\n    if request.headers.get('X-Magic-Envelope'):\n        return \"diaspora\"\n    \n    # Check if the request body contains a Diaspora-specific signature\n    if request.body and b'diaspora' in request.body:\n        return \"diaspora\"\n    \n    # If none of the above, try to identify based on the request method and path\n    # This is a fallback to the existing protocol identification logic\n    return identify_protocol_by_request(request).PROTOCOL_NAME"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37216, "num_retrieved": 152, "inference_time": 4.506288975477219, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) == MatrixProtocol", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) == MatrixProtocol", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) == MatrixProtocol", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) == MatrixProtocol", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request) == MatrixProtocol"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37008, "num_retrieved": 144, "inference_time": 4.506288975477219, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\")"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35910, "num_retrieved": 107, "inference_time": 4.506288975477219, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Pattern to match tags (e.g., #tag) that are not inside code blocks\n    # Code blocks are marked by ``` or ``` followed by language\n    code_block_pattern = r'```(?:\\S*)\\n(.*?)\\n```'\n    \n    # Split the text into parts by code blocks\n    parts = []\n    code_block_match = re.search(code_block_pattern, result, re.DOTALL)\n    \n    if code_block_match:\n        # Split the text into parts before, inside, and after code blocks\n        before = result[:code_block_match.start()]\n        inside = code_block_match.group(1)\n        after = result[code_block_match.end():]\n        \n        # Process the part before the code block\n        parts.append(before)\n        \n        # Process the code block content (skip tags inside)\n        parts.append(inside)\n        \n        # Process the part after the code block\n        parts.append(after)\n        \n        # Reconstruct the text with processed parts\n        result = before + inside + after\n        \n    # Process the text to find tags\n    # Pattern to match hashtags that are not inside code blocks\n    tag_pattern = r'(?<!\\S)#(\\w+)(?!\\S)'\n    \n    # Find all tags in the text\n    matches = re.finditer(tag_pattern, result, re.IGNORECASE)\n    for match in matches:\n        tag = match.group(1).lower()\n        tags.add(tag)\n        \n        # If a replacer function is provided, replace the tag\n        if replacer:\n            result = result.replace(f\"#{tag}\", replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip tag processing\n            result_lines.append(line)\n            continue\n            \n        # Process line for tags\n        # Use regex to find hashtags, ignoring those inside code blocks\n        # We'll use a simple approach: find # followed by word, but skip if inside code block\n        # We'll also skip if the line starts with code block marker\n        if line.strip().startswith('```'):\n            result_lines.append(line)\n            continue\n            \n        # Find hashtags\n        import re\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', replacement)\n        \n        result_lines.append(line)\n    \n    # Join lines back into text\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    code_block_pattern = r'```(?:\\w+)?\\s*(.*?)\\s*```'\n    \n    # Replace code blocks with a placeholder to avoid matching tags inside them\n    code_blocks = []\n    while True:\n        match = re.search(code_block_pattern, result, re.DOTALL)\n        if not match:\n            break\n        code_blocks.append(match.group(1))\n        result = result[:match.start()] + \"[CODE_BLOCK_PLACEHOLDER]\" + result[match.end():]\n    \n    # Find hashtags in the text (outside of code blocks)\n    tag_pattern = r'#(\\w+)(?=[\\s\\W]|$)'\n    matches = re.finditer(tag_pattern, result, re.IGNORECASE)\n    \n    for match in matches:\n        tag = match.group(1).lower()\n        tags.add(tag)\n        \n        if replacer:\n            replacement = replacer(tag)\n            result = result.replace(f\"#{tag}\", replacement)\n    \n    # Restore code blocks\n    for code_block in code_blocks:\n        result = result.replace(\"[CODE_BLOCK_PLACEHOLDER]\", f\"```{code_block}```\")\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip tag processing\n            result_lines.append(line)\n            continue\n            \n        # Process line for tags\n        # Use regex to find hashtags that are not inside code blocks\n        # We need to handle cases where hashtags are inside code blocks\n        # Code blocks are identified by lines that start with ``` or are inside a code block\n        # We'll use a simple approach: skip any tag that is inside a line that starts with code block marker\n        \n        # Find hashtags that are not inside code blocks\n        # We'll use a simple regex that matches hashtags that are not inside code blocks\n        # We'll also handle cases where hashtags are inside code blocks\n        # We'll use a simple approach: skip any tag that is inside a line that starts with code block marker\n        \n        # Use regex to find hashtags that are not inside code blocks\n        # We'll use a simple regex that matches hashtags that are not inside code blocks\n        # We'll also handle cases where hashtags are inside code blocks\n        # We'll use a simple approach: skip any tag that is inside a line that starts with code block marker\n        \n        # Find hashtags that are not inside code blocks\n        # We'll use a simple regex that matches hashtags that are not inside code blocks\n        # We'll also handle cases where hashtags are inside code blocks\n        # We'll use a simple approach: skip any tag that is inside a line that starts with code block marker\n        \n        # Use regex to find hashtags that are not inside code blocks\n        # We'll use a simple regex that matches hashtags that are not inside code blocks\n        # We'll also handle cases where hashtags are inside code blocks\n        # We'll use a simple approach: skip any tag that is inside a line that starts", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith(\"```\"):\n            # Inside code block, skip tag processing\n            result_lines.append(line)\n            continue\n            \n        # Process line for tags\n        # Use regex to find hashtags that are not inside code blocks\n        # We need to handle cases where hashtags might be inside code blocks\n        # The pattern should match #tag but not inside code blocks\n        import re\n        # Pattern to match hashtags that are not inside code blocks\n        # We'll use a negative lookahead to avoid matching inside code blocks\n        # But since we already handled code blocks above, we can just look for hashtags\n        tag_pattern = r'#(\\w+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Process each match\n        line_with_tags = line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the result of the replacer function\n            if replacer:\n                replacement = replacer(tag)\n                line_with_tags = line_with_tags.replace(f\"#{tag}\", replacement)\n        \n        result_lines.append(line_with_tags)\n    \n    # Join lines back into text\n    result_text = \"\\n\".join(result_lines)\n    \n    return tags, result_text"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37440, "num_retrieved": 162, "inference_time": 4.506288975477219, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs with HTML links\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Handle mentions (e.g., @username)\n    mention_pattern = r'@([^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(mention_pattern, r'<a class=\"mention\" href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Handle hashtags (e.g., #tag)\n    hashtag_pattern = r'#([^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(hashtag_pattern, r'<a class=\"hashtag\" href=\"#\\1\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs with HTML links\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace mentions (e.g. @username) with HTML links\n    mention_pattern = r'@([^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(mention_pattern, r'<a href=\"mailto:\\1\" class=\"mention\">\\1</a>', text)\n    \n    # Replace hashtags (e.g. #tag) with HTML links\n    hashtag_pattern = r'#([^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"#\\1\" class=\"hashtag\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs with HTML links\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[^\\s<>\"{}|\\\\^`{}\\[\\]]+\\.[", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs with HTML links\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Handle mentions (e.g., @username)\n    mention_pattern = r'@([^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(mention_pattern, r'<a class=\"mention\" href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Handle hashtags (e.g., #tag)\n    hashtag_pattern = r'#([^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(hashtag_pattern, r'<a class=\"hashtag\" href=\"#\\1\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs with HTML links\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`]+(?:\\.[^\\s<>\"{}|\\\\^`]+)*\\b)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Handle mentions (e.g., @username)\n    mention_pattern = r'@([^\\s<>\"{}|\\\\^`]+)'\n    text = re.sub(mention_pattern, r'<a href=\"mailto:\\1\" class=\"mention\">\\1</a>', text)\n    \n    # Handle hashtags (e.g., #tag)\n    hashtag_pattern = r'#([^\\s<>\"{}|\\\\^`]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"#\\1\" class=\"hashtag\">\\1</a>', text)\n    \n    return text"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37556, "num_retrieved": 159, "inference_time": 4.506288975477219, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except requests.exceptions.RequestException:\n        pass\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37469, "num_retrieved": 160, "inference_time": 4.506288975477219, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Remove any non-alphanumeric characters except for underscores and hyphens\n    cleaned_tag = re.sub(r'[^a-zA-Z0-9_\\-]', '', tag)\n    \n    # Tags should be at least one character long\n    if not cleaned_tag:\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if cleaned_tag.startswith('-') or cleaned_tag.startswith('_') or \\\n       cleaned_tag.endswith('-') or cleaned_tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(cleaned_tag) > 50:\n        return False\n    \n    # Tags should not be a single character (like \"a\" or \"x\")\n    if len(cleaned_tag) == 1:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not start with a number\n    if tag[0].isdigit():\n        return False\n    \n    # Tags should not contain special characters except for alphanumeric and hyphens\n    if not re.match(r'^[a-zA-Z0-9_-]+$', tag):\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(tag) < 1:\n        return False\n    \n    # Tags should not be just a single underscore or hyphen\n    if tag == \"_\" or tag == \"-\" or tag == \" \":\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not start with a number\n    if tag[0].isdigit():\n        return False\n    \n    # Tags should not contain special characters except for alphanumeric and hyphens\n    if not re.match(r'^[a-zA-Z0-9_-]+$', tag):\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(tag) < 1:\n        return False\n    \n    # Tags should not be just a single underscore or hyphen\n    if tag == \"_\" or tag == \"-\" or tag == \" \":\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not start with a number\n    if tag[0].isdigit():\n        return False\n    \n    # Tags should not contain invalid characters (only alphanumeric and common tag symbols)\n    if not re.match(r'^[a-zA-Z0-9_\\-#]+$', tag):\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(tag) < 1:\n        return False\n    \n    # Tags should not be just a single symbol like \"#\"\n    if tag == \"#\" or tag == \"-\" or tag == \"_\":\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not start with a number\n    if tag[0].isdigit():\n        return False\n    \n    # Tags should not contain special characters except for alphanumeric and hyphens\n    if not re.match(r'^[a-zA-Z0-9_-]+$', tag):\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(tag) < 1:\n        return False\n    \n    # Tags should not be just a single underscore or hyphen\n    if tag == \"_\" or tag == \"-\" or tag == \" \":\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36392, "num_retrieved": 127, "inference_time": 4.6965446174144745, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35779, "num_retrieved": 102, "inference_time": 4.6965446174144745, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Validate handle format (basic check)\n    if not validate_handle(sender_handle) or not validate_handle(entity_handle):\n        return False\n    \n    # Ensure the sender and entity handles match\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Validate handle format (basic check)\n    if not validate_handle(sender_handle) or not validate_handle(entity_handle):\n        return False\n    \n    # Ensure both handles match exactly\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36452, "num_retrieved": 126, "inference_time": 4.6965446174144745, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    document_path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": \"Diaspora Federation\",\n        \"version\": \"1.0\",\n        \"links\": {\n            \"self\": f\"{url}{document_path}\",\n            \"api\": f\"{url}{document_path}\",\n            \"activitypub\": f\"{url}/.well-known/activitypub\"\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": True,\n        \"protocols\": [\"activitypub\"],\n        \"features\": {\n            \"activitypub\": True,\n            \"diaspora\": True\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0,\n                \"activeHalfyear\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        }\n    }\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    document_path = document_path or \"/.well-known/nodeinfo\"\n    \n    # Create the base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora Federation\",\n            \"version\": \"1.0.0\"\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": True,\n        \"protocols\": [\"activitypub\"],\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0,\n                \"activeHalfyear\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        },\n        \"server\": {\n            \"baseUrl\": url.rstrip(\"/\"),\n            \"name\": \"Diaspora Federation Server\",\n            \"version\": \"1.0.0\"\n        }\n    }\n    \n    # Add links to the document\n    links = []\n    if nodeinfo_document[\"openRegistrations\"]:\n        links.append({\n            \"rel\": \"https://nodeinfo.diaspora.software/links/registrations\",\n            \"href\": f\"{url}{document_path}\"\n        })\n    \n    # Add a link to the NodeInfo document itself\n    links.append({\n        \"rel\": \"https://nodeinfo.diaspora.software/links/nodeinfo\",\n        \"href\": f\"{url}{document_path}\"\n    })\n    \n    nodeinfo_document[\"links\"] = links\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    document_path = document_path or \"/.well-known/nodeinfo\"\n    \n    # Create the base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora Federation\",\n            \"version\": \"1.0.0\"\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"protocols\": [\"activitypub\"],\n        \"openRegistrations\": True,\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0,\n                \"activeHalfyear\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        },\n        \"server\": {\n            \"baseUrl\": url.rstrip(\"/\"),\n            \"name\": \"Diaspora Federation Server\",\n            \"version\": \"1.0.0\"\n        },\n        \"organization\": {\n            \"name\": \"Diaspora Federation\",\n            \"contact\": \"contact@diaspora.software\"\n        }\n    }\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    document_path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": \"Diaspora Federation\",\n        \"version\": \"1.0\",\n        \"links\": {\n            \"self\": f\"{url}{document_path}\",\n            \"api\": f\"{url}{document_path}\",\n            \"activitypub\": f\"{url}/.well-known/activitypub\"\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": True,\n        \"protocols\": [\"activitypub\"],\n        \"features\": {\n            \"activitypub\": True,\n            \"diaspora\": True\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0,\n                \"activeHalfyear\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        }\n    }\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    document_path = document_path or \"/.well-known/nodeinfo\"\n    \n    # Create the base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora Federation\",\n            \"version\": \"1.0.0\"\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": True,\n        \"protocols\": [\"activitypub\"],\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0,\n                \"activeHalfyear\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        },\n        \"organization\": {\n            \"name\": \"Diaspora Federation\",\n            \"contact\": \"contact@federation.example.com\"\n        }\n    }\n    \n    # Add server information\n    nodeinfo_document[\"server\"] = {\n        \"baseUrl\": url.rstrip(\"/\"),\n        \"name\": \"Diaspora Federation\",\n        \"version\": \"1.0.0\"\n    }\n    \n    # Add links to services\n    nodeinfo_document[\"links\"] = [\n        {\n            \"rel\": \"http://nodeinfo.diaspora.software/links/1.0\",\n            \"href\": f\"{url}{document_path}\"\n        }\n    ]\n    \n    return nodeinfo_document"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36678, "num_retrieved": 130, "inference_time": 4.6965446174144745, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Decode the signature from base64\n        sig = urlsafe_b64decode(signature.encode(\"ascii\"))\n        \n        # Create the signature contents: data + application/xml + base64url + RSA-SHA256\n        data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n        sig_contents = '.'.join([\n            data,\n            b64encode(b\"application/xml\").decode(\"ascii\"),\n            b64encode(b\"base64url\").decode(\"ascii\"),\n            b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        ])\n        \n        # Create the hash of the signature contents\n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Import the public key\n        public_key_obj = RSA.importKey(public_key)\n        \n        # Verify the signature using PKCS1_v1_5\n        cipher = PKCS1_v1_5.new(public_key_obj)\n        return cipher.verify(sig_hash, sig)\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Decode the signature from base64\n        sig = urlsafe_b64decode(signature.encode(\"ascii\"))\n        \n        # Create the signature contents: data + application/xml + base64url + RSA-SHA256\n        data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n        sig_contents = '.'.join([\n            data,\n            b64encode(b\"application/xml\").decode(\"ascii\"),\n            b64encode(b\"base64url\").decode(\"ascii\"),\n            b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        ])\n        \n        # Create the hash of the signature contents\n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Import the public key\n        public_key_obj = RSA.importKey(public_key)\n        \n        # Verify the signature using PKCS1 v1.5\n        cipher = PKCS1_v1_5.new(public_key_obj)\n        return cipher.verify(sig_hash, sig)\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Decode the signature from base64\n        sig = urlsafe_b64decode(signature.encode(\"ascii\"))\n        \n        # Create the signature contents: data + application/xml + base64url + RSA-SHA256\n        data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n        sig_contents = '.'.join([\n            data,\n            b64encode(b\"application/xml\").decode(\"ascii\"),\n            b64encode(b\"base64url\").decode(\"ascii\"),\n            b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        ])\n        \n        # Create the hash of the signature contents\n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Import the public key\n        public_key_obj = RSA.importKey(public_key)\n        \n        # Verify the signature using PKCS1 v1.5\n        cipher = PKCS1_v1_5.new(public_key_obj)\n        return cipher.verify(sig_hash, sig)\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Decode the signature from base64\n        sig = urlsafe_b64decode(signature.encode(\"ascii\"))\n        \n        # Create the signature contents: data + application/xml + base64url + RSA-SHA256\n        data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n        sig_contents = '.'.join([\n            data,\n            b64encode(b\"application/xml\").decode(\"ascii\"),\n            b64encode(b\"base64url\").decode(\"ascii\"),\n            b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        ])\n        \n        # Create the hash of the signature contents\n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Import the public key\n        pub_key = RSA.importKey(public_key)\n        \n        # Verify the signature using PKCS1_v1_5\n        cipher = PKCS1_v1_5.new(pub_key)\n        return cipher.verify(sig_hash, sig)\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Decode the signature from base64\n        sig_bytes = urlsafe_b64decode(signature.encode(\"ascii\"))\n        \n        # Create the signature content: data + application/xml + base64url + RSA-SHA256\n        data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n        sig_contents = '.'.join([\n            data,\n            b64encode(b\"application/xml\").decode(\"ascii\"),\n            b64encode(b\"base64url\").decode(\"ascii\"),\n            b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        ])\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Import the public key\n        pub_key = RSA.importKey(public_key)\n        \n        # Create a PKCS1 v1.5 verifier\n        cipher = PKCS1_v1_5.new(pub_key)\n        \n        # Verify the signature\n        return cipher.verify(sig_hash, sig_bytes)\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37179, "num_retrieved": 151, "inference_time": 4.6965446174144745, "generate_results": ["import json\nimport re\nfrom typing import Dict, Any\nfrom xrd import XRD, Link, Element\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON format\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n            \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = [str(alias) for alias in aliases]\n            \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n                    \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            xrd = XRD()\n            # Parse the XRD document\n            # Extract subject from the Subject element\n            subject = None\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Subject\":\n                    subject = element.get(\"acct\")\n                    break\n                    \n            # Extract aliases from Alias elements\n            aliases = []\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Alias\":\n                    aliases.append(element.get(\"href\"))\n                    \n            # Extract links from Link elements\n            links = []\n            for link in xrd.links:\n                links.append({\n                    \"rel\": link.rel,\n                    \"type\": link.type_,\n                    \"href\": link.href\n                })\n                \n            result = {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n            return result\n            \n        except Exception:\n            # If parsing fails, return", "import json\nimport re\nfrom typing import Dict, Any\nfrom xrd import XRD, Link, Element\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON format\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n            \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = [str(alias) for alias in aliases]\n            \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n                    \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            xrd = XRD()\n            # Parse the XRD document\n            # Extract subject from the Subject element\n            subject = None\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Subject\":\n                    subject = element.text\n                    break\n                    \n            # Extract aliases from Alias elements\n            aliases = []\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Alias\":\n                    aliases.append(element.text)\n                    \n            # Extract links from Link elements\n            links = []\n            for link in xrd.links:\n                links.append({\n                    \"rel\": link.rel,\n                    \"type\": link.type_,\n                    \"href\": link.href\n                })\n                \n            result = {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n            return result\n            \n        except Exception:\n            # If both parsing methods fail, return empty dict", "import json\nimport re\nfrom typing import Dict, Any\nfrom xrd import XRD, Link, Element\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON format\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n            \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = aliases\n            \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            xrd = XRD()\n            # Parse the XRD document\n            # Extract subject from the Subject element\n            subject = None\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Subject\":\n                    subject = element.text\n                    break\n                    \n            # Extract aliases from Alias elements\n            aliases = []\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Alias\":\n                    aliases.append(element.text)\n                    \n            # Extract links from Link elements\n            links = []\n            for link in xrd.links:\n                links.append({\n                    \"rel\": link.rel,\n                    \"type\": link.type_,\n                    \"href\": link.href\n                })\n                \n            result = {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n            return result\n            \n        except Exception:\n            # If parsing fails, return empty dict\n            return {}", "import json\nimport re\nfrom typing import Dict, Any\nfrom xrd import XRD, Link, Element\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON format\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n            \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = [str(alias) for alias in aliases]\n            \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n                    \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            xrd = XRD()\n            # Parse the XRD document\n            # Extract subject from the XRD\n            subject = None\n            for element in xrd.elements:\n                if element.tag == \"Subject\":\n                    subject = element.text\n                    break\n                    \n            # Extract aliases from the XRD\n            aliases = []\n            for element in xrd.elements:\n                if element.tag == \"Alias\":\n                    aliases.append(element.text)\n                    \n            # Extract links from the XRD\n            links = []\n            for link in xrd.links:\n                links.append({\n                    \"rel\": link.rel,\n                    \"type\": link.type_,\n                    \"href\": link.href\n                })\n                \n            result = {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n            return result\n            \n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "import json\nimport re\nfrom typing import Dict, Any\nfrom xrd import XRD, Link, Element\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON format\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n            \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = [str(alias) for alias in aliases]\n            \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n                    \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            xrd = XRD()\n            # Parse the XRD document\n            # Extract subject from the Subject element\n            subject = None\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Subject\":\n                    subject = element.text\n                    break\n                    \n            # Extract aliases from Alias elements\n            aliases = []\n            for element in xrd.elements:\n                if isinstance(element, Element) and element.tag == \"Alias\":\n                    aliases.append(element.text)\n                    \n            # Extract links from Link elements\n            links = []\n            for link in xrd.links:\n                links.append({\n                    \"rel\": link.rel,\n                    \"type\": link.type_,\n                    \"href\": link.href\n                })\n                \n            result = {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n            return result\n            \n        except Exception:\n            # If both parsing methods fail, return empty dict"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37521, "num_retrieved": 163, "inference_time": 4.6965446174144745, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document, status_code, exception = fetch_document(host=handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        if status_code == 200:\n            return document\n        return None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document, status_code, exception = fetch_document(host=handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        if status_code == 200:\n            return document\n        return None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document, status_code, exception = fetch_document(host=handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        if status_code == 200:\n            return document\n        return None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document, status_code, exception = fetch_document(host=handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        if status_code == 200:\n            return document\n        return None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document, status_code, exception = fetch_document(host=handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        if status_code == 200:\n            return document\n        return None\n    except Exception:\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36543, "num_retrieved": 127, "inference_time": 4.6965446174144745, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create the webfinger response structure\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{handle.split('@')[1]}/profile/{handle.split('@')[0]}\",\n            f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[0]}\"\n        ],\n        \"links\": [\n            {\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/hcard/{handle.split('@')[0]}\"\n            },\n            {\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}\"\n            },\n            {\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[0]}\"\n            },\n            {\n                \"rel\": \"salmon\",\n                \"href\": f\"https://{handle.split('@')[1]}/receive/users/{handle.split('@')[0]}\"\n            }\n        ]\n    }\n    \n    # Add self link if found in the original document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n    \n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create the webfinger response dictionary\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{handle.split('@')[1]}/profile\",\n            f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[1].split('.')[0]}\"\n        ],\n        \"links\": [\n            {\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/hcard/{handle.split('@')[1].split('.')[0]}\"\n            },\n            {\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}\"\n            },\n            {\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[1].split('.')[0]}\"\n            },\n            {\n                \"rel\": \"salmon\",\n                \"href\": f\"https://{handle.split('@')[1]}/receive/users/{handle.split('@')[1].split('.')[0]}\"\n            }\n        ]\n    }\n    \n    # Add self link if present in the original document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n    \n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create the webfinger document structure\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{handle.split('@')[1]}/profile\",\n            f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[1].split('.')[0]}\"\n        ],\n        \"links\": [\n            {\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/hcard/{handle.split('@')[1].split('.')[0]}\"\n            },\n            {\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}\"\n            },\n            {\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[1].split('.')[0]}\"\n            },\n            {\n                \"rel\": \"salmon\",\n                \"href\": f\"https://{handle.split('@')[1]}/receive/users/{handle.split('@')[1].split('.')[0]}\"\n            }\n        ]\n    }\n    \n    # Add self link if present in the original document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n    \n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create the webfinger document structure\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{handle.split('@')[1]}/profile\",\n            f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[1].split('.')[0]}\"\n        ],\n        \"links\": [\n            {\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/hcard/{handle.split('@')[1].split('.')[0]}\"\n            },\n            {\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}\"\n            },\n            {\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[1].split('.')[0]}\"\n            },\n            {\n                \"rel\": \"salmon\",\n                \"href\": f\"https://{handle.split('@')[1]}/receive/users/{handle.split('@')[1].split('.')[0]}\"\n            }\n        ]\n    }\n    \n    # Add self link if present in the original document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n    \n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create the webfinger response structure\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{handle.split('@')[1]}/profile/{handle.split('@')[0]}\",\n            f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[0]}\"\n        ],\n        \"links\": [\n            {\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/hcard/{handle.split('@')[0]}\"\n            },\n            {\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}\"\n            },\n            {\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": f\"https://{handle.split('@')[1]}/profile/{handle.split('@')[0]}\"\n            },\n            {\n                \"rel\": \"salmon\",\n                \"href\": f\"https://{handle.split('@')[1]}/receive/users/{handle.split('@')[0]}\"\n            }\n        ]\n    }\n    \n    # Add self link if present in the original document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n    \n    return webfinger"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37925, "num_retrieved": 171, "inference_time": 4.6965446174144745, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create an XRD instance and populate it with the parsed data\n    xrd = XRD()\n    \n    # Add links from the parsed document\n    for link in doc.get('links', []):\n        rel = link.get('rel')\n        type_ = link.get('type')\n        href = link.get('href')\n        if rel and type_ and href:\n            xrd.links.append(Link(rel=rel, type_=type_, href=href))\n    \n    # Add elements from the parsed document\n    for element in doc.get('elements', []):\n        rel = element.get('rel')\n        type_ = element.get('type')\n        href = element.get('href')\n        if rel and type_ and href:\n            xrd.elements.append(Element(rel=rel, type_=type_, href=href))\n    \n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Create an XRD instance and populate it with the retrieved data\n    xrd = XRD()\n    \n    # Add links from the document if present\n    if 'links' in doc:\n        for link in doc['links']:\n            rel = link.get('rel')\n            type_ = link.get('type')\n            href = link.get('href')\n            if rel and type_ and href:\n                xrd.links.append(Link(rel=rel, type_=type_, href=href))\n    \n    # Add elements if present\n    if 'elements' in doc:\n        for element in doc['elements']:\n            xrd.elements.append(Element(**element))\n    \n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n        \n    # Create an XRD instance and populate it with the retrieved data\n    xrd = XRD()\n    \n    # Add links from the document if present\n    if 'links' in doc:\n        for link in doc['links']:\n            rel = link.get('rel')\n            type_ = link.get('type')\n            href = link.get('href')\n            if rel and href:\n                xrd.links.append(Link(rel=rel, type_=type_, href=href))\n    \n    # Add elements if present\n    if 'elements' in doc:\n        for element in doc['elements']:\n            xrd.elements.append(Element(element['name'], element['value']))\n    \n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n        \n    # Create an XRD instance and populate it with the retrieved data\n    xrd = XRD()\n    \n    # Add links from the document if present\n    if 'links' in doc:\n        for link in doc['links']:\n            rel = link.get('rel')\n            type_ = link.get('type')\n            href = link.get('href')\n            if rel and href:\n                xrd.links.append(Link(rel=rel, type_=type_, href=href))\n    \n    # Add elements if present\n    if 'elements' in doc:\n        for element in doc['elements']:\n            xrd.elements.append(Element(element['name'], element['value']))\n    \n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n        \n    # Create an XRD instance and populate it with the retrieved data\n    xrd = XRD()\n    \n    # Add links from the document\n    if 'links' in doc:\n        for link in doc['links']:\n            rel = link.get('rel')\n            type_ = link.get('type')\n            href = link.get('href')\n            if rel and type_ and href:\n                xrd.links.append(Link(rel=rel, type_=type_, href=href))\n    \n    # Add elements if present\n    if 'elements' in doc:\n        for element in doc['elements']:\n            xrd.elements.append(Element(element['name'], element['value']))\n    \n    return xrd"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35164, "num_retrieved": 85, "inference_time": 4.6965446174144745, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35662, "num_retrieved": 93, "inference_time": 4.6965446174144745, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # If key is provided, use it as the first key\n    if key:\n        # Initialize the current dictionary with the first key\n        if key not in current:\n            current[key] = {}\n        current = current[key]\n    \n    # Process each additional key in the chain\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # If key is provided, use it as the first key\n    if key:\n        # Initialize the current dictionary with the first key\n        if key not in current:\n            current[key] = {}\n        current = current[key]\n    \n    # Process each additional key in the chain\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # If key is provided, use it as the first key\n    if key:\n        # Initialize the current dictionary with the first key\n        if key not in current:\n            current[key] = {}\n        current = current[key]\n    \n    # Process each additional key in the chain\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # If key is provided, use it as the first key\n    if key:\n        # Initialize the current dictionary with the first key\n        if key not in current:\n            current[key] = {}\n        current = current[key]\n    \n    # Process each additional key in the chain\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value at the end of the chain\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # If key is provided, use it as the first key\n    if key:\n        # Start with the first key\n        current[key] = {}\n    \n    # Process each additional key in the chain\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36524, "num_retrieved": 116, "inference_time": 4.6965446174144745, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(settings_dirs, spec_option)"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38412, "num_retrieved": 175, "inference_time": 4.6965446174144745, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.ansible_config_manager()"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38587, "num_retrieved": 181, "inference_time": 4.6965446174144745, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # Based on the context, this would typically be a static or class-level property\n    # that returns an instance of WorkspaceManager\n    if not hasattr(cls, '_workspace_manager'):\n        cls._workspace_manager = WorkspaceManager(workspaces_base_dir=os.path.join(os.path.expanduser('~'), '.infrared', 'workspaces'))\n    return cls._workspace_manager", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # based on the context, this would typically be a static or class-level property\n    # Since the context shows that CoreServices.workspace_manager() is used,\n    # we infer that this method returns an instance of WorkspaceManager\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # based on the context, this would typically be a static or class-level property\n    # Since the context shows that CoreServices.workspace_manager() is used,\n    # we infer that workspace_manager is a method on CoreServices or a similar service\n    # However, the exact implementation is not provided in the context.\n    # Based on the pattern in the code, we return a placeholder implementation.\n    # In a real implementation, this would likely return an instance of WorkspaceManager.\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # based on the context, we would typically retrieve it from a core services registry\n    # Since the exact implementation is not provided, we return a placeholder\n    # that would be replaced with actual implementation in the real codebase\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # based on the context, we need to return an instance of WorkspaceManager\n    # This is a placeholder implementation based on the context\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 38442, "num_retrieved": 180, "inference_time": 4.6965446174144745, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be initialized\n    # based on the context provided in the retrieved files\n    # The implementation would typically return an instance of InfraredPluginManager\n    # as seen in the context where InfraredPluginManager is defined in core/services/plugins.py\n    from infrared.core.services.plugins import InfraredPluginManager\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be created once\n    # based on the context, it appears to be a class method that returns an instance\n    # of InfraredPluginManager from CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be initialized\n    # based on the context provided in the retrieved files\n    # The implementation should return an instance of InfraredPluginManager\n    # as it is the class responsible for managing plugins\n    from infrared.core.services.plugins import InfraredPluginManager\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be instantiated\n    # based on the context provided in the retrieved files\n    # The implementation should return an instance of InfraredPluginManager\n    # as seen in the context where it's used in CoreServices.plugins_manager()\n    from infrared.core.services.plugins import InfraredPluginManager\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be initialized\n    # based on the context provided in the retrieved files\n    # The implementation would typically return an instance of InfraredPluginManager\n    # which is defined in the core/services/plugins.py file\n    \n    # Since the exact implementation details are not provided in the context,\n    # we return a placeholder that would typically be instantiated from the\n    # CoreServices class as seen in other parts of the codebase\n    from infrared.core.services.plugins import InfraredPluginManager\n    return InfraredPluginManager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35991, "num_retrieved": 102, "inference_time": 4.6965446174144745, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields (adjust based on actual spec structure)\n    required_fields = [\n        'plugin_type',\n        'description',\n        'subparsers',\n        'config'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation for subparsers\n    if 'subparsers' not in spec_data or not spec_data['subparsers']:\n        raise exceptions.IRValidatorException(\"Missing or empty subparsers section\")\n\n    # Validate plugin_type\n    if not isinstance(spec_data['plugin_type'], str):\n        raise exceptions.IRValidatorException(\"plugin_type must be a string\")\n\n    # Validate description\n    if not isinstance(spec_data['description'], str):\n        raise exceptions.IRValidatorException(\"description must be a string\")\n\n    # Validate config section\n    if 'config' in spec_data:\n        if not isinstance(spec_data['config'], dict):\n            raise exceptions.IRValidatorException(\"config must be a dictionary\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields (adjust based on actual spec structure)\n    required_fields = [\n        'plugin_type',\n        'name',\n        'description',\n        'version',\n        'options',\n        'config'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation for specific fields if needed\n    if 'options' not in spec_data or not isinstance(spec_data['options'], dict):\n        raise exceptions.IRValidatorException(\"Options must be a dictionary\")\n\n    if 'config' not in spec_data or not isinstance(spec_data['config'], dict):\n        raise exceptions.IRValidatorException(\"Config must be a dictionary\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields that should be present in the spec\n    required_fields = [\n        'plugin_type',\n        'name',\n        'description',\n        'version',\n        'options',\n        'config'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation for specific fields if needed\n    if 'plugin_type' in spec_data and not isinstance(spec_data['plugin_type'], str):\n        raise exceptions.IRValidatorException(\"plugin_type must be a string\")\n\n    if 'name' in spec_data and not isinstance(spec_data['name'], str):\n        raise exceptions.IRValidatorException(\"name must be a string\")\n\n    if 'description' in spec_data and not isinstance(spec_data['description'], str):\n        raise exceptions.IRValidatorException(\"description must be a string\")\n\n    if 'version' in spec_data and not isinstance(spec_data['version'], str):\n        raise exceptions.IRValidatorException(\"version must be a string\")\n\n    if 'options' in spec_data and not isinstance(spec_data['options'], dict):\n        raise exceptions.IRValidatorException(\"options must be a dictionary\")\n\n    if 'config' in spec_data and not isinstance(spec_data['config'], dict):\n        raise exceptions.IRValidatorException(\"config must be a dictionary\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields (these should be present in the spec)\n    required_fields = [\n        'plugin_type',\n        'name',\n        'description',\n        'version',\n        'options',\n        'config'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation for specific fields if needed\n    if 'plugin_type' not in spec_data or not spec_data['plugin_type']:\n        raise exceptions.IRValidatorException(\"Plugin type is required and cannot be empty\")\n\n    if 'name' not in spec_data or not spec_data['name']:\n        raise exceptions.IRValidatorException(\"Plugin name is required and cannot be empty\")\n\n    if 'description' not in spec_data or not spec_data['description']:\n        raise exceptions.IRValidatorException(\"Plugin description is required and cannot be empty\")\n\n    if 'version' not in spec_data or not spec_data['version']:\n        raise exceptions.IRValidatorException(\"Plugin version is required and cannot be empty\")\n\n    # Validate options structure\n    if 'options' in spec_data and not isinstance(spec_data['options'], dict):\n        raise exceptions.IRValidatorException(\"Options must be a dictionary\")\n\n    # Validate config structure\n    if 'config' in spec_data and not isinstance(spec_data['config'], dict):\n        raise exceptions.IRValidatorException(\"Config must be a dictionary\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields (adjust based on actual spec structure)\n    required_fields = [\n        'plugin_type',\n        'name',\n        'description',\n        'version',\n        'options',\n        'config'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation for specific fields if needed\n    if 'options' not in spec_data or not isinstance(spec_data['options'], dict):\n        raise exceptions.IRValidatorException(\"Options must be a dictionary\")\n\n    if 'config' not in spec_data or not isinstance(spec_data['config'], dict):\n        raise exceptions.IRValidatorException(\"Config must be a dictionary\")\n\n    return spec_data"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35924, "num_retrieved": 99, "inference_time": 4.6965446174144745, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            content = file.read()\n            if not content.strip():\n                raise cls.IRValidatorException(\"YAML file is empty\")\n            \n            data = yaml.safe_load(content)\n            if data is None:\n                raise cls.IRValidatorException(\"YAML file is invalid: empty or malformed\")\n            \n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            content = file.read()\n            if not content.strip():\n                raise cls.IRValidatorException(\"YAML file is empty\")\n            \n            data = yaml.safe_load(content)\n            if data is None:\n                raise cls.IRValidatorException(\"YAML file is invalid: empty or malformed\")\n            \n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            content = file.read()\n            if not content.strip():\n                raise cls.IRValidatorException(\"YAML file is empty\")\n            \n            data = yaml.safe_load(content)\n            if data is None:\n                raise cls.IRValidatorException(\"YAML file is invalid: empty or malformed\")\n            \n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            content = file.read()\n            if not content.strip():\n                raise cls.IRValidatorException(\"YAML file is empty\")\n            \n            data = yaml.safe_load(content)\n            if data is None:\n                raise cls.IRValidatorException(\"YAML file is invalid or empty\")\n            \n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            content = file.read()\n            if not content.strip():\n                raise cls.IRValidatorException(\"YAML file is empty\")\n            \n            data = yaml.safe_load(content)\n            if data is None:\n                raise cls.IRValidatorException(\"YAML file is invalid: empty or malformed\")\n            \n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37247, "num_retrieved": 138, "inference_time": 3.989132508635521, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the function name and typical behavior, it likely processes\n    # group inclusion directives in spec files\n    # Since no context is provided about how to implement this specifically,\n    # we return an empty implementation\n    pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the function name and typical behavior in configuration parsers,\n    # it likely processes a 'groups' directive that includes other groups from\n    # external sources or other specifications.\n    #\n    # Since no specific implementation details are provided in the context,\n    # this is a placeholder that would typically involve:\n    # 1. Looking for an 'include' directive in the parser_dict\n    # 2. Resolving the included groups from specified sources\n    # 3. Merging those groups into the current parser's group structure\n    \n    # Placeholder implementation - actual implementation would depend on\n    # the specific structure of the spec files and how groups are defined\n    pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the function name and typical behavior, it likely\n    # processes a dictionary of groups to include in the parser\n    # However, no specific implementation details are available\n    # in the retrieved context\n    \n    # Placeholder implementation - actual implementation would\n    # depend on the specific requirements of the infrared project\n    # and how group inclusion is handled in the spec files\n    \n    # If there's an 'include' key in parser_dict, process it\n    if 'include' in parser_dict:\n        include_groups = parser_dict['include']\n        # Process each group in the include dictionary\n        for group_name, group_config in include_groups.items():\n            # Add the group to the parser with its configuration\n            # This is a generic placeholder - actual implementation\n            # would depend on the specific group handling logic\n            pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical behavior, it likely processes\n    # group inclusion directives in spec files\n    # Since no implementation details are provided in the context,\n    # this is a placeholder that would need to be implemented based\n    # on the actual spec file parsing logic\n    pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical behavior, it likely processes\n    # group inclusion directives in spec files\n    # Since no implementation details are provided in the context,\n    # this is a placeholder that would need to be implemented\n    # based on the actual spec file parsing logic\n    pass"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37378, "num_retrieved": 145, "inference_time": 3.989132508635521, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process main parser defaults\n    if 'defaults' in spec_dict:\n        defaults.update(spec_dict['defaults'])\n    \n    # Process subparsers defaults\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            if 'defaults' in subparser_dict:\n                defaults.update(subparser_dict['defaults'])\n    \n    # Process shared groups defaults\n    if 'shared_groups' in spec_dict:\n        for group in spec_dict['shared_groups']:\n            if 'defaults' in group:\n                defaults.update(group['defaults'])\n    \n    # Process option defaults from spec\n    for parser in self.iterate_parsers():\n        for option_spec in self._get_all_options_spec(parser):\n            option_name = option_spec['name']\n            option_default = option_spec.get('default')\n            if option_default is not None:\n                defaults[option_name] = option_default\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get the spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser and subparsers\n    for parser_name, parser_dict in spec_dict.get('subparsers', {}).items():\n        # Process groups and options in the parser\n        for group in parser_dict.get('groups', []):\n            for option_name, option_dict in group.get('options', {}).items():\n                # Get default value from option specification\n                default_value = option_dict.get('default')\n                if default_value is not None:\n                    defaults[option_name] = default_value\n        \n        # Process flat options\n        for option_name, option_dict in parser_dict.get('options', {}).items():\n            default_value = option_dict.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    # Process the main parser\n    for group in spec_dict.get('groups', []):\n        for option_name, option_dict in group.get('options', {}).items():\n            default_value = option_dict.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    for option_name, option_dict in spec_dict.get('options', {}).items():\n        default_value = option_dict.get('default')\n        if default_value is not None:\n            defaults[option_name] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get default values from spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process main parser defaults\n    if 'defaults' in spec_dict:\n        defaults.update(spec_dict['defaults'])\n    \n    # Process subparsers defaults\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            if 'defaults' in subparser_dict:\n                defaults.update(subparser_dict['defaults'])\n    \n    # Process shared groups defaults\n    if 'shared_groups' in spec_dict:\n        for group in spec_dict['shared_groups']:\n            if 'defaults' in group:\n                defaults.update(group['defaults'])\n    \n    # Process option defaults from spec\n    for parser in self.iterate_parsers():\n        for option_spec in self._get_all_options_spec(parser):\n            option_name = option_spec['name']\n            option_default = option_spec.get('default')\n            if option_default is not None:\n                defaults[option_name] = option_default\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get defaults from spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process main parser defaults\n    if 'defaults' in spec_dict:\n        defaults.update(spec_dict['defaults'])\n    \n    # Process subparsers defaults\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            if 'defaults' in subparser_dict:\n                defaults.update(subparser_dict['defaults'])\n    \n    # Process shared groups defaults\n    if 'shared_groups' in spec_dict:\n        for group in spec_dict['shared_groups']:\n            if 'defaults' in group:\n                defaults.update(group['defaults'])\n    \n    # Process option defaults from spec\n    for parser in self.iterate_parsers():\n        for option_spec in self._get_all_options_spec(parser):\n            option_name = option_spec['name']\n            if 'default' in option_spec:\n                defaults[option_name] = option_spec['default']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get default values from the spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process subparsers and their defaults\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            # Get defaults from subparser options\n            if 'options' in subparser_dict:\n                for option_name, option_dict in subparser_dict['options'].items():\n                    if 'default' in option_dict:\n                        defaults[option_name] = option_dict['default']\n            \n            # Process groups within subparser\n            if 'groups' in subparser_dict:\n                for group in subparser_dict['groups']:\n                    if 'options' in group:\n                        for option_name, option_dict in group['options'].items():\n                            if 'default' in option_dict:\n                                defaults[option_name] = option_dict['default']\n    \n    # Process main parser defaults\n    if 'options' in spec_dict:\n        for option_name, option_dict in spec_dict['options'].items():\n            if 'default' in option_dict:\n                defaults[option_name] = option_dict['default']\n    \n    # Process groups in main parser\n    if 'groups' in spec_dict:\n        for group in spec_dict['groups']:\n            if 'options' in group:\n                for option_name, option_dict in group['options'].items():\n                    if 'default' in option_dict:\n                        defaults[option_name] = option_dict['default']\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37462, "num_retrieved": 146, "inference_time": 3.989132508635521, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36323, "num_retrieved": 114, "inference_time": 3.989132508635521, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific deprecation rules\n    deprecated_args = []\n    \n    # Example logic: check for known deprecated arguments in both cli_args and answer_file_args\n    # This is a template - actual implementation would need specific deprecation rules\n    for arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n        if arg in cli_args or arg in answer_file_args:\n            deprecated_args.append(arg)\n    \n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg in deprecated_args:\n            print(f\"  - {arg}\")\n        print(\"Please update your configuration to use the new arguments.\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36308, "num_retrieved": 111, "inference_time": 3.989132508635521, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36959, "num_retrieved": 130, "inference_time": 3.989132508635521, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36858, "num_retrieved": 128, "inference_time": 3.989132508635521, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # Implementation not found in context\n    # Based on typical behavior, this function might return arguments that are not processed\n    # or are marked as silent in the argument parsing system\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, silenced argument names\n    \"\"\"\n    # Implementation for get_silent_args function\n    # Based on the context, there is no direct information about silenced arguments\n    # Therefore, this function is implemented as a placeholder\n    # In a real scenario, this function would check for specific argument patterns or flags\n    # that indicate silence or suppression of output\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # Implementation is missing in the context\n    # Based on the function name and typical usage, this function likely\n    # identifies arguments that are set to silent mode or have silent behavior\n    # However, no specific logic or context is provided for this function\n    # Therefore, returning an empty list as a placeholder\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # Implementation not found in context\n    # Based on typical behavior, this function might return arguments that are not processed\n    # or arguments that are marked as silent in the parser configuration\n    # Since no context is provided for the actual implementation, \n    # we return an empty list as a placeholder\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # Implementation not found in context\n    # Based on function name and typical usage, this likely returns arguments that are not processed\n    # However, no context provides the logic for determining \"silent\" arguments\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36720, "num_retrieved": 124, "inference_time": 3.989132508635521, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args:\n            absent.append(arg)\n    \n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args:\n            absent.append(arg)\n    \n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args:\n            absent.append(arg)\n    \n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args:\n            absent.append(arg)\n    \n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args:\n            absent.append(arg)\n    \n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36390, "num_retrieved": 114, "inference_time": 3.989132508635521, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1\" or \"arg1=value1, arg2=value2\"\n        condition_parts = required_when.split(',')\n        \n        matched = False\n        for condition_part in condition_parts:\n            condition_part = condition_part.strip()\n            if not condition_part:\n                continue\n                \n            # Parse the condition: key=value\n            try:\n                condition_key, condition_value = condition_part.split('=', 1)\n            except ValueError:\n                continue\n                \n            # Check if the condition is met\n            # Get the value of the condition key from args\n            condition_value_from_args = args.get(command_name, {}).get(condition_key)\n            \n            # If the condition value is provided and matches the expected value\n            if condition_value_from_args is not None and str(condition_value_from_args) == str(condition_value):\n                matched = True\n                break\n                \n        if matched:\n            matched_args.append(option_spec['name'])\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg_name=value\" or \"arg_name\"\n        condition_parts = required_when.split('=')\n        condition_arg = condition_parts[0]\n        \n        # Check if the condition is met\n        # First, check if the condition argument exists in args\n        condition_value = None\n        if condition_arg in args.get(command_name, {}):\n            condition_value = args[command_name][condition_arg]\n        elif len(condition_parts) > 1:\n            # If it's a value condition, check if the value matches\n            condition_value = condition_parts[1]\n        \n        # If the condition is met, add the option name to matched_args\n        # For simple conditions like \"arg_name\", we just check if the arg exists\n        # For complex conditions, we need to evaluate them\n        if condition_value is not None:\n            # For now, we'll assume the condition is met if the value is truthy\n            # In a real implementation, we'd need to evaluate the condition properly\n            matched_args.append(option_spec['name'])\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1\" or \"arg1=value1, arg2=value2\"\n        condition_parts = required_when.split(',')\n        \n        matched = False\n        for condition_part in condition_parts:\n            condition_part = condition_part.strip()\n            if not condition_part:\n                continue\n                \n            # Parse condition: \"arg=value\"\n            try:\n                condition_arg, condition_value = condition_part.split('=', 1)\n            except ValueError:\n                continue\n                \n            # Check if the condition is met\n            # Get the value of the condition argument from args\n            condition_arg_value = args.get(command_name, {}).get(condition_arg)\n            \n            # If the condition is met (value matches or is truthy), mark this option as matched\n            if condition_arg_value is not None and (condition_value == condition_arg_value or \n                                                   (condition_value.lower() == 'true' and condition_arg_value is True) or \n                                                   (condition_value.lower() == 'false' and condition_arg_value is False)):\n                matched = True\n                break\n                \n        if matched:\n            matched_args.append(option_spec['name'])\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1\" or \"arg1\" or \"arg1==value1\"\n        condition = required_when\n        \n        # Check if the condition is met\n        # First, check if the condition is a simple argument name\n        if condition in args.get(command_name, {}):\n            matched_args.append(option_spec['name'])\n        # Check if the condition is a complex expression like \"arg1==value1\"\n        elif '=' in condition:\n            condition_parts = condition.split('==')\n            if len(condition_parts) == 2:\n                condition_arg = condition_parts[0]\n                condition_value = condition_parts[1]\n                # Check if the condition argument exists and matches the value\n                if condition_arg in args.get(command_name, {}):\n                    if str(args[command_name][condition_arg]) == condition_value:\n                        matched_args.append(option_spec['name'])\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1\" or \"arg1\" or \"arg1==value1\"\n        condition = required_when\n        \n        # Check if the condition is met\n        # For simplicity, we'll evaluate the condition based on the presence of arguments\n        # In a real implementation, this would involve more complex logic to parse and evaluate the condition\n        \n        # Check if the condition is met by looking at the args\n        # We'll assume that if the condition is a simple argument name, it must be present\n        # If the condition is a complex expression, it would need to be evaluated\n        \n        # For now, we'll check if the required_when condition is a simple argument name\n        # and if that argument is present in args\n        if condition in args.get(command_name, {}):\n            matched_args.append(option_spec['name'])\n    \n    return matched_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36471, "num_retrieved": 116, "inference_time": 3.989132508635521, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length is specified in the argument spec\n            # This is a placeholder - actual implementation would need\n            # to reference the expected length from argument specs\n            # For now, we'll just check if the value is too long\n            # (this would normally be based on a length constraint defined elsewhere)\n            if len(arg_value) > 100:  # Example length limit\n                invalid_options.append((arg_name, arg_value, 100))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length is specified in the argument spec or configuration\n            # This is a placeholder - actual implementation would depend on how length is defined\n            # For now, we'll just check if the string is too long (example length of 100)\n            if len(arg_value) > 100:\n                invalid_options.append((arg_name, arg_value, 100))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length is specified in the argument spec or configuration\n            # This is a placeholder - actual implementation would depend on how length is defined\n            # For now, we'll just check if the string is too long (example length check)\n            if len(arg_value) > 100:  # Example length limit\n                invalid_options.append((arg_name, arg_value, 100))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if the string length exceeds any specified limit\n            # This is a placeholder - actual implementation would depend on\n            # specific length constraints defined elsewhere\n            # For now, we'll just check if the value is too long\n            if len(arg_value) > 100:  # Example length limit\n                invalid_options.append((arg_name, arg_value, 100))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length is specified in the argument spec\n            # This is a placeholder - actual implementation would depend on how length is specified\n            # For example, if length is stored in a separate spec or configuration\n            # We would check against that length\n            pass\n    # If invalid options found, raise exception\n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36433, "num_retrieved": 115, "inference_time": 3.989132508635521, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    invalid_options = []\n\n    for option_spec in option_specs:\n        command_name = option_spec['command_name']\n        option_name = option_spec['option_name']\n        choices = option_spec.get('choices', [])\n\n        if not choices:\n            continue\n\n        if option_name not in args.get(command_name, {}):\n            continue\n\n        option_value = args[command_name][option_name]\n        if option_value not in choices:\n            invalid_options.append((command_name, option_name, choices))\n\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    invalid_options = []\n    \n    for option_spec in option_specs:\n        command_name = option_spec['command_name']\n        option_name = option_spec['option_name']\n        choices = option_spec.get('choices', [])\n        \n        if not choices:\n            continue\n            \n        if command_name not in args or option_name not in args[command_name]:\n            continue\n            \n        option_value = args[command_name][option_name]\n        \n        if option_value not in choices:\n            invalid_options.append((command_name, option_name, choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    invalid_options = []\n\n    for option_spec in option_specs:\n        command_name = option_spec['command_name']\n        option_name = option_spec['option_name']\n        choices = option_spec.get('choices', [])\n\n        if not choices:\n            continue\n\n        if option_name not in args.get(command_name, {}):\n            continue\n\n        option_value = args[command_name][option_name]\n        if option_value not in choices:\n            invalid_options.append((command_name, option_name, choices))\n\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    invalid_options = []\n\n    for option_spec in option_specs:\n        command_name = option_spec['command_name']\n        option_name = option_spec['option_name']\n        choices = option_spec.get('choices', [])\n\n        if not choices:\n            continue\n\n        if option_name not in args.get(command_name, {}):\n            continue\n\n        option_value = args[command_name][option_name]\n        if option_value not in choices:\n            invalid_options.append((command_name, option_name, choices))\n\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    invalid_options = []\n\n    for option_spec in option_specs:\n        command_name = option_spec['command_name']\n        option_name = option_spec['option_name']\n        choices = option_spec.get('choices', [])\n\n        if not choices:\n            continue\n\n        if option_name not in args.get(command_name, {}):\n            continue\n\n        option_value = args[command_name][option_name]\n        if option_value not in choices:\n            invalid_options.append((command_name, option_name, choices))\n\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36202, "num_retrieved": 108, "inference_time": 3.989132508635521, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # TODO: Implement this function\n    pass"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37106, "num_retrieved": 134, "inference_time": 3.989132508635521, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance based on the type_name\n    complex_type_class = None\n    \n    # Determine the appropriate complex type based on type_name\n    if type_name == 'value':\n        complex_type_class = Value\n    elif type_name == 'bool':\n        complex_type_class = Bool\n    elif type_name == 'flag':\n        complex_type_class = Flag\n    elif type_name == 'key_value_list':\n        complex_type_class = KeyValueList\n    elif type_name == 'list_of_files':\n        complex_type_class = ListOfFileNames\n    elif type_name == 'nested_dict':\n        complex_type_class = NestedDict\n    elif type_name == 'additional_options':\n        complex_type_class = AdditionalOptionsType\n    else:\n        # Default to Value if type is not recognized\n        complex_type_class = Value\n    \n    # Create and return the instance with required parameters\n    return complex_type_class(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance based on the type_name\n    complex_type_class = None\n    \n    # Map type names to their corresponding classes\n    if type_name == 'value':\n        complex_type_class = Value\n    elif type_name == 'bool':\n        complex_type_class = Bool\n    elif type_name == 'flag':\n        complex_type_class = Flag\n    elif type_name == 'key_value_list':\n        complex_type_class = KeyValueList\n    elif type_name == 'list_of_files':\n        complex_type_class = ListOfFileNames\n    elif type_name == 'nested_dict':\n        complex_type_class = NestedDict\n    elif type_name == 'additional_options':\n        complex_type_class = AdditionalOptionsType\n    else:\n        # Default to Value if type is not recognized\n        complex_type_class = Value\n    \n    # Create and return the instance with required parameters\n    if complex_type_class:\n        return complex_type_class(\n            arg_name=option_name,\n            settings_dirs=self.settings_dirs,\n            sub_command_name=subcommand,\n            spec_option=spec_option\n        )\n    else:\n        return Value(arg_name=option_name)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance based on the type_name\n    complex_type_class = None\n    \n    # Map type names to their corresponding classes\n    type_mapping = {\n        'value': Value,\n        'bool': Bool,\n        'flag': Flag,\n        'list': ListFileType,\n        'key_value': KeyValueList,\n        'nested_dict': NestedDict,\n        'additional_options': AdditionalOptionsType,\n        'file_list': ListFileType,\n        'var_dir': VarDirType,\n        'list_of_files': ListOfFileNames\n    }\n    \n    # Get the appropriate complex type class\n    if type_name in type_mapping:\n        complex_type_class = type_mapping[type_name]\n    \n    # If no specific class is found, use ComplexType as fallback\n    if complex_type_class is None:\n        complex_type_class = ComplexType\n    \n    # Create and return the instance with required parameters\n    return complex_type_class(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance based on the type_name\n    complex_type_class = None\n    \n    # Map type names to their corresponding classes\n    type_mapping = {\n        'value': Value,\n        'bool': Bool,\n        'flag': Flag,\n        'list': ListFileType,\n        'key_value': KeyValueList,\n        'nested_dict': NestedDict,\n        'additional_options': AdditionalOptionsType,\n        'file_list': ListFileType,\n        'var_dir': VarDirType,\n        'list_of_files': ListFileType\n    }\n    \n    # Get the appropriate complex type class\n    if type_name in type_mapping:\n        complex_type_class = type_mapping[type_name]\n    \n    # If no specific class is found, use ComplexType as fallback\n    if complex_type_class is None:\n        complex_type_class = ComplexType\n    \n    # Create and return the instance with the required parameters\n    return complex_type_class(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance based on the type_name\n    complex_type_class = None\n    \n    # Determine the appropriate complex type based on type_name\n    if type_name == 'list':\n        complex_type_class = ListFileType\n    elif type_name == 'nested_dict':\n        complex_type_class = NestedDict\n    elif type_name == 'key_value_list':\n        complex_type_class = KeyValueList\n    elif type_name == 'file_list':\n        complex_type_class = ListOfFileNames\n    else:\n        # Default to ComplexType if no specific type is found\n        complex_type_class = ComplexType\n    \n    # Create and return the instance with required parameters\n    return complex_type_class(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n   )"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35727, "num_retrieved": 94, "inference_time": 3.989132508635521, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Iterate through all arguments\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if the argument is a control argument (common IR behavior control)\n            # These typically include flags like dry-run, verbose, output, etc.\n            if arg in ['--dry-run', '--debug', '--verbose', '--output', '--extra-vars', '--inventory', '--ansible-args']:\n                # These are control arguments that affect IR behavior\n                # Extract value if present\n                value = None\n                if '=' in arg:\n                    value = arg.split('=', 1)[1]\n                control_args[arg] = value\n            else:\n                # All other arguments are considered nested arguments\n                # These will be used by Ansible playbooks\n                # Extract value if present\n                value = None\n                if '=' in arg:\n                    value = arg.split('=', 1)[1]\n                nested_args[arg] = value\n        else:\n            # Handle non-flags (e.g., positional arguments)\n            # These are typically nested arguments\n            nested_args[arg] = None\n    \n    # Handle custom arguments that are passed as extra-vars\n    # These are typically in the format of key=value\n    # We need to identify which ones are custom and which are nested\n    # Custom arguments are those that are not part of the standard control or nested arguments\n    # For now, we'll treat all arguments that are not control arguments as nested\n    # and any that are not in the standard format as custom\n    \n    # Extract custom arguments from the nested_args\n    custom_args = {}\n    for key, value in nested_args.items():\n        if key not in ['--dry-run', '--debug', '--verbose', '--output', '--extra-vars', '--inventory', '--ansible-args']:\n            # Check if it's", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Iterate through all arguments\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if the argument is a control argument (common IR behavior)\n            # These typically include flags like dry-run, verbose, output, etc.\n            if arg in ['--dry-run', '--debug', '--verbose', '--output', '--extra-vars', '--ansible-args']:\n                # These are control arguments\n                control_args[arg] = args[args.index(arg) + 1] if args.index(arg) + 1 < len(args) else None\n            else:\n                # Everything else is considered a nested argument\n                # Split the argument name and value\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    nested_args[key] = value\n                else:\n                    # For arguments without values, we need to handle them differently\n                    # This is a simplified approach - in practice, we might need to parse the full argument structure\n                    nested_args[arg] = None\n        else:\n            # Handle non-flags (e.g., positional arguments)\n            # These might be passed as nested arguments\n            nested_args[arg] = None\n    \n    # Separate custom arguments (those that are not nested)\n    # Custom arguments are typically passed as extra-vars or in a format that doesn't follow the nested pattern\n    custom_args = {}\n    for arg in args:\n        if arg.startswith('--extra-vars') or arg.startswith('--ansible-args'):\n            # Extract the value after the flag\n            value_idx = args.index(arg) + 1\n            if value_idx < len(args):\n                value = args[value_idx]\n                # Parse the value to extract key-value pairs\n                if '=' in value:\n                    key, val = value.split('=', 1)\n                    custom_args[key] = val", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if the argument is a control argument (common IR behavior)\n            # These are typically arguments that control IR behavior like verbosity, dry-run, etc.\n            if arg in ['--debug', '--dry-run', '--verbose', '--output', '--extra-vars', '--inventory']:\n                # These are control arguments\n                control_args[arg] = args[args.index(arg) + 1] if args.index(arg) + 1 < len(args) else None\n            else:\n                # These are nested arguments that will be put into the spec yml file\n                # Split the argument name and value\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    # Handle nested keys (e.g., key1.key2.key3)\n                    if '.' in key:\n                        keys = key.split('.')\n                        # Use dict_insert to handle nested keys\n                        nested_args[keys[0]] = {keys[1]: {keys[2]: value}} if len(keys) > 2 else {keys[1]: value}\n                    else:\n                        nested_args[key] = value\n                else:\n                    # Handle arguments without values\n                    nested_args[arg] = None\n        else:\n            # Handle non-argument values (e.g., positional arguments)\n            # These are typically not control or nested arguments\n            # They might be handled by other logic in the system\n            pass\n\n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Iterate through all arguments\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if the argument is a control argument (common IR behavior control)\n            # These typically include flags like dry-run, verbose, output, etc.\n            if arg in ['--dry-run', '--debug', '--verbose', '--output', '--extra-vars', '--ansible-args']:\n                # These are control arguments that affect IR behavior\n                # Extract value if present\n                value = None\n                if arg == '--dry-run':\n                    value = True\n                elif arg == '--debug':\n                    value = True\n                elif arg == '--verbose':\n                    value = args.get('verbose', 0)\n                elif arg == '--output':\n                    value = args.get('output', None)\n                elif arg == '--extra-vars':\n                    value = args.get('extra-vars', None)\n                elif arg == '--ansible-args':\n                    value = args.get('ansible-args', None)\n                \n                if value is not None:\n                    control_args[arg.replace('--', '')] = value\n            else:\n                # For other arguments, treat them as nested arguments\n                # These will be used by Ansible playbooks\n                # Extract the value (if present)\n                value = None\n                if arg == '--additional-args':\n                    value = args.get('additional-args', None)\n                else:\n                    # For other arguments, extract the value\n                    value = args.get(arg.replace('--', ''), None)\n                \n                if value is not None:\n                    # Handle nested arguments with dot notation\n                    if '=' in arg:\n                        key, val = arg.split('=', 1)\n                        # Split key by dots to handle nested structure\n                        keys = key.split('.')\n                        nested_args[keys[0]] = val\n                    else:\n                        # Simple", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if the argument is a control argument (common IR behavior)\n            # These typically include flags like dry-run, verbose, output, etc.\n            if arg in ['--dry-run', '--debug', '--verbose', '--output', '--extra-vars', '--generate-answers-file']:\n                # These are control arguments\n                control_args[arg] = args[args.index(arg) + 1] if args.index(arg) + 1 < len(args) else None\n            else:\n                # Everything else is considered a nested argument\n                # We'll handle nested arguments by splitting them by dots and inserting into nested_args\n                # For now, we'll just add them to nested_args\n                # In a real implementation, we would parse the nested structure\n                nested_args[arg] = args[args.index(arg) + 1] if args.index(arg) + 1 < len(args) else None\n        else:\n            # Handle non-flags (like values without --)\n            # This is a simplified approach - in reality, we'd need to parse the structure\n            if arg.startswith('extra-vars'):\n                # Handle extra-vars specially\n                custom_args[arg] = args[args.index(arg) + 1] if args.index(arg) + 1 < len(args) else None\n            else:\n                # Default to nested\n                nested_args[arg] = args[args.index(arg) + 1] if args.index(arg) + 1 < len(args) else None\n\n    return control_args, nested_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36322, "num_retrieved": 113, "inference_time": 3.989132508635521, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, str):\n            # Handle string format like \"key=value\"\n            if '=' in extra_var:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n        elif isinstance(extra_var, dict):\n            # Handle dictionary format\n            dict_utils.dict_merge(vars_dict, extra_var)\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, str):\n            # Split on '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            else:\n                # If no value, treat as key with empty value\n                vars_dict[parts[0]] = \"\"\n        elif isinstance(extra_var, dict):\n            # Directly merge dictionary\n            dict_utils.dict_merge(vars_dict, extra_var)\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, str):\n            # Split on '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            else:\n                # If no value, treat as key with empty value\n                vars_dict[parts[0]] = \"\"\n        elif isinstance(extra_var, dict):\n            # Directly merge dictionary\n            dict_utils.dict_merge(vars_dict, extra_var)\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, str):\n            # Handle string format like \"key=value\"\n            if '=' in extra_var:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n        elif isinstance(extra_var, dict):\n            # Handle dictionary format\n            dict_utils.dict_merge(vars_dict, extra_var)\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, str):\n            # Split on '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            else:\n                # If no value, treat as key with empty value\n                vars_dict[parts[0]] = \"\"\n        elif isinstance(extra_var, dict):\n            # Directly merge dictionary\n            dict_utils.dict_merge(vars_dict, extra_var)\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36034, "num_retrieved": 104, "inference_time": 3.989132508635521, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible-playbook command with required arguments\n    ansible_cmd = ['ansible-playbook', '-i', ir_workspace.inventory]\n    \n    # Add verbose level if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            if value is not None:\n                ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Execute the ansible-playbook command\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible command with required arguments\n    ansible_cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbose level if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            if value is not None:\n                ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Execute the ansible playbook\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Return the exit code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        # Return the exit code from the failed command\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible-playbook command with required arguments\n    ansible_cmd = ['ansible-playbook', '-i', ir_workspace.inventory]\n    \n    # Add verbose level if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            if value is not None:\n                ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Execute the ansible-playbook command\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible-playbook command with required arguments\n    ansible_command = ['ansible-playbook', playbook_path]\n    \n    # Add verbose level if specified\n    if verbose is not None:\n        ansible_command.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_command.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add additional arguments from ansible_args dictionary\n    if ansible_args:\n        for arg_name, arg_value in ansible_args.items():\n            if isinstance(arg_value, list):\n                ansible_command.extend([f\"--{arg_name}={item}\" for item in arg_value])\n            else:\n                ansible_command.extend([f\"--{arg_name}={arg_value}\"])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_command,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible command with required arguments\n    ansible_command = ['ansible-playbook', '-i', ir_workspace.inventory]\n    \n    # Add verbose level if specified\n    if verbose is not None:\n        ansible_command.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_command.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            if value is not None:\n                ansible_command.extend(['--' + arg, str(value)])\n    \n    # Execute the ansible playbook\n    result = subprocess.run(\n        ansible_command,\n        cwd=ir_workspace.path,\n        capture_output=True,\n        text=True,\n        check=False\n    )\n    \n    # Return the exit code\n    return result.returncode"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35717, "num_retrieved": 96, "inference_time": 4.895134409268697, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for easier processing\n    # This is a simplified version - in practice, cli_args would be parsed appropriately\n    ansible_args = {}\n    for arg in cli_args:\n        if arg.startswith(\"--\"):\n            # Extract key-value pair from argument\n            key = arg[2:]\n            if '=' in key:\n                key, value = key.split('=', 1)\n                ansible_args[key] = value\n            else:\n                ansible_args[key] = True\n    \n    # Prepare the playbook execution with the provided variables and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=ansible_args.get('verbose', None),\n        extra_vars=vars_dict,\n        ansible_args=ansible_args.get('ansible-args', None))\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for easier handling\n    # This is a simplified version - in reality, cli_args would be processed\n    # through the infrared CLI parsing system\n    \n    # Prepare the playbook execution with the provided variables\n    # The actual ansible execution would be handled by the execute.ansible_playbook function\n    \n    # Set up the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    \n    # Build the ansible command with the provided variables\n    # The actual execution would be delegated to the execute module\n    \n    # Execute the playbook with the provided variables and workspace context\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,  # Default to no verbose output unless specified\n        extra_vars=vars_dict,\n        ansible_args=cli_args  # Pass through any additional ansible arguments\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for easier handling\n    # This is a simplified version - in practice, cli_args would be parsed appropriately\n    ansible_args = {}\n    for arg in cli_args:\n        if arg.startswith(\"--\"):\n            # Extract key-value pair\n            key = arg[2:]\n            if '=' in key:\n                key, value = key.split('=', 1)\n                ansible_args[key] = value\n            else:\n                ansible_args[key] = True\n    \n    # Prepare the playbook execution with the provided vars_dict and ansible_args\n    # This is a simplified implementation - actual implementation would use Ansible's CLI\n    # or a wrapper to execute the playbook with the given parameters\n    \n    # In a real implementation, this would call execute.ansible_playbook with the appropriate parameters\n    # For now, we return a placeholder result\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=ansible_args.get('verbose', None),\n        extra_vars=vars_dict,\n        ansible_args=ansible_args.get('ansible-args', None)\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for easier handling\n    # This is a simplified version - in practice, cli_args would be processed\n    # through the infrared CLI parsing system\n    \n    # Prepare the playbook execution with the provided variables\n    # The actual ansible execution would be handled by the execute.ansible_playbook function\n    # which is referenced in the context\n    \n    # Set up the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    \n    # Build the ansible command with the provided variables\n    # The actual execution would be delegated to the execute module\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,  # Default to no verbose output unless specified\n        extra_vars=vars_dict,\n        ansible_args=cli_args  # Pass through any additional arguments\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the ansible playbook execution with the provided variables\n    ansible_playbook_path = ir_plugin.playbook\n    \n    # Build the ansible command with extra variables\n    ansible_args = []\n    for arg in cli_args:\n        if arg.startswith(\"--\"):\n            ansible_args.append(arg)\n    \n    # Execute the ansible playbook with the provided variables\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ansible_playbook_path,\n        verbose=False,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35677, "num_retrieved": 91, "inference_time": 4.895134409268697, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the option specifications for the given parser_name\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Iterate over each option specification\n    for option_spec in options:\n        option_name = option_spec['name']\n        option_type = option_spec.get('type', 'string')\n        \n        # Skip if no type is specified\n        if option_type == 'string':\n            continue\n            \n        # Convert the value based on the option type\n        if option_name in values_dict:\n            value = values_dict[option_name]\n            \n            # Handle specific types\n            if option_type == 'int':\n                try:\n                    values_dict[option_name] = int(value)\n                except ValueError:\n                    pass\n            elif option_type == 'bool':\n                if isinstance(value, str):\n                    values_dict[option_name] = value.lower() in ('true', '1', 'yes', 'on')\n                elif isinstance(value, (int, bool)):\n                    values_dict[option_name] = bool(value)\n            elif option_type == 'float':\n                try:\n                    values_dict[option_name] = float(value)\n                except ValueError:\n                    pass\n            elif option_type == 'list':\n                if isinstance(value, str):\n                    values_dict[option_name] = value.split(',')\n                elif isinstance(value, (list, tuple)):\n                    values_dict[option_name] = list(value)\n            elif option_type == 'dict':\n                if isinstance(value, str):\n                    # Parse string representation of dictionary\n                    try:\n                        values_dict[option_name] = eval(value)\n                    except:\n                        pass\n                elif isinstance(value, (dict, list)):\n                    values_dict[option_name] = dict(value)\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the option specifications for the given parser_name\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Process each option in the values_dict\n    for option_name, option_value in values_dict.items():\n        # Find the option specification\n        option_spec = next((opt for opt in options if opt['name'] == option_name), None)\n        \n        if option_spec:\n            # Extract the type from the option specification\n            option_type = option_spec.get('type')\n            \n            # If type is specified, convert the value accordingly\n            if option_type:\n                # Handle specific types like int, bool, etc.\n                if option_type == 'int':\n                    try:\n                        values_dict[option_name] = int(option_value)\n                    except ValueError:\n                        pass\n                elif option_type == 'bool':\n                    if option_value.lower() in ['true', '1', 'yes', 'on']:\n                        values_dict[option_name] = True\n                    elif option_value.lower() in ['false', '0', 'no', 'off']:\n                        values_dict[option_name] = False\n                    else:\n                        values_dict[option_name] = False\n                elif option_type == 'float':\n                    try:\n                        values_dict[option_name] = float(option_value)\n                    except ValueError:\n                        pass\n                elif option_type == 'list':\n                    # Split comma-separated values\n                    values_dict[option_name] = [v.strip() for v in option_value.split(',') if v.strip()]\n                elif option_type == 'dict':\n                    # Parse key-value pairs\n                    if option_value and ',' in option_value:\n                        kv_pairs = option_value.split(',')\n                        result_dict = {}\n                        for pair in kv_pairs:\n                            if '=' in pair:\n                                key, value = pair.split('=', 1)\n                                result_dict[key.strip()] = value.strip()\n                        values_dict[option_name] = result_dict\n                elif option_type == 'path':\n                    # Ensure path is absolute\n                    values_dict[option_name] = os.path.abspath(option_value)\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the option specifications for the given parser_name\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Iterate over each option specification\n    for option_spec in options:\n        option_name = option_spec['name']\n        option_type = option_spec.get('type', 'string')\n        \n        # If the option is not in values_dict, skip it\n        if option_name not in values_dict:\n            continue\n            \n        # Get the current value\n        value = values_dict[option_name]\n        \n        # Convert the value based on the option type\n        if option_type == 'int':\n            try:\n                values_dict[option_name] = int(value)\n            except ValueError:\n                pass\n        elif option_type == 'bool':\n            if isinstance(value, str):\n                values_dict[option_name] = value.lower() in ('true', '1', 'yes', 'on')\n            elif isinstance(value, (int, bool)):\n                values_dict[option_name] = bool(value)\n        elif option_type == 'float':\n            try:\n                values_dict[option_name] = float(value)\n            except ValueError:\n                pass\n        elif option_type == 'list':\n            if isinstance(value, str):\n                values_dict[option_name] = value.split(',')\n            elif isinstance(value, (list, tuple)):\n                values_dict[option_name] = list(value)\n        elif option_type == 'dict':\n            if isinstance(value, str):\n                # Simple string to dict conversion (basic implementation)\n                values_dict[option_name] = {}\n            elif isinstance(value, (dict,)):\n                values_dict[option_name] = dict(value)\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the option specifications for the given parser_name\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Process each option in the values_dict\n    for option_name, option_value in values_dict.items():\n        # Find the option specification\n        option_spec = next((opt for opt in options if opt['name'] == option_name), None)\n        \n        if option_spec:\n            # Determine the type based on the option specification\n            option_type = option_spec.get('type', 'string')\n            \n            # Convert the value based on the type\n            if option_type == 'int':\n                try:\n                    values_dict[option_name] = int(option_value)\n                except (ValueError, TypeError):\n                    pass\n            elif option_type == 'bool':\n                if option_value.lower() in ['true', '1', 'yes', 'on']:\n                    values_dict[option_name] = True\n                elif option_value.lower() in ['false', '0', 'no', 'off']:\n                    values_dict[option_name] = False\n                else:\n                    values_dict[option_name] = False\n            elif option_type == 'float':\n                try:\n                    values_dict[option_name] = float(option_value)\n                except (ValueError, TypeError):\n                    pass\n            elif option_type == 'list':\n                # Split comma-separated values into a list\n                if option_value:\n                    values_dict[option_name] = [v.strip() for v in option_value.split(',')]\n            elif option_type == 'dict':\n                # Parse key-value pairs from string\n                if option_value:\n                    pairs = option_value.split(',')\n                    values_dict[option_name] = {}\n                    for pair in pairs:\n                        if '=' in pair:\n                            key, val = pair.split('=', 1)\n                            values_dict[option_name][key.strip()] = val.strip()\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the option specifications for the given parser_name\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Process each option in the values_dict\n    for option_name, option_value in values_dict.items():\n        # Find the option specification\n        option_spec = next((opt for opt in options if opt['name'] == option_name), None)\n        \n        if option_spec:\n            # Determine the action and type from the option specification\n            action = option_spec.get('action', '')\n            type_ = option_spec.get('type', 'string')\n            \n            # Convert based on the type\n            if type_ == 'int':\n                try:\n                    values_dict[option_name] = int(option_value)\n                except ValueError:\n                    pass\n            elif type_ == 'bool':\n                if option_value.lower() in ['true', '1', 'yes', 'on']:\n                    values_dict[option_name] = True\n                elif option_value.lower() in ['false', '0', 'no', 'off']:\n                    values_dict[option_name] = False\n                else:\n                    values_dict[option_name] = False\n            elif type_ == 'float':\n                try:\n                    values_dict[option_name] = float(option_value)\n                except ValueError:\n                    pass\n            elif type_ == 'list':\n                # Split comma-separated values\n                if option_value:\n                    values_dict[option_name] = [v.strip() for v in option_value.split(',')]\n                else:\n                    values_dict[option_name] = []\n            elif type_ == 'dict':\n                # Handle dictionary type - split key=value pairs\n                if option_value:\n                    values_dict[option_name] = {}\n                    for item in option_value.split(','):\n                        if '=' in item:\n                            key, value = item.split('=', 1)\n                            values_dict[option_name][key.strip()] = value.strip()\n                else:\n                    values_dict[option_name] = {}\n    \n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36682, "num_retrieved": 122, "inference_time": 4.895134409268697, "generate_results": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    \n    # Construct the path to the plugin spec file\n    spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Check if the spec file exists\n    if not os.path.exists(spec_file):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_file}\")\n    \n    # Load the spec file content\n    with open(spec_file, 'r') as file:\n        spec_content = file.read()\n    \n    # Parse the spec content into a dictionary\n    spec_dict = yaml.safe_load(spec_content)\n    \n    # Create a flat dictionary from the spec\n    flat_dict = {}\n    \n    # Extract plugin type\n    if 'config' in spec_dict:\n        plugin_type = spec_dict['config'].get('plugin_type')\n        if plugin_type:\n            flat_dict['plugin_type'] = plugin_type\n    \n    # Extract entry point\n    if 'config' in spec_dict:\n        entry_point = spec_dict['config'].get('entry_point')\n        if entry_point:\n            flat_dict['entry_point'] = entry_point\n    \n    # Extract roles path\n    if 'config' in spec_dict:\n        roles_path = spec_dict['config'].get('roles_path')\n        if roles_path:\n            flat_dict['roles_path'] = roles_path\n    \n    # Extract description\n    if 'description' in spec_dict:\n        flat_dict['description'] = spec_dict['description']\n    \n    # Extract subparsers (if any)\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        if subparsers:\n            flat_dict['subparsers'] = subparsers\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    from configparser import ConfigParser\n    \n    # Construct the path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Check if the spec file exists\n    if not os.path.exists(spec_file_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_file_path}\")\n    \n    # Load the spec file content\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    # Parse the spec content into a dictionary\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise ValueError(f\"Error parsing plugin spec file: {e}\")\n    \n    # Create a flat dictionary from the spec\n    flat_dict = {}\n    \n    # Extract plugin type\n    if 'config' in spec_dict:\n        plugin_type = spec_dict['config'].get('plugin_type')\n        if plugin_type:\n            flat_dict['plugin_type'] = plugin_type\n    \n    # Extract entry point\n    if 'config' in spec_dict:\n        entry_point = spec_dict['config'].get('entry_point')\n        if entry_point:\n            flat_dict['entry_point'] = entry_point\n    \n    # Extract roles path\n    if 'config' in spec_dict:\n        roles_path = spec_dict['config'].get('roles_path')\n        if roles_path:\n            flat_dict['roles_path'] = roles_path\n    \n    # Extract description\n    if 'description' in spec_dict:\n        flat_dict['description'] = spec_dict['description']\n    \n    # Extract subparsers (if any)\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        flat_dict['subparsers'] = subparsers\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    from configparser import ConfigParser\n    \n    # Construct the path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Check if the spec file exists\n    if not os.path.exists(spec_file_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_file_path}\")\n    \n    # Load the spec file content\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    # Parse the spec content into a dictionary\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise ValueError(f\"Error parsing plugin spec file: {e}\")\n    \n    # Create a flat dictionary from the spec\n    flat_dict = {}\n    \n    # Extract plugin type and description\n    if 'config' in spec_dict:\n        plugin_type = spec_dict['config'].get('plugin_type')\n        description = spec_dict.get('description')\n        if plugin_type:\n            flat_dict['plugin_type'] = plugin_type\n        if description:\n            flat_dict['description'] = description\n    \n    # Extract entry point\n    if 'config' in spec_dict:\n        entry_point = spec_dict['config'].get('entry_point')\n        if entry_point:\n            flat_dict['entry_point'] = entry_point\n    \n    # Extract roles path\n    if 'config' in spec_dict:\n        roles_path = spec_dict['config'].get('roles_path')\n        if roles_path:\n            flat_dict['roles_path'] = roles_path\n    \n    # Extract subparsers (if any)\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        flat_dict['subparsers'] = subparsers\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    from configparser import ConfigParser\n    \n    # Construct the path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Check if the spec file exists\n    if not os.path.exists(spec_file_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_file_path}\")\n    \n    # Load the spec file content\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    # Parse the spec content into a dictionary\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise ValueError(f\"Error parsing plugin spec file: {e}\")\n    \n    # Create a flat dictionary from the spec\n    flat_dict = {}\n    \n    # Extract plugin type\n    if 'config' in spec_dict:\n        plugin_type = spec_dict['config'].get('plugin_type')\n        if plugin_type:\n            flat_dict['plugin_type'] = plugin_type\n    \n    # Extract entry point\n    if 'config' in spec_dict:\n        entry_point = spec_dict['config'].get('entry_point')\n        if entry_point:\n            flat_dict['entry_point'] = entry_point\n    \n    # Extract roles path\n    if 'config' in spec_dict:\n        roles_path = spec_dict['config'].get('roles_path')\n        if roles_path:\n            flat_dict['roles_path'] = roles_path\n    \n    # Extract description\n    if 'description' in spec_dict:\n        flat_dict['description'] = spec_dict['description']\n    \n    # Extract subparsers (if any)\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        flat_dict['subparsers'] = subparsers\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    from configparser import ConfigParser\n    \n    # Construct the path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Check if the spec file exists\n    if not os.path.exists(spec_file_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_file_path}\")\n    \n    # Load the spec file content\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    # Parse the spec content into a dictionary\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise ValueError(f\"Error parsing plugin spec file: {e}\")\n    \n    # Create a flat dictionary from the spec\n    flat_dict = {}\n    \n    # Extract basic plugin properties\n    if 'description' in spec_dict:\n        flat_dict['description'] = spec_dict['description']\n    \n    if 'plugin_type' in spec_dict:\n        flat_dict['plugin_type'] = spec_dict['plugin_type']\n    \n    if 'entry_point' in spec_dict:\n        flat_dict['entry_point'] = spec_dict['entry_point']\n    \n    if 'roles_path' in spec_dict:\n        flat_dict['roles_path'] = spec_dict['roles_path']\n    \n    # Extract subparsers if present\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        if subparsers:\n            # Get the first (and only) subparser key\n            first_subparser = next(iter(subparsers.keys()))\n            flat_dict['subparsers'] = first_subparser\n    \n    return flat_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 37096, "num_retrieved": 131, "inference_time": 4.895134409268697, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.ansible_config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.ansible_config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.ansible_config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.ansible_config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.ansible_config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 36929, "num_retrieved": 130, "inference_time": 4.895134409268697, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    spec_dict = self.specification\n    if not spec_dict:\n        return\n    \n    # Create a parser for the current spec\n    parser_name = self.name\n    parser_dict = spec_dict.get('subparsers', {}).get(parser_name, {})\n    \n    # Add the subparser if it exists\n    if parser_dict:\n        cmd_parser = root_subparsers.add_parser(\n            parser_name,\n            help=parser_dict.get('help', parser_dict.get('description', '')),\n            description=parser_dict.get('description', parser_dict.get('help', '')),\n            formatter_class=spec_dict.get('formatter_class', argparse.RawTextHelpFormatter)\n        )\n        \n        # Add groups and options to the parser\n        self._add_groups(self, cmd_parser, parser_name, parser_dict)", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    spec_dict = self.specification\n    if not spec_dict:\n        return\n    \n    # Create a parser for the current spec\n    parser_name = self.name\n    parser_dict = spec_dict.get('subparsers', {}).get(parser_name, {})\n    \n    # Add the subparser if it exists\n    if parser_dict:\n        cmd_parser = root_subparsers.add_parser(\n            parser_name,\n            help=parser_dict.get('help', parser_dict.get('description', '')),\n            description=parser_dict.get('description', parser_dict.get('help', '')),\n            formatter_class=spec_dict.get('formatter_class', argparse.RawTextHelpFormatter)\n        )\n        \n        # Add groups and options to the parser\n        self._add_groups(self, cmd_parser, parser_name, parser_dict)", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    spec_dict = self.specification\n    if not spec_dict:\n        return\n    \n    # Create a parser for the current spec\n    parser_name = self.name\n    parser_dict = spec_dict.get('subparsers', {}).get(parser_name, {})\n    \n    # Add the subparser if it exists\n    if parser_dict:\n        cmd_parser = root_subparsers.add_parser(\n            parser_name,\n            help=parser_dict.get('help', parser_dict.get('description', '')),\n            description=parser_dict.get('description', parser_dict.get('help', '')),\n            formatter_class=spec_dict.get('formatter_class', argparse.RawTextHelpFormatter)\n        )\n        \n        # Add groups and options to the parser\n        self._add_groups(self, cmd_parser, parser_name, parser_dict)", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    spec_dict = self.spec_dict\n    parser_name = self.name\n    \n    # Create a subparser for this spec\n    subparser = root_subparsers.add_parser(\n        parser_name,\n        help=spec_dict.get('help', spec_dict.get('description', '')),\n        description=spec_dict.get('description', spec_dict.get('help', ''))\n    )\n    \n    # Add groups and options to the subparser\n    if 'groups' in spec_dict:\n        for group_dict in spec_dict['groups']:\n            group = subparser.add_argument_group(group_dict['title'])\n            for opt_name, opt_dict in group_dict.get('options', {}).items():\n                group.add_argument(\n                    opt_name,\n                    **opt_dict\n                )\n    \n    # Add flat options\n    for opt_name, opt_dict in spec_dict.get('options', {}).items():\n        subparser.add_argument(\n            opt_name,\n            **opt_dict\n        )\n    \n    # Add shared groups if defined\n    if 'shared_groups' in spec_dict:\n        for shared_group in spec_dict['shared_groups']:\n            group = subparser.add_argument_group(shared_group['title'])\n            for opt_name, opt_dict in shared_group.get('options', {}).items():\n                group.add_argument(\n                    opt_name,\n                    **opt_dict\n                )", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    spec_dict = self.spec_dict\n    parser_dict = spec_dict\n    \n    # Process subparsers\n    subparsers = parser_dict.get('subparsers', {})\n    if subparsers:\n        dest_path = 'subcommand'\n        for subparser_name, subparser_dict in parser_dict.get('subparsers', {}).items():\n            cmd_parser = root_subparsers.add_parser(\n                subparser_name,\n                help=subparser_dict.get('help', subparser_dict.get('description', '')),\n                description=subparser_dict.get('description', subparser_dict.get('help', '')),\n                formatter_class=parser_dict.get('formatter_class', argparse.RawTextHelpFormatter))\n            \n            # Add groups and options to the subparser\n            self._add_groups(self, cmd_parser, subparser_name, subparser_dict, path_prefix=dest_path)\n    \n    # Add shared groups if defined\n    shared_groups = parser_dict.get('shared_groups', [])\n    if shared_groups:\n        for group_dict in shared_groups:\n            group = root_subparsers.add_argument_group(group_dict['title'])\n            for opt_name, opt_dict in group_dict.get('options', {}).items():\n                group.add_argument(\n                    opt_name,\n                    **opt_dict\n                )"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 32768, "actual_tokens": 35108, "num_retrieved": 107, "inference_time": 4.895134409268697, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")"]}
